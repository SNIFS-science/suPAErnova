{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook creates the plots for \n",
    "\n",
    "### *A Probabilistic Autoencoder for Type Ia Supernovae*\n",
    "\n",
    "Note that the dataset is not yet public, so most figures as they appear in the paper cannot be directly reproduced. But, this notebook should give a clear example on how the analysis was performed, and how to reproduce a similar analysis on another dataset.\n",
    "\n",
    "The trained models are made public, and you will find them in `../outputs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print('tensorflow version: ', tf.__version__)\n",
    "print('devices: ', tf.config.list_physical_devices('GPU') )\n",
    "\n",
    "tfk  = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "print(\"TFK Version\", tfk.__version__)\n",
    "\n",
    "# %pip install tensorflow-probability==0.9.0                                                                          \n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfb  = tfp.bijectors\n",
    "tfd  = tfp.distributions\n",
    "print(\"TFP Version\", tfp.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "#%matplotlib inline\n",
    "\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorboard.plugins.hparams as HParams\n",
    "import argparse\n",
    "\n",
    "from suPAErnova.utils.YParams import YParams\n",
    "from suPAErnova.utils.data_loader import *\n",
    "from suPAErnova.utils.calculations import *\n",
    "\n",
    "from suPAErnova.models.losses import *\n",
    "from suPAErnova.models.posterior import *\n",
    "\n",
    "from suPAErnova.models import loader \n",
    "from suPAErnova.models import flows\n",
    "\n",
    "fs = 16\n",
    "\n",
    "#SET UP FIGURE AESTHETICS \n",
    "plt.rcParams.update({'font.family' : 'lmodern', 'font.size': 18,                                                                                                                                                    \n",
    "                     'axes.labelsize': 18, 'legend.fontsize': 16, \n",
    "                     'xtick.labelsize': 16, 'ytick.labelsize': 16, \n",
    "                     'axes.linewidth': 1.})                                                                                                                                                                    \n",
    "\n",
    "\n",
    "fs = 16\n",
    "\n",
    "params = {'text.usetex' : True,\n",
    "        \"font.family\": \"sans-serif\",\n",
    "        \"font.sans-serif\": [\"Helvetica Neue\"]}\n",
    "\n",
    "figdir = '../figures/'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "\n",
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = colors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "\n",
    "cmap = plt.get_cmap('viridis')\n",
    "cmap_scatter = truncate_colormap(cmap, 0.0, 1.0)\n",
    "cmap_spectra = plt.get_cmap('coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create colorlaw using the *extinction* package\n",
    "\n",
    "https://extinction.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import extinction\n",
    "# # Load the fiducial color law.\n",
    "# av = 1.0\n",
    "# Rv = 2.8\n",
    "# wave = train_data['wavelengths']\n",
    "# color_law = extinction.fitzpatrick99(\n",
    "#             train_data['wavelengths'], av, Rv\n",
    "#         )\n",
    "\n",
    "# np.savetxt(\"../data/F99_colorlaw.txt\", np.c_[wave, color_law])\n",
    "                                            \n",
    "# w, color_law_old, CL_deriv = np.loadtxt(\"../data/F99_colorlaw_old.txt\", unpack=True)\n",
    "# plt.plot(train_data['wavelengths'], color_law_old)\n",
    "# plt.plot(train_data['wavelengths'], color_law)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(train_data['wavelengths'], color_law_old/color_law)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in trained models and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "want_models = True\n",
    "\n",
    "latent_dim = 3 \n",
    "kfold = 0\n",
    "\n",
    "# savefig = True \n",
    "savefig = False\n",
    "\n",
    "file_string = 'layers256-128-32_3stage_train_decorrelate_all_seed0' \n",
    "\n",
    "train_data_file = '../outputs/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "test_data_file  = '../outputs/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "\n",
    "param_file = '../outputs/params/AE_kfold{:d}_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "                                                                                      \n",
    "# Load data\n",
    "def load_data(file_path):\n",
    "    return np.load(file_path, allow_pickle=True).item()\n",
    "\n",
    "train_data = load_data(train_data_file)\n",
    "test_data  = load_data(test_data_file)\n",
    "\n",
    "params['min_train_redshift'] = 0.02\n",
    "params['max_train_redshift'] = 1.00\n",
    "params['max_light_cut'] = [-10, 40]\n",
    "params['twins_cut'] = False\n",
    "\n",
    "if want_models:\n",
    "    # Load in models, as well as the outputs above\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--yaml_config\", default='../config/posterior_analysis.yaml', type=str)\n",
    "    parser.add_argument(\"--config\", default='posterior', type=str)\n",
    "    parser.add_argument(\"--print_params\", default=True, action='store_true')\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    params = YParams(os.path.abspath(args.yaml_config), args.config, print_params=True)            \n",
    "    params['latent_dim'] = latent_dim\n",
    "\n",
    "#     params = np.load(param_file, allow_pickle=True).item()['parameters']\n",
    "    params['nlayers'] = 12\n",
    "    params['nunit'] =  8\n",
    "\n",
    "    pprint(vars(params))\n",
    "    # Get PAE model\"\n",
    "    PAE = loader.PAE(params)\n",
    "\n",
    "    istart = 2\n",
    "\n",
    "    # get latent representations from encoder and flow                                                                       \n",
    "    train_data['z_latent'] = PAE.encoder((train_data['spectra'], train_data['times'], train_data['mask'])).numpy()\n",
    "    test_data['z_latent']  = PAE.encoder((test_data['spectra'], test_data['times'], test_data['mask'])).numpy()\n",
    "\n",
    "    train_data['u_latent'] = PAE.flow.bijector.inverse(train_data['z_latent'][:, istart:]).numpy()\n",
    "    test_data['u_latent']  = PAE.flow.bijector.inverse(test_data['z_latent'][:, istart:]).numpy()\n",
    "\n",
    "    # get reconstructions\n",
    "    train_data['spectra_ae'] = PAE.decoder((train_data['z_latent'], train_data['times'], train_data['mask'])).numpy()\n",
    "    test_data['spectra_ae']  = PAE.decoder((test_data['z_latent'], test_data['times'], test_data['mask'])).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct plotting functions for custom corner plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from scipy import interpolate\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "# cmap = plt.cm.cividis\n",
    "def plot_grid(data, labels=None, errs=None, \n",
    "              color_arr=None, color_label=None, color_symmetric=False,\n",
    "              plot_density=False, plot_diagonal=False,\n",
    "              xyminmax=None,\n",
    "              figsize=(8,8), spacing=0.05, s=30, ms_mean=6, alpha=1.0, cmap=plt.cm.viridis, markeredgecolor='k',\n",
    "              savefig=False, filename='plot_grid.pdf', rasterized=False, include_mean=False, nbins=25, nbins_contour=15,\n",
    "             show_correlation=True, fontsize=12):\n",
    "    \n",
    "    \"\"\"plot variables against each other. Similar to seaborns pairplot, but more functionality\"\"\"\n",
    "    \n",
    "    # colors = plt.cm.coolwarm_r(np.linspace(0.6, 1.0, 256))\n",
    "    # colors[0:4, :3] = 1.\n",
    "    # colors_1D = colors[int(0.8*colors.shape[0])]\n",
    "    colors = cmap(np.linspace(0.0, 1.0, 256))\n",
    "    colors_1D = cmap(0.8)\n",
    "\n",
    "    \n",
    "    # cmap = LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "\n",
    "    sigmas = np.array([1., 2.])#, 3.]) # sigmas\n",
    "    colors_contour = plt.cm.coolwarm(np.linspace(0.6, 1.0, 256))\n",
    "    # contour_colors = colors_contour[ np.linspace(0, colors_contour.shape[0]-1, sigmas.shape[0]).astype(int) ]\n",
    "    contour_colors = np.zeros((len(sigmas), 4))\n",
    "    contour_colors[:, -1] = np.linspace(0.2, 0.7, contour_colors.shape[0])\n",
    "\n",
    "    nx = data.shape[1] - 1\n",
    "    ny = data.shape[1] - 1\n",
    "    istart = 1\n",
    "    jstart = 1\n",
    "    \n",
    "    if plot_density:\n",
    "        plot_diagonal=True\n",
    "    if not plot_diagonal:\n",
    "        istart = 0\n",
    "        jstart = 0\n",
    "\n",
    "    if color_arr is None:\n",
    "        color_arr_plt = color_arr\n",
    "    else:\n",
    "        color_arr_plt = color_arr.copy()\n",
    "\n",
    "    if color_symmetric:\n",
    "        # cminmax = min(np.abs(np.min(color_arr_plt)), np.abs(np.max(color_arr_plt)))\n",
    "        cminmax = 0.25\n",
    "        color_arr_plt[color_arr_plt < -cminmax] = -cminmax\n",
    "        color_arr_plt[color_arr_plt > cminmax] = cminmax\n",
    "\n",
    "    if labels is None:\n",
    "        labels = ['z_{:d}'.format(i) for i in range(data.shape[1])]\n",
    "        \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = gridspec.GridSpec(ncols=ny+jstart, nrows=nx+istart, figure=fig)\n",
    "\n",
    "    plt.subplots_adjust(wspace=spacing, hspace=spacing)\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.cm.viridis\n",
    "       \n",
    "    if plot_diagonal:\n",
    "        for i in range(data.shape[1]):\n",
    "            jj = (ny-i+1)%(ny+1)\n",
    "\n",
    "            ax = fig.add_subplot(gs[i, i])\n",
    "            \n",
    "            ax.hist(data[:, jj], histtype='step', \n",
    "                    lw=2, \n",
    "                    bins=nbins, \n",
    "                    color=colors_1D)\n",
    "            \n",
    "            for axis in ['top','bottom','left','right']:\n",
    "                ax.spines[axis].set_linewidth(1.)\n",
    "\n",
    "            ax.tick_params('both', length=6, width=1., which='major', color='k', direction='in')                                         \n",
    "            ax.tick_params('both', length=3, width=1., which='minor', color='k', direction='in')\n",
    "            \n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_yticks([])\n",
    "            if i == data.shape[1] - 1:\n",
    "                ax.set_xlim(data[:, jj].min(), data[:, jj].max() )\n",
    "                xti = ax.get_xticks()\n",
    "                xti = ['{:.2f}'.format(it) for it in xti]\n",
    "                ax.set_xticklabels(xti, rotation=30)\n",
    "\n",
    "            if i < data.shape[1] - 1:\n",
    "                ax.set_xticklabels([])\n",
    "                      \n",
    "    for i in range(nx):\n",
    "        for j in range(i, ny):\n",
    "            \n",
    "            ii = ny-j \n",
    "            jj = (ny-i+1)%(ny+1)\n",
    "\n",
    "            ax = fig.add_subplot(gs[j+jstart, i])\n",
    "            if color_arr_plt is not None:\n",
    "                if i==0 and j==0:\n",
    "                    xx = np.linspace(color_arr_plt.min(), color_arr_plt.max(), 256)\n",
    "                    dummie_cax = ax.scatter(xx, xx, c=xx, cmap=cmap)\n",
    "                    # Clear axis\n",
    "                    ax.cla()\n",
    "\n",
    "            for axis in ['top','bottom','left','right']:\n",
    "                ax.spines[axis].set_linewidth(1.)\n",
    "\n",
    "            ax.tick_params('both', length=6, width=1., which='major', color='k', direction='in')                                         \n",
    "            ax.tick_params('both', length=3, width=1., which='minor', color='k', direction='in')\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(labels[ii])\n",
    "            else:\n",
    "                ax.set_yticklabels([])\n",
    "            \n",
    "            if j == ny-1:\n",
    "                ax.set_xlabel(labels[jj])\n",
    "            else:\n",
    "                ax.set_xticklabels([])\n",
    "\n",
    "            if not plot_density:\n",
    "                if color_arr_plt is None:\n",
    "                    if errs is None:\n",
    "                        ax.scatter(data[:, jj], data[:, ii], s=s, alpha=alpha, rasterized=rasterized)\n",
    "                    else:\n",
    "                        ax.errorbar(data[:, jj], data[:, ii], xerr=errs[:, jj], yerr=errs[:, ii], marker='o', \n",
    "                                    ms=s, alpha=alpha, markeredgecolor=markeredgecolor,\n",
    "                                    rasterized=rasterized, ls='none')\n",
    "\n",
    "                else:\n",
    "                    if errs is None:\n",
    "                        c = ax.scatter(data[:, jj], data[:, ii], s=s, c=color_arr_plt, edgecolors='k', lw=0.25, alpha=alpha, cmap=cmap, rasterized=rasterized)\n",
    "                        if include_mean:\n",
    "                            ax.errorbar(np.mean(data[:, jj]), np.mean(data[:, ii]), xerr=np.std(data[:, jj]), yerr=np.std(data[:, ii]),\n",
    "                                       ms=ms_mean, marker='o', markeredgecolor='k',\n",
    "                                       color='w', lw=2, alpha=alpha)\n",
    "\n",
    "                    else:\n",
    "                        carr = (color_arr_plt-color_arr_plt.min())/(color_arr_plt.max()-color_arr_plt.min())\n",
    "                        for ierr in range(data[:, jj].shape[0]):\n",
    "                            ax.errorbar(data[ierr, jj], data[ierr, ii], xerr=errs[ierr, jj], yerr=errs[ierr, ii], markeredgecolor=markeredgecolor,\n",
    "                                        color=cmap(carr[ierr]), marker='o', ms=s, alpha=alpha, rasterized=rasterized, ls='none', \n",
    "                                         lw=2)\n",
    "                if show_correlation:\n",
    "                    r, _ = pearsonr(data[:, jj], data[:, ii])\n",
    "                    ax.text(0.96, 0.96, f\"r={r:.2f}\",\n",
    "                            horizontalalignment='right',\n",
    "                            verticalalignment='top',\n",
    "                            transform=ax.transAxes,\n",
    "                            path_effects=[pe.Stroke(linewidth=3, foreground='w'), pe.Normal()],\n",
    "                           fontsize=fontsize)\n",
    "\n",
    "            if plot_density:\n",
    "                ax.hexbin(data[:, jj], data[:, ii], gridsize=nbins, cmap=cmap)#, norm=matplotlib.colors.LogNorm())\n",
    "                # ax.contour(counts)#, extent=[xbins.min(), xbins.max(), ybins.min(), ybins.max()],\n",
    "                           # linewidths=3)\n",
    "                H, xedges, yedges = np.histogram2d(data[:, jj], data[:, ii], bins=nbins_contour)#, bins=(xedges, yedges))\n",
    "\n",
    "                H /= H.sum()\n",
    "\n",
    "                n = 10000\n",
    "                t = np.linspace(0, H.max(), n)\n",
    "                integral = ((H >= t[:, None, None]) * H).sum(axis=(1,2))\n",
    "\n",
    "                intervals = (1 - np.exp(-(sigmas)**2/2)) # CDF in 2D\n",
    "\n",
    "                f = interpolate.interp1d(integral, t)\n",
    "                t_contours = f(intervals[::-1])\n",
    "                ax.contour(H.T, t_contours, \n",
    "                           extent=[data[:, jj].min(), data[:,jj].max(), data[:,ii].min(), data[:,ii].max()],\n",
    "                          colors=contour_colors)\n",
    "                if include_mean:\n",
    "                        ax.errorbar(np.mean(data[:, jj]), np.mean(data[:, ii]), xerr=np.std(data[:, jj]), yerr=np.std(data[:, ii]),\n",
    "                                    ms=ms_mean, marker='o', markeredgecolor='w',\n",
    "                                    color='k', lw=2, alpha=alpha)\n",
    "                        \n",
    "            if xyminmax is not None:\n",
    "                ax.set_ylim(xyminmax[ii])\n",
    "                ax.set_xlim(xyminmax[jj])\n",
    "            # else:\n",
    "            #     ax.set_ylim(data[:, ii].min(), data[:, ii].max() )\n",
    "            #     ax.set_xlim(data[:, jj].min(), data[:, jj].max() )\n",
    "                \n",
    "            if j == ny-1:\n",
    "                xti = ax.get_xticks()\n",
    "                xti = ['{:.2f}'.format(it) for it in xti]\n",
    "                ax.set_xticklabels(xti, rotation=30)\n",
    "\n",
    "            if i == 0:\n",
    "                xti = ax.get_yticks()\n",
    "                xti = ['{:.2f}'.format(it) for it in xti]\n",
    "                ax.set_yticklabels(xti)\n",
    "                \n",
    "    if color_arr_plt is not None:\n",
    "        cax = fig.add_axes([0.92, 0.25, 0.015, 0.5])\n",
    "        fig.colorbar(dummie_cax, cax=cax, label=color_label)\n",
    "\n",
    "    if savefig:\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "  \n",
    "\n",
    "def plot_grid_one_vs_other(data1, data2, \n",
    "                           labels1=None, labels2=None, \n",
    "                           errs1=None, errs2=None, \n",
    "                           color_arr=None, color_label=None, xyminmax=None, color_symmetric=False,\n",
    "              figsize=(8,8), spacing=0.05, s=30, alpha=1.0, cmap=plt.cm.viridis, markeredgecolor='k',\n",
    "              savefig=False, filename='plot_grid.pdf', rasterized=False,\n",
    "                          show_correlation=True, fontsize=12):\n",
    "    \"\"\"plot variables against each other. Similar to seaborns pairplot, but more functionality\"\"\"\n",
    "    nx = data1.shape[1]\n",
    "    ny = data2.shape[1]\n",
    "    print(nx, ny)\n",
    "    \n",
    "    if color_symmetric:\n",
    "        # Assume magnitude\n",
    "\n",
    "        cmin = -0.25\n",
    "        cmax = 0.25\n",
    "        color_arr[color_arr < cmin] = cmin\n",
    "        color_arr[color_arr > cmax] = cmax\n",
    "\n",
    "    if labels1 is None:\n",
    "        labels1 = ['z1_{:d}'.format(i) for i in range(data1.shape[1])]\n",
    "    if labels2 is None:\n",
    "        labels2 = ['z2_{:d}'.format(i) for i in range(data2.shape[1])]\n",
    "                \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = gridspec.GridSpec(ncols=nx, nrows=ny, figure=fig)\n",
    "\n",
    "    plt.subplots_adjust(wspace=spacing, hspace=spacing)\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.cm.viridis\n",
    "       \n",
    "    for i in range(nx):\n",
    "        for j in range(ny):\n",
    "            ax = fig.add_subplot(gs[j, i])\n",
    "            if color_arr is not None:\n",
    "                if i==0 and j==0:\n",
    "                    xx = np.linspace(color_arr.min(), color_arr.max(), 256)\n",
    "                    dummie_cax = ax.scatter(xx, xx, c=xx, cmap=cmap)\n",
    "                    # Clear axis\n",
    "                    ax.cla()\n",
    "\n",
    "            for axis in ['top','bottom','left','right']:\n",
    "                ax.spines[axis].set_linewidth(1.)\n",
    "\n",
    "            ax.tick_params('both', length=6, width=1., which='major', color='k', direction='in')                                         \n",
    "            ax.tick_params('both', length=3, width=1., which='minor', color='k', direction='in')\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(labels2[j])\n",
    "            else:\n",
    "                ax.set_yticklabels([])\n",
    "            \n",
    "            if j == ny-1:\n",
    "                ax.set_xlabel(labels1[i])\n",
    "            else:\n",
    "                ax.set_xticklabels([])\n",
    "\n",
    "            if color_arr is None:\n",
    "                if errs1 is None:\n",
    "                    ax.scatter(data1[:, i], data2[:, j], s=s, alpha=alpha, rasterized=rasterized)\n",
    "                else:\n",
    "                    ax.errorbar(data1[:, i], data2[:, j], xerr=errs1[:, i], yerr=errs2[:, j], marker='o', \n",
    "                                ms=s, alpha=alpha, markeredgecolor=markeredgecolor,\n",
    "                                rasterized=rasterized, ls='none')\n",
    "\n",
    "            else:\n",
    "                if errs1 is None:\n",
    "                    c = ax.scatter(data1[:, i], data2[:, j], s=s, c=color_arr, edgecolors='k', lw=0.25, alpha=alpha, cmap=cmap, rasterized=rasterized)\n",
    "                else:\n",
    "                    carr = (color_arr-color_arr.min())/(color_arr.max()-color_arr.min())\n",
    "                    # carr = color_arr\n",
    "                    for ierr in range(data1[:, i].shape[0]):\n",
    "                        ax.errorbar(data1[ierr, i], data2[ierr, j], xerr=errs1[ierr, i], yerr=errs2[ierr, j], markeredgecolor=markeredgecolor,\n",
    "                                    color=cmap(carr[ierr]), marker='o', ms=s, alpha=alpha, rasterized=rasterized, ls='none', \n",
    "                                     lw=2)\n",
    "\n",
    "            if show_correlation:\n",
    "                r, _ = pearsonr(data1[:, i], data2[:, j])\n",
    "                ax.text(0.96, 0.96, f\"r={r:.2f}\",\n",
    "                        horizontalalignment='right',\n",
    "                        verticalalignment='top',\n",
    "                        transform=ax.transAxes,\n",
    "                        path_effects=[pe.Stroke(linewidth=3, foreground='w'), pe.Normal()],\n",
    "                       fontsize=fontsize)\n",
    "\n",
    "            if xyminmax is not None:\n",
    "                ax.set_ylim(xyminmax[ii])\n",
    "                ax.set_xlim(xyminmax[jj])\n",
    "                \n",
    "            if j == ny-1:\n",
    "                xti = ax.get_xticks()\n",
    "                xti = ['{:.2f}'.format(it) for it in xti]\n",
    "                ax.set_xticklabels(xti, rotation=40)\n",
    "\n",
    "            if i == 0:\n",
    "                xti = ax.get_yticks()\n",
    "                xti = ['{:.2f}'.format(it) for it in xti]\n",
    "                ax.set_yticklabels(xti)\n",
    "                                \n",
    "            # if j == ny-1:\n",
    "            #     ax.set_xticklabels(ax.get_xticks(), rotation=-15)\n",
    "                \n",
    "    if color_arr is not None:\n",
    "        # cax = fig.add_axes([0.98, 0.25, 0.01, 0.5])\n",
    "        # fig.colorbar(dummie_cax, cax=cax, label=color_label)\n",
    "        cax = fig.add_axes([0.98, 0.25, 0.01, 0.5])\n",
    "        fig.colorbar(dummie_cax, cax=cax, label=color_label)\n",
    "\n",
    "    if savefig:\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corner Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savefig = True\n",
    "savefig = False\n",
    "\n",
    "labels = ['$\\Delta p$', '$\\Delta M$', '$\\Delta A_V$'] + ['$z_{:d}$'.format(i+1) for i in range(latent_dim)]\n",
    "print(test_data['z_samples'].shape)\n",
    "\n",
    "z_use = test_data['z_latent'].copy()\n",
    "z_use[:,0]*=50\n",
    "\n",
    "# dm = get_train_mask(train_data, params)\n",
    "# inds = np.argsort(test_data['redshift'])[::-1]\n",
    "inds = np.arange(test_data['redshift'].shape[0])\n",
    "\n",
    "inds = [2, 38] #43]#2,24]\n",
    "\n",
    "# plot_grid(z_use, labels=labels)\n",
    "for iplt in range(len(inds)):\n",
    "    print(test_data['names'][inds[iplt]])\n",
    "    print(inds[iplt], test_data['z_samples'].shape)\n",
    "    print(test_data['redshift'][inds[iplt]])\n",
    "    print(test_data['amplitude_mcmc'][inds[iplt]], test_data['amplitude_mcmc_err'][inds[iplt]])\n",
    "    print(np.mean(test_data['z_samples'][:, inds[iplt], :], axis=0), np.std(test_data['z_samples'][:, inds[iplt], :], axis=0))\n",
    "    \n",
    "    zplt = test_data['z_samples'][:, inds[iplt], :].copy()\n",
    "    zplt[:, 0] *= 50\n",
    "    \n",
    "    labelsi = np.roll(labels, -1)\n",
    "    zplt  = np.roll(zplt, -1, axis=1)\n",
    "\n",
    "    plot_grid(zplt, \n",
    "              labels=labelsi, \n",
    "              figsize=(12,12),  s=10, include_mean=True, ms_mean=8,\n",
    "              plot_density=True, savefig=savefig, filename='../figures/HMC_chains_density_{:03d}_latentdim{:02d}.pdf'.format(inds[iplt], latent_dim),\n",
    "             cmap=plt.cm.Blues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig = False\n",
    "# savefig = True\n",
    "\n",
    "filename = '../figures/grid_PAE_{:d}Dlatent.pdf'.format(latent_dim)\n",
    "\n",
    "data = {}\n",
    "data['z_latent_mcmc'] = np.concatenate((train_data['z_latent_mcmc'], test_data['z_latent_mcmc']), axis=0)\n",
    "data['z_latent_mcmc_err'] = np.concatenate((train_data['z_latent_mcmc_err'], test_data['z_latent_mcmc_err']), axis=0)\n",
    "\n",
    "data['z_latent_mcmc'][:,0] *= 50 #days\n",
    "data['z_latent_mcmc_err'][:,0]  *= 50 #days\n",
    "\n",
    "data['redshift'] = np.concatenate((train_data['redshift'], test_data['redshift']), axis=0)\n",
    "data['times_orig'] = np.concatenate((train_data['times_orig'], test_data['times_orig']), axis=0)\n",
    "\n",
    "labels = ['$\\Delta p$', '$\\Delta M$', '$\\Delta A_V$'] + ['$z_{:d}$'.format(i+1) for i in range(latent_dim)]\n",
    "print(labels)\n",
    "\n",
    "dm = get_train_mask(data, params)\n",
    "\n",
    "arr_use = data['z_latent_mcmc'][dm]\n",
    "err_use = data['z_latent_mcmc_err'][dm]\n",
    "\n",
    "labels = np.roll(labels, -1)\n",
    "arr_use = np.roll(arr_use, -1, axis=1)\n",
    "err_use = np.roll(err_use, -1, axis=1)\n",
    "\n",
    "plot_grid(arr_use, errs=err_use, \n",
    "          labels=labels, savefig=savefig, filename=filename,\n",
    "          color_arr=arr_use[:, 0], color_label='Magnitude Residual', color_symmetric=True,\n",
    "         figsize=(8,8), cmap=cmap_scatter, s=5, alpha=0.8, markeredgecolor='none') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple spectra plots to make sure model is doing what it should be\n",
    "for iplt in range(5):\n",
    "    plt.figure()\n",
    "    plt.plot(train_data['spectra'][iplt,0], label='obs')\n",
    "    plt.plot(train_data['spectra_ae'][iplt,0], label='AE')\n",
    "    plt.plot(train_data['spectra_map'][iplt,0], label='MAP')\n",
    "    plt.plot(train_data['spectra_mcmc'][iplt,0], label='MCMC')#*10**(-0.4*-0.34430096))\n",
    "\n",
    "    plt.legend()\n",
    "    \n",
    "print(train_data['z_latent'].mean(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play around with synthetic photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sncosmo\n",
    "\n",
    "def obs_from_rf_factor(z): #, cosmo=COSMO):\n",
    "    return z\n",
    "\n",
    "    # factor = 1/1e15\n",
    "    # factor = factor*(1.05)/(1+z)\n",
    "    # dlref = cosmo.luminosity_distance(0.05).value\n",
    "    # dl = cosmo.luminosity_distance(z).value\n",
    "    # factor = factor * dlref**2 / dl**2\n",
    "    # return factor\n",
    "\n",
    "def lc_from_spectra(band, fluxes, z, phases, spec_ids, flux_errs=None):\n",
    "    lc = []\n",
    "    for i, (phase, spec_id) in enumerate(zip(phases, spec_ids)):\n",
    "        fac = obs_from_rf_factor(z)\n",
    "        flux = fluxes[spec_id] * fac\n",
    "        if flux_errs is not None:\n",
    "            fluxerr = flux_errs[spec_id] * fac\n",
    "        else:\n",
    "            fluxerr = None\n",
    "        spec = sncosmo.Spectrum(wave=obs_wave*(1+z), flux=flux, fluxerr=fluxerr, time=phase)\n",
    "        lc.append(spec.bandfluxcov(band))\n",
    "    return lc\n",
    "\n",
    "dm = 0.\n",
    "av = 0.\n",
    "xi1 = 0.\n",
    "xi2 = 0.\n",
    "xi3 = 0.\n",
    "z = 0.05\n",
    "\n",
    "filter_edges = {'u': np.array([3302., 4102.]),\n",
    "                'b': np.array([4102., 5100.]),\n",
    "                'v': np.array([5200., 6289.]),\n",
    "                'r': np.array([6289., 7607.]),\n",
    "                'i': np.array([7607., 8598.])}\n",
    "\n",
    "model = sncosmo.Model(source='salt2')\n",
    "model.set(t0=0, z=z)\n",
    "model.wavelengths = train_data['wavelengths']\n",
    "delta_dm = -2.5*np.log10(obs_from_rf_factor(z))\n",
    "model_phases = np.linspace(-10, 40, 100)\n",
    "\n",
    "wavelength = [4000., 5000.]\n",
    "transmission = [1., 1.]\n",
    "\n",
    "print(model)\n",
    "iplt = 0\n",
    "nspec = 10\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for band_name, band_edges in filter_edges.items():\n",
    "    print(band_edges)\n",
    "    band = sncosmo.Bandpass(band_edges, band_edges*0+1, name=band_name)\n",
    "    sncosmo.register(band, force=True)\n",
    "    \n",
    "        \n",
    "    phases = train_data['times_orig'][iplt, :nspec, 0]\n",
    "    model.flux = train_data['spectra'][iplt, :nspec]\n",
    "    model.phases = phases\n",
    "\n",
    "    bandflux = model.bandflux(band_name, phases)\n",
    "    print(phases.shape, model.flux.shape, bandflux.shape)\n",
    "    plt.plot(phases, bandflux, label=band_name)\n",
    "    \n",
    "plt.legend(ncol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as pe\n",
    "import sncosmo\n",
    "\n",
    "# Plot SN while varying z latent params to see effects\n",
    "\n",
    "savefig = True\n",
    "# savefig = False\n",
    "\n",
    "figdir = '../figures/'\n",
    "\n",
    "# Set up figure aesthetics\n",
    "latent = ['$z_{:d}$'.format(i+1) for i in range(train_data['z_latent'].shape[1]-3)]\n",
    "labels = ['$\\Delta t$', '$\\Delta M$', '$\\Delta A_V$'] + latent\n",
    "\n",
    "\n",
    "alpha=0.8\n",
    "dim_plus_2 = 1 #2\n",
    "lw = 2\n",
    "# Get observation times and bin latent params\n",
    "dm = get_train_mask(train_data, params)\n",
    "z_latent_use = train_data['z_latent_mcmc']\n",
    "\n",
    "nsamples = 15\n",
    "percentiles = np.linspace(10, 90, nsamples)\n",
    "z_bin_edge = np.percentile(z_latent_use, percentiles, axis=0)\n",
    "nsamples = z_bin_edge.shape[0]\n",
    "\n",
    "Asamp = np.linspace(0.5, 1.5, nsamples)\n",
    "\n",
    "nshow = 6 #3\n",
    "times_lin = np.linspace(0,1, params['n_timestep'])[None, ...]\n",
    "mask_     = np.ones((1, params['n_timestep'], 288))\n",
    "\n",
    "times_orig_lin = times_lin*50 - 10\n",
    "\n",
    "print(times_lin, times_orig_lin)\n",
    "\n",
    "cmaps = [cmap_spectra] * 5\n",
    "# cmaps = [plt.cm.Purples, plt.cm.Blues, plt.cm.Greens, plt.cm.Oranges, plt.cm.Reds]\n",
    "\n",
    "colors = [cmap(np.linspace(0.0,1,nsamples)) for cmap in cmaps]\n",
    "\n",
    "z_median = np.median(z_latent_use[dm], axis=0)[None, ...]\n",
    "\n",
    "# get u array of zeros\n",
    "nbands = 5\n",
    "filter_edges = {'u': np.array([3302., 4102.]),\n",
    "                'b': np.array([4102., 5100.]),\n",
    "                'v': np.array([5200., 6289.]),\n",
    "                'r': np.array([6289., 7607.]),\n",
    "                'i': np.array([7607., 8598.])}\n",
    "\n",
    "ntime_splits = 3\n",
    "\n",
    "for dim in range(3, z_bin_edge.shape[1]):\n",
    "\n",
    "    aoffsets = [0.75, 1.5]\n",
    "    zsamp = z_bin_edge[:, dim]\n",
    "        \n",
    "    title = 'Varying {:s}'.format(labels[dim])\n",
    "\n",
    "    fig, ((ax1)) = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(8,6))#, gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax1.spines[axis].set_linewidth(1.)\n",
    "\n",
    "    ax1.tick_params('both', length=6, width=1., which='major', color='k',direction='in')                                         \n",
    "    ax1.tick_params('both', length=3, width=1., which='minor', color='k',direction='in')\n",
    "\n",
    "    ymin = 100\n",
    "    ymax = -100\n",
    "\n",
    "    # get origin value\n",
    "    z_ = z_median.copy()\n",
    "\n",
    "    for sample in range(z_bin_edge.shape[0]):\n",
    "\n",
    "        # set u values along slice\n",
    "        z_ = z_median.copy()\n",
    "        z_[:, dim:dim+dim_plus_2] = zsamp[sample]\n",
    "\n",
    "        bandfluxes = np.zeros((nbands, params['n_timestep']*ntime_splits))\n",
    "        phases = []\n",
    "\n",
    "        isamp = 0\n",
    "        for itime_mult in range(1, ntime_splits+1):\n",
    "            # make time difference smaller by factor of this in order to more smoothly interpolate\n",
    "            times_lin_use = times_lin / ntime_splits + (itime_mult-1.)/ntime_splits\n",
    "            phases.extend(list(times_lin_use[0]))\n",
    "            # times_lin_use = np.clip(times_lin_use, 1./50, 1-1./50)\n",
    "\n",
    "            xpred_i = np.array(PAE.decoder((z_, times_lin_use, mask_))[0]) #/ (10**(-0.4* (CL*np.exp(z_[:, -1]) + z_[:, 0])))\n",
    "\n",
    "            for ispec in range(params['n_timestep']):\n",
    "\n",
    "                spec = sncosmo.Spectrum(wave=train_data['wavelengths'], flux=xpred_i[ispec])\n",
    "                for iband, (band_name, band_edges) in enumerate(filter_edges.items()):\n",
    "                    band = sncosmo.Bandpass(band_edges, band_edges*0+1, name=band_name)\n",
    "                    sncosmo.register(band, force=True)\n",
    "\n",
    "                    bandflux = spec.bandflux(band_name)/1.e14\n",
    "                    \n",
    "                    bandfluxes[iband, isamp] = bandflux\n",
    "                \n",
    "                    \n",
    "                isamp += 1\n",
    "\n",
    "        for iband in range(bandfluxes.shape[0]):\n",
    "            ax1.plot(np.array(phases)*50-10, bandfluxes[iband]-iband/3*2, c=colors[iband][sample], lw=lw, alpha=alpha)\n",
    "            \n",
    "    for iband, (band_name, band_edges) in enumerate(filter_edges.items()):\n",
    "        ax1.text(37.5,  bandfluxes[iband, -1]-iband/3*2+0.2, band_name, ha='center')\n",
    "#         for i in range(nshow):\n",
    "#             labp = None\n",
    "#             labo = None\n",
    "#             if i==0:\n",
    "#                 labs = 'Encoder'\n",
    "#                 labp = 'MAP'\n",
    "#             if plttype == 'normal':\n",
    "#                 xpi = xpred_i[i] + offset[i]\n",
    "                \n",
    "                \n",
    "#         ax1.plot(train_data['wavelengths'], xpi, '-',\n",
    "#                  c=colors[sample], lw=lw, alpha=alpha,\n",
    "#                 path_effects=[pe.Stroke(linewidth=lw+1, foreground='w'), pe.Normal()],\n",
    "#                 )#, label=labp)\n",
    "\n",
    "  \n",
    "    ax1.set_ylabel('Normalized Flux')\n",
    "    ax1.set_xlabel('Phase [days]')\n",
    "\n",
    "    # ymax = offset[0] + aoffsets[iplt] \n",
    "    # ymin = offset[nshow-1]\n",
    "    # ax1.set_title(title, fontsize=20, y=0.97, pad=0)\n",
    "    ax1.set_title(\"Synthetic Photometry\", fontsize=20, y=0.97, pad=0)\n",
    "\n",
    "\n",
    "    \n",
    "    ax1.set_frame_on(False)\n",
    "    ax1.get_xaxis().tick_bottom()\n",
    "    ax1.axes.get_yaxis().set_visible(False)\n",
    "    xmin, xmax = ax1.get_xaxis().get_view_interval()\n",
    "    ymin, ymax = ax1.get_yaxis().get_view_interval()\n",
    "    print(ymin,ymax)\n",
    "    ax1.add_artist(plt.Line2D((xmin, xmax), (ymin, ymin), color='black', linewidth=2))\n",
    "\n",
    "    ax1.legend(loc='upper right', frameon=False, ncol=3)\n",
    "    ax1.set_ylim(-3., 1.2)\n",
    "\n",
    "\n",
    "#     ax1.axis('off')\n",
    "\n",
    "#     ax2.set_yscale('log')\n",
    "    # ax1.set_xlim(train_data['wavelengths'][0], train_data['wavelengths'][-1])\n",
    "#     plt.subplots_adjust(hspace=0.025)\n",
    "\n",
    "    if savefig: \n",
    "        fname = 'vary_zlatent_lc_{:02d}Dlatent_dim{:d}'.format(latent_dim, dim)\n",
    "        if dim_plus_2 > 1:\n",
    "            fname = 'vary_zlatent_lc_dimplus2_{:02d}Dlatent_dim{:d}'.format(latent_dim, dim)\n",
    "\n",
    "        plt.savefig(figdir+fname+'.pdf', bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of SALT2 time uncertainty and PAE \\Delta p parameter\n",
    "dm = get_train_mask(train_data, params)\n",
    "\n",
    "plt.scatter(train_data['dphase'][dm], train_data['z_latent_mcmc'][dm, 0])\n",
    "plt.xlabel(\"SALT2 phase uncertainty\")\n",
    "plt.ylabel(\"PAE $\\Delta p$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plot spectra and model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def frac_to_mag(fractional_difference):\n",
    "    \"\"\"Convert a fractional difference to a difference in magnitude\n",
    "    Because this transformation is asymmetric for larger fractional changes, we\n",
    "    take the average of positive and negative differences.\n",
    "    \n",
    "    Copied from https://github.com/snfactory/twins_embedding\n",
    "    \"\"\"\n",
    "    pos_mag = 2.5 * np.log10(1. + fractional_difference)\n",
    "    neg_mag = 2.5 * np.log10(1. - fractional_difference)\n",
    "    mag_diff = (pos_mag - neg_mag) / 2.0\n",
    "    return mag_diff\n",
    "\n",
    "# Fit for the intrinsic dispersion\n",
    "def gaussian_likelihood(residuals, measurement_uncertainties, intrinsic_dispersion):\n",
    "    variance = measurement_uncertainties**2 + intrinsic_dispersion**2\n",
    "    return 1 / np.sqrt(2 * np.pi * variance) * np.exp(-residuals**2 / 2 / variance)\n",
    "\n",
    "def negative_log_likelihood(x):\n",
    "    likelihoods = gaussian_likelihood(residuals, measurement_uncertainties, x[0])\n",
    "    return -np.sum(np.log(likelihoods))\n",
    "\n",
    "def compute_intrinsic_dispersion(spec_true, spec_pred, sigma,  weighted=False, outlier_cut=98):\n",
    "    \"\"\"Calculate std of a true and reconstructed spectra                                     \n",
    "                                                                                                                    \n",
    "    Parameters                                                                                                      \n",
    "    ----------                                                                                                      \n",
    "    spec_true: array (N_sn, n_timesamples, data_dim)                                                                \n",
    "       measured spectra                                                                                             \n",
    "    spec_pred: array (N_sn, n_timesamples, data_dim)                                                                \n",
    "       model spectra                                                                                                \n",
    "    sigma: array (N_sn, n_timesamples, data_dim)                                                                    \n",
    "       measurement uncertainty                                                                                      \n",
    "    time: array (N_sn, n_timesamples)                                                                               \n",
    "       observation time                                                                                             \n",
    "    \"\"\"\n",
    "   \n",
    "    residuals = np.abs(spec_pred.copy() - spec_true.copy())\n",
    "    \n",
    "    # sometimes there are huge outliers,                                                                            \n",
    "    # so set errors larger than some percentile to the value corresponding to that percentile        \n",
    "    intrinsic_dispersion = np.zeros(residuals.shape[0])\n",
    "    \n",
    "    for wbin in range(residuals.shape[0]):\n",
    "        measurement_uncertainty = sigma[wbin]\n",
    "        residual = residuals[wbin]\n",
    "\n",
    "        # Fit for the intrinsic dispersion\n",
    "        def gaussian_likelihood(residual, measurement_uncertainty, intrinsic_dispersion_i):\n",
    "            variance = measurement_uncertainty**2 + intrinsic_dispersion_i**2\n",
    "            return 1. / np.sqrt(2 * np.pi * variance) * np.exp(-residuals**2 / 2 / variance)\n",
    "\n",
    "        def negative_log_likelihood(x):\n",
    "            likelihoods = gaussian_likelihood(residual, measurement_uncertainty, x[0])\n",
    "            likelihoods += 1e-9\n",
    "            return -np.sum(np.log(likelihoods))\n",
    "        \n",
    "        if np.abs(residual) > measurement_uncertainty:\n",
    "            initial_guess = np.sqrt(residual**2 - measurement_uncertainty**2)\n",
    "\n",
    "            res = minimize(negative_log_likelihood, [initial_guess])\n",
    "            itrial = 0\n",
    "            while res.success is False and itrial < 10:\n",
    "                xini = np.abs(np.random.normal(initial_guess, measurement_uncertainty, 1))\n",
    "                res = minimize(negative_log_likelihood, xini)\n",
    "                itrial += 1\n",
    "\n",
    "            intrinsic_dispersion[wbin] = np.abs(res.x[0])\n",
    "        else:\n",
    "            intrinsic_dispersion[wbin] = 0\n",
    "    \n",
    "    return  intrinsic_dispersion\n",
    "\n",
    "\n",
    "def compute_intrinsic_dispersion_vs_time(spec_true, spec_pred, sigma, mask, time, weighted=False, outlier_cut=98, ntbins=10):\n",
    "    \"\"\"Calculate std of true and reconstructed spectra as a function of time                                        \n",
    "                                                                                                                    \n",
    "    Parameters                                                                                                      \n",
    "    ----------                                                                                                      \n",
    "    spec_true: array (N_sn, n_timesamples, data_dim)                                                                \n",
    "       measured spectra                                                                                             \n",
    "    spec_pred: array (N_sn, n_timesamples, data_dim)                                                                \n",
    "       model spectra                                                                                                \n",
    "    sigma: array (N_sn, n_timesamples, data_dim)                                                                    \n",
    "       measurement uncertainty                                                                                      \n",
    "    time: array (N_sn, n_timesamples)                                                                               \n",
    "       observation time                                                                                             \n",
    "    \"\"\"\n",
    "    dm = np.min(mask, axis=-1) == 1.\n",
    "    \n",
    "    t_bin_edge = np.linspace(0, 1, ntbins+1)\n",
    "    t_bin_cent = (t_bin_edge[:-1] + t_bin_edge[1:])/2\n",
    "    # print(t_bin_edge *50 -10)\n",
    "    s0 = spec_true[dm]\n",
    "    s1 = spec_pred[dm]\n",
    "\n",
    "    s0[s0 < 0] = 0\n",
    "    s1[s1 < 0] = 0\n",
    "    \n",
    "#     residual = -2.5*np.log10(s1/s0)\n",
    "#     sig = 2.5*np.log10(np.abs(sigma[dm])/s0)\n",
    "    \n",
    "    residual = spec_pred[dm].copy() - spec_true[dm].copy()\n",
    "    sig = sigma[dm].copy()\n",
    "\n",
    "    t   = time[dm][:,0]\n",
    "    \n",
    "    # print(sig.min(), np.abs(residual).min(), np.abs(residual/sig).min())\n",
    "    residual = np.reshape(residual, (-1, 288))\n",
    "    sig = np.reshape(sig, (-1, 288))\n",
    "    s0 = np.reshape(s0, (-1, 288))\n",
    "    s1 = np.reshape(s1, (-1, 288))\n",
    "\n",
    "    # print(sig.shape)\n",
    "    # sometimes there are huge outliers,                                                                            \n",
    "    # so set errors larger than some percentile to the value corresponding to that percentile        \n",
    "    intrinsic_dispersion = np.zeros((len(t_bin_cent), sig.shape[1]))\n",
    "    measurement_dispersion = np.zeros((len(t_bin_cent), sig.shape[1]))\n",
    "    mean_spectra = np.zeros((len(t_bin_cent), sig.shape[1]))\n",
    "\n",
    "    bins = np.digitize(t, t_bin_edge) - 1\n",
    "#     for tbin in range(1):\n",
    "    for tbin in np.unique(bins):\n",
    "        dm   = bins == tbin\n",
    "#         print(sig[dm].min(0), sig[dm].max(0))\n",
    "#         print(residual[dm].min(0), residual[dm].max(0))\n",
    "        mean_spectra[tbin] = s0[dm].mean(0)\n",
    "\n",
    "        for wbin in range(sig.shape[1]):\n",
    "            measurement_uncertainties = sig[dm][:, wbin]\n",
    "            residuals = residual[dm][:, wbin]\n",
    "            \n",
    "            # Fit for the intrinsic dispersion\n",
    "            def gaussian_likelihood(residuals, measurement_uncertainties, intrinsic_dispersion_i):\n",
    "                variance = measurement_uncertainties**2 + intrinsic_dispersion_i**2\n",
    "                return 1. / np.sqrt(2 * np.pi * variance) * np.exp(-residuals**2 / 2 / variance)\n",
    "\n",
    "            def negative_log_likelihood(x):\n",
    "                likelihoods = gaussian_likelihood(residuals, measurement_uncertainties, x[0])\n",
    "                likelihoods += 1e-9\n",
    "                return -np.sum(np.log(likelihoods))\n",
    "\n",
    "            mean_resid = np.mean(np.abs(residuals))\n",
    "#             print(mean_resid)\n",
    "            mean_meas = np.mean(measurement_uncertainties)\n",
    "            mean_resid = np.sqrt(np.sum(residuals**2)/residuals.shape[0])\n",
    "#             print(mean_resid)\n",
    "#             initial_guess = np.sqrt(mean_resid**2 - mean_meas**2)\n",
    "\n",
    "            if mean_meas < mean_resid:\n",
    "                initial_guess = np.sqrt(mean_resid**2 - mean_meas**2)\n",
    "            else: \n",
    "#                 print(mean_resid**2 - mean_meas**2)\n",
    "                initial_guess = 0.001\n",
    "    \n",
    "            res = minimize(negative_log_likelihood, [initial_guess])\n",
    "            itrial = 0\n",
    "            while res.success is False and itrial < 2500:\n",
    "                xini = np.abs(np.random.normal(initial_guess, np.mean(measurement_uncertainties), 1))\n",
    "                res = minimize(negative_log_likelihood, xini)\n",
    "                itrial += 1\n",
    "            intrinsic_dispersion[tbin, wbin] = np.abs(res.x[0])\n",
    "            measurement_dispersion[tbin, wbin] = np.mean(measurement_uncertainties)\n",
    "\n",
    "    return t_bin_cent, mean_spectra, measurement_dispersion, intrinsic_dispersion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import suPAErnova.utils.calculations as calculations\n",
    "import matplotlib.patheffects as pe\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.patheffects as pe\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "ncolor = 256\n",
    "neg_frac = 0.2 # time ranges from -10 to 40\n",
    "\n",
    "colors1_salt = plt.cm.Blues(np.linspace(0.4, 0.7, int(ncolor*neg_frac)))\n",
    "colors2_salt = plt.cm.Blues_r(np.linspace(0.3, 0.8, int(ncolor*(1-neg_frac))))\n",
    "\n",
    "# combine them and build a new colormap\n",
    "colors_salt = np.vstack((colors1_salt, colors2_salt))\n",
    "\n",
    "colors1_pae = plt.cm.Reds(np.linspace(0.4, 0.7, int(ncolor*neg_frac)))\n",
    "colors2_pae = plt.cm.Reds_r(np.linspace(0.3, 0.8, int(ncolor*(1-neg_frac))))\n",
    "\n",
    "# combine them and build a new colormap\n",
    "colors_pae = np.vstack((colors1_pae, colors2_pae))\n",
    "\n",
    "cmap_salt = LinearSegmentedColormap.from_list('my_colormap', colors_salt)\n",
    "cmap_pae = LinearSegmentedColormap.from_list('my_colormap', colors_pae)\n",
    "\n",
    "def plot_spectra_map(data, spec1='salt2',spec2='ae', nshow_interp=26, figsize=(10, 16),\n",
    "                     lwt=5., lw=1.5, lwpe=2, alpha=1.0, alpha_obs=0.5, alpha_fill=0.3, smooth_bins=7,\n",
    "                     ispec=0, train=True, savefig=False, tstr='train', relative_error=False):\n",
    "    \n",
    "    # Set up plot and aesthetics\n",
    "    title = data['names'][ispec]\n",
    "    # Set plot aesthetics                                                                                                 \n",
    "    aoffsets = [0.4, 0.15, 7]\n",
    "\n",
    "    cmap = plt.cm.coolwarm\n",
    "    colors = cmap(np.linspace(0, 1, 256))\n",
    "\n",
    "    min_sep_interp = 1.5 # minimum seperation of interpolated model and true data, in days\n",
    "    # c0  = 'C3'\n",
    "    # c1  = 'C0'\n",
    "    c0  = colors[int(0.9*len(colors))]\n",
    "    c1  = colors[int(0.1*len(colors))]    \n",
    "    \n",
    "    ### Get data (non-masked spectra)\n",
    "    spec1 = spec1.upper()\n",
    "    spec2 = spec2.upper()\n",
    "    dm = np.max(data['mask'][ispec], axis=-1) != 0.\n",
    "    x_obs = data['spectra'][ispec, dm].copy()\n",
    "    \n",
    "    if spec1=='SALT2':\n",
    "        x_1 = data['spectra_salt'][ispec, dm].copy()\n",
    "    if spec1=='AE':\n",
    "        x_1 = data['spectra_ae'][ispec, dm].copy()\n",
    "    if spec1=='MAP':\n",
    "        x_1 = data['spectra_map'][ispec, dm].copy()\n",
    "\n",
    "    if spec2=='SALT2':\n",
    "        x_2 = data['spectra_salt'][ispec, dm].copy()\n",
    "    if spec2=='AE':\n",
    "        x_2 = data['spectra_ae'][ispec, dm].copy()\n",
    "    if spec2=='MAP':\n",
    "        x_2 = data['spectra_map'][ispec, dm].copy()\n",
    "    if spec2=='PAE':\n",
    "        x_2 = data['spectra_mcmc'][ispec, dm].copy()\n",
    "\n",
    "    sigma  = data['sigma'][ispec, dm].copy()\n",
    "    times_orig  = data['times_orig'][ispec, dm].copy()\n",
    "    times  = data['times'][ispec, dm].copy()\n",
    "\n",
    "    wavelengths = data['wavelengths']\n",
    "    red = data['redshift'][ispec]\n",
    "\n",
    "    n_spectra = x_obs.shape[0]\n",
    "    print(\"Nspectra \", n_spectra)\n",
    "    \n",
    "    # get reconstruction uncertainty\n",
    "    if spec1!=\"SALT\": \n",
    "        if spec2==\"MAP\":\n",
    "            tuse = times + data['dtime_map'][ispec]\n",
    "        elif spec2==\"PAE\":\n",
    "            tuse = times + data['dtime_mcmc'][ispec]\n",
    "        else:\n",
    "            tuse = times\n",
    "            \n",
    "        sigma1 = tf.transpose(tfp.math.interp_regular_1d_grid(\n",
    "                x=tf.transpose(tuse[..., 0]),\n",
    "                x_ref_min=ae_noise_t_bin_cent[0],\n",
    "                x_ref_max=ae_noise_t_bin_cent[-1],\n",
    "                y_ref=data['sigma_ae_time'])) # fill_value='extrapolate')   \n",
    "\n",
    "    if spec2!=\"SALT\": \n",
    "        if spec2==\"MAP\":\n",
    "            tuse = times + data['dtime_map'][ispec]\n",
    "        elif spec2==\"PAE\":\n",
    "            tuse = times + data['dtime_mcmc'][ispec]\n",
    "        else:\n",
    "            tuse = times\n",
    "            \n",
    "        sigma2 = tf.transpose(tfp.math.interp_regular_1d_grid(\n",
    "                x=tf.transpose(tuse[..., 0]),\n",
    "                x_ref_min=ae_noise_t_bin_cent[0],\n",
    "                x_ref_max=ae_noise_t_bin_cent[-1],\n",
    "                y_ref=data['sigma_ae_time'])) # fill_value='extrapolate')   \n",
    "\n",
    "\n",
    "    # Turn off axis lines\n",
    "#         ax1.set_frame_on(False)\n",
    "#         ax1.get_xaxis().tick_bottom()\n",
    "#         ax1.axes.get_yaxis().set_visible(False)\n",
    "#         xmin, xmax = ax1.get_xaxis().get_view_interval()\n",
    "#         ymin, ymax = ax1.get_yaxis().get_view_interval()\n",
    "#         ax1.add_artist(plt.Line2D((xmin, xmax), (ymin, ymin), color='black', linewidth=2))\n",
    "\n",
    "    # set constant offset scale\n",
    "    #         offset = np.arange(len(times)) * -1 * aoffsets[iplt]\n",
    "    wave_txt = wavelengths[-1]+100\n",
    "    \n",
    "    fontsize = 14\n",
    "    \n",
    "    offset_scale = 10\n",
    "    \n",
    "    offset = -times * offset_scale + offset_scale\n",
    "\n",
    "    phase_text_loc = offset_scale+0.2\n",
    "    \n",
    "    ymin = -0.1\n",
    "    ymax = offset_scale + 1.0\n",
    "    # Plot spectra 1 by 1\n",
    "    toffset_min = 0.3\n",
    "  \n",
    "    toffset_prev = 0.0\n",
    "\n",
    "    ind_tmax = np.argmin(times_orig[:n_spectra])\n",
    "    \n",
    "    # Get interpolated model predictions at specified times\n",
    "    # First get spectra from latent variables at linear spaced times between -10 and 40\n",
    "    times_interp = np.zeros((1, params['n_timestep'])).astype(np.float32) \n",
    "    times_interp[:, :nshow_interp] = np.linspace(0, 1, nshow_interp)\n",
    "    mask_interp    = np.ones((1, params['n_timestep'], 288))\n",
    "    mask_interp[:, nshow_interp:] = 0\n",
    "    offset_interp = -times_interp * offset_scale + offset_scale\n",
    "\n",
    "    times_orig_interp = times_interp*50 - 10\n",
    "    # times_interp[0,0] += 1./50\n",
    "    # times_interp[0, nshow_interp-1] -= 1./50\n",
    "\n",
    "    spectra_interp = PAE.decoder((data['z_latent_mcmc'][ispec:ispec+1], times_interp, mask_interp))[0] \n",
    "\n",
    "    dm = mask_interp[0, ..., 0] == 1\n",
    "    dm_time = [np.min(np.abs(times_orig[:, 0] - i)) > min_sep_interp for i in times_orig_interp[0]] # remove closer than this to observed spectra\n",
    "    dm = dm & dm_time\n",
    "\n",
    "    spectra_interp = spectra_interp[dm]\n",
    "    times_interp = times_interp[0, dm]\n",
    "    times_orig_interp = times_orig_interp[0, dm]\n",
    "    offset_interp = offset_interp[0, dm]\n",
    "\n",
    "    sigma_interp = tf.transpose(\n",
    "        tfp.math.interp_regular_1d_grid(\n",
    "            x=tf.transpose(times_interp),\n",
    "            x_ref_min=ae_noise_t_bin_cent[0],\n",
    "            x_ref_max=ae_noise_t_bin_cent[-1],\n",
    "            y_ref=data['sigma_ae_time'],\n",
    "        ),\n",
    "    ) # fill_value='extrapolate')   \n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=3, ncols=1, figsize=figsize,\n",
    "        gridspec_kw={'height_ratios': [8, 1, 1]},\n",
    "    )                                  \n",
    "    fig.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for ax in axs:\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(1.5)\n",
    "\n",
    "        ax.tick_params('both', length=6, width=1.5, which='major', color='k',direction='in')\n",
    "        ax.tick_params('both', length=3, width=1.5, which='minor', color='k',direction='in')\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        \n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        \n",
    "        ax.set_xlim(wavelengths[0], wavelengths[-1])\n",
    "\n",
    "    axs[0].get_xaxis().set_ticks([])\n",
    "    axs[1].get_xaxis().set_ticks([])\n",
    "\n",
    "    axs[0].set_ylabel('Normalized Restframe Flux + const.') \n",
    "    axs[1].set_ylabel('Ratio')\n",
    "    axs[2].set_ylabel('Ratio')\n",
    "    axs[2].set_xlabel('Wavelength [$\\AA$]')\n",
    "\n",
    "    axs[0].set_title(title, fontsize=18)   \n",
    "    \n",
    "    # Plot interpolated model spectra\n",
    "    for i in range(spectra_interp.shape[0]):\n",
    "        c0 = cmap_pae(times_interp[i])\n",
    "\n",
    "        xi = spectra_interp[i] + offset_interp[i]\n",
    "        axs[0].fill_between(\n",
    "            wavelengths,\n",
    "            xi - spectra_interp[i]*sigma_interp[i],  \n",
    "            xi + spectra_interp[i]*sigma_interp[i],\n",
    "            color=c0, alpha=alpha_fill, lw=0)\n",
    "           \n",
    "\n",
    "        axs[0].plot(wavelengths, spectra_interp[i] + offset_interp[i], '-', c=c0, lw=lw, alpha=alpha)#,\n",
    "\n",
    "        phase_label = f\"+{times_orig_interp[i]:.1f}\" if times_orig_interp[i] > 0 else f\"{times_orig_interp[i]:.1f}\"\n",
    "        axs[0].text(wave_txt, offset_interp[i]+(xi[-1]-offset_interp[i]), phase_label, fontsize=fontsize, ha='left', va='center')\n",
    "\n",
    "    # Plot observations and matching models\n",
    "           \n",
    "    axs[0].plot(wavelengths, wavelengths*-10, lw=2, color= 'k', alpha=0.9, label=\"Observation\")\n",
    "    axs[0].plot(wavelengths, wavelengths*-10, lw=2, color= cmap_salt(0.2), alpha=0.9, label=\"SALT2\")\n",
    "    axs[0].plot(wavelengths, wavelengths*-10, lw=2, color= cmap_pae(0.2), alpha=0.9, label=\"PAE\")\n",
    "\n",
    "    for i in range(n_spectra):\n",
    "        \n",
    "        if i > 0 and abs(times_orig[i][0]-times_orig[i-1][0]) < 0.5:\n",
    "            # Don't plot spectra that are on top of each other\n",
    "            continue\n",
    "        c1 = cmap_salt(times[i][0])\n",
    "        c0 = cmap_pae(times[i][0])\n",
    "        \n",
    "        labp = None\n",
    "        labs = None\n",
    "        labo = None\n",
    "        ls= '-'\n",
    "\n",
    "        toffset_i = offset[i]\n",
    "        dt_add = 0\n",
    "        if np.abs(toffset_i - toffset_prev) < toffset_min and i >0:\n",
    "            dt_add = np.abs(toffset_i - toffset_prev)-toffset_min\n",
    "\n",
    "\n",
    "        xi = x_obs[i] + offset[i]\n",
    "        x1i = x_1[i] + offset[i]\n",
    "        x2i = x_2[i] + offset[i]\n",
    "\n",
    "        #### Plot top panel (spectra)\n",
    "        axs[0].plot(wavelengths, xi, '-', c='k', lw=lwt, alpha=alpha_obs, label=labo)#colors[i])                       \n",
    "        axs[0].fill_between(wavelengths, xi-sigma[i],  xi+sigma[i], color='gray', alpha=alpha_fill, lw=1)\n",
    "\n",
    "        if spec1!=\"SALT2\": \n",
    "            if relative_error:\n",
    "                axs[0].fill_between(wavelengths, x1i-x_1[i]*sigma1[i],  x1i+x_2[i]*sigma1[i], color=c1, alpha=alpha_fill, lw=0)\n",
    "            else:\n",
    "                axs[0].fill_between(wavelengths, x1i-sigma1[i],  x1i+sigma1[i], color=c1, alpha=alpha_fill, lw=0)\n",
    "\n",
    "        if spec2!=\"SALT2\": \n",
    "            if relative_error:\n",
    "                axs[0].fill_between(wavelengths, x2i-x_2[i]*sigma2[i],  x2i+x_2[i]*sigma2[i], color=c0, alpha=alpha_fill, lw=0)\n",
    "            else:\n",
    "                axs[0].fill_between(wavelengths, x2i-sigma2[i],  x2i+sigma2[i], color=c0, alpha=alpha_fill, lw=0)\n",
    "\n",
    "        axs[0].plot(wavelengths, x1i, '-', c=c1, lw=lw, alpha=alpha, label=labs,\n",
    "                path_effects=[pe.Stroke(linewidth=lwpe, foreground='w'), pe.Normal()])\n",
    "        axs[0].plot(wavelengths, x2i, ls=ls, c=c0, lw=lw, alpha=alpha, label=labp,\n",
    "                path_effects=[pe.Stroke(linewidth=lwpe, foreground='w'), pe.Normal()])\n",
    "\n",
    "        #### Plot bottom ratio panel\n",
    "        # axs[1].plot(wavelengths, xi*0+1, '-', c='k', lw=lwt, alpha=0.8, label=labo)#colors[i])                       \n",
    "        # axs[1].fill_between(wavelengths, (x_obs[i]-sigma[i])/x_obs[i],  (x_obs[i]+sigma[i])/x_obs[i], color='gray', alpha=0.8, lw=1)\n",
    "\n",
    "        if spec1!=\"SALT2\": \n",
    "            if relative_error:\n",
    "                axs[1].fill_between(\n",
    "                    wavelengths,\n",
    "                    (x_1[i]-x_1[i]*sigma1[i])/x_obs[i],\n",
    "                    (x_1[i]+x_1[i]*sigma1[i])/x_obs[i],\n",
    "                    color=c1,\n",
    "                    alpha=alpha_fill,\n",
    "                    lw=0,\n",
    "                    zorder=-abs(ind_tmax - i),\n",
    "                )\n",
    "            else:\n",
    "                axs[1].fill_between(wavelengths, x1i-sigma1[i],  x1i+sigma1[i], color=c1, alpha=alpha_fill, lw=0)\n",
    "\n",
    "        if spec2!=\"SALT2\": \n",
    "            l = (x_2[i]+x_2[i]*sigma1[i])/x_obs[i]\n",
    "            u = (x_2[i]-x_2[i]*sigma1[i])/x_obs[i]\n",
    "            \n",
    "            l = savgol_filter(l, smooth_bins, 1)\n",
    "            u = savgol_filter(u, smooth_bins, 1)\n",
    "                    \n",
    "            if relative_error:\n",
    "                axs[2].fill_between(\n",
    "                    wavelengths,\n",
    "                    l,\n",
    "                    u,\n",
    "                    color=c0,\n",
    "                    alpha=0.1,\n",
    "                    lw=0,\n",
    "                    zorder=-abs(ind_tmax - i),\n",
    "                )\n",
    "            else:\n",
    "                axs[2].fill_between(wavelengths, x2i-sigma2[i],  x2i+sigma2[i], color=c0, alpha=alpha_fill, lw=0)\n",
    "\n",
    "        sig1 = x_1[i]/x_obs[i]\n",
    "        sig2 = x_2[i]/x_obs[i]\n",
    "        \n",
    "        # sig1 = np.abs(x_1[i]-x_obs[i])/sigma[i]\n",
    "        # sig2 = np.abs(x_2[i]-x_obs[i])/sigma[i]\n",
    "\n",
    "#         resid1 = np.abs(x_1[i] - x_obs[i])\n",
    "#         sig1 = np.zeros(resid1.shape[0])\n",
    "#         mask = resid1**2 > sigma[i]**2\n",
    "#         sig1[mask] = frac_to_mag(np.sqrt(resid1[mask]**2 - sigma[i][mask]**2))\n",
    "    \n",
    "#         resid2 = np.abs(x_2[i] - x_obs[i])\n",
    "#         sig2 = np.zeros(resid2.shape[0])\n",
    "#         mask = resid2**2 > sigma[i]**2\n",
    "#         sig2[mask] = frac_to_mag(np.sqrt(resid2[mask]**2 - sigma[i][mask]**2))\n",
    "    \n",
    "        sig1 = savgol_filter(sig1, smooth_bins, 1)\n",
    "        sig2 = savgol_filter(sig2, smooth_bins, 1)\n",
    "        \n",
    "        axs[1].plot(wavelengths, sig1, '-', c=c1, lw=lw, alpha=alpha, label=labs, zorder=-abs(ind_tmax - i),\n",
    "                path_effects=[pe.Stroke(linewidth=lwpe, foreground='w'), pe.Normal()])\n",
    "        axs[2].plot(wavelengths, sig2, ls=ls, c=c0, lw=lw, alpha=alpha, label=labp, zorder=-abs(ind_tmax - i),\n",
    "                path_effects=[pe.Stroke(linewidth=lwpe, foreground='w'), pe.Normal()])        \n",
    "        \n",
    "\n",
    "        # add phase in days\n",
    "        if i==0:\n",
    "            axs[0].text(wave_txt+200, phase_text_loc, 'Phase\\n[days]', fontsize=fontsize, ha='center', va='bottom')\n",
    "\n",
    "        phase_label = f\"+{times_orig[i][0]:.1f}\" if times_orig[i][0] > 0 else f\"{times_orig[i][0]:.1f}\"\n",
    "        axs[0].text(wave_txt, offset[i]+(xi[-1]-offset[i])+dt_add, phase_label, fontsize=fontsize, ha='left', va='center')\n",
    "\n",
    "        # ymin = min(ymin, min(xi), min(x1i), min(x2i) )\n",
    "        # ymax = max(ymax, max(xi), max(x1i), max(x2i) )\n",
    "\n",
    "    axs[1].plot(wavelengths, xi*0+1, '-', c='k', lw=2, alpha=0.7, label=labo)#colors[i])                       \n",
    "    axs[2].plot(wavelengths, xi*0+1, '-', c='k', lw=2, alpha=0.7, label=labo)#colors[i])   \n",
    "    \n",
    "    axs[1].text(wavelengths[int(len(wavelengths)*3/4)], 1.6, \"SALT2\", ha='center')\n",
    "    axs[2].text(wavelengths[int(len(wavelengths)*3/4)], 1.6, \"PAE\", ha='center')\n",
    "\n",
    "    axs[0].legend(loc='upper center', bbox_to_anchor=(0.5, 1.01),\n",
    "               frameon=False, ncol=3, columnspacing=1., handletextpad=0.5, handlelength=1.5)\n",
    "\n",
    "    axs[0].set_ylim(ymin, ymax)\n",
    "    axs[1].set_ylim(0.5, 2.0)\n",
    "    axs[2].set_ylim(0.5, 2.0)\n",
    "\n",
    "    # axs[1].set_yscale('log')\n",
    "\n",
    "    toffset_prev = offset[i]\n",
    "\n",
    "    if savefig:\n",
    "        # plt.savefig('../figures/spectra_fit_{:02d}latentdim_{:s}_{:s}_{:s}_{:s}_{:03d}.pdf'.format(latent_dim, plttype, tstr, spec1.upper(), spec2.upper(), ispec), bbox_inches='tight')\n",
    "        plt.savefig('../figures/spectra_fit_{:02d}latentdim_{:s}_{:s}_{:s}_{:03d}.pdf'.format(latent_dim, tstr, spec1.upper(), spec2.upper(), ispec), bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "dm = (test_data['redshift'] > params['min_train_redshift']) & \\\n",
    "             (test_data['redshift'] < params['max_train_redshift'])\n",
    "\n",
    "test_data['sigma_ae_time'], ae_noise_t_bin_edge, ae_noise_t_bin_cent = calculations.compute_sigma_ae_time(test_data['spectra'][dm],\n",
    "                                                                                            np.clip(test_data['spectra_mcmc'][dm], 0, np.inf),\n",
    "                                                                                            test_data['sigma'][dm],\n",
    "                                                                                            test_data['times'][dm],\n",
    "                                                                                            test_data['mask'][dm])\n",
    "# savefig = False\n",
    "savefig = True\n",
    "\n",
    "nshow_interp = 21 #3\n",
    "\n",
    "istart = 0 #ind_max #4 #44\n",
    "nplt = 20\n",
    "# plts = [88,9]\n",
    "plts = [ 11,  40,  51,  61,  63, 105, 109, 123, 131] # ones not near peak\n",
    "\n",
    "# for iplt in range(istart, istart+nplt):\n",
    "\n",
    "inds = np.argsort(test_data['mask'].sum(1)[:, 0])#[::-1]\n",
    "\n",
    "# for iplt in inds[dm[inds]][istart:istart+nplt]: \n",
    "for iplt in [2, 38]: \n",
    "# for iplt in [108, 143, 102, 164]:\n",
    "# for iplt in [102]:#2,6, 38, 43]: \n",
    "# for iplt in [30, 24]:#2,6, 38, 43]: \n",
    "# for iplt in [38]: \n",
    "\n",
    "    print(iplt)\n",
    "\n",
    "    carr = np.arange(train_data['z_samples'][:, iplt].shape[-2])\n",
    "\n",
    "    latent = ['$z_{:d}$'.format(i+1) for i in range(train_data['z_latent'].shape[1]-3)]\n",
    "    labels = ['$\\Delta t$', '$\\Delta m$', '$\\Delta A_V$'] + latent\n",
    "    \n",
    "    # plot_spectra_map(train_data, ispec=iplt, nshow_interp=nshow_interp, spec1='salt2', spec2='pae', train=True, \n",
    "    #                  savefig=savefig, tstr='train', relative_error=True)\n",
    "    \n",
    "    plot_spectra_map(test_data, ispec=iplt, nshow_interp=nshow_interp, spec1='salt2', spec2='pae', train=False, \n",
    "                     savefig=savefig, tstr='test', relative_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.patheffects as pe\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "# Simulate data\n",
    "# When we quote an \"intrinsic dispersion\", what we are measuring is the additional dispersion that you need to explain the observed variance in your data.\n",
    "\n",
    "# Assume that you have a data flux f, a model flux m, measurement uncertainties sigma_f, and you want to measure an intrinsic dispersion sigma_i. Then the statistical model is:\n",
    "\n",
    "# f ~ N(m, sigma_f^2 + sigma_i^2)\n",
    "\n",
    "# We fit for the maximum likelihood estimate of sigma_i. This is roughly equivalent to \"picking the value of sigma_i that sets the chi2/DoF to 1\". Here's a demo of how to do this in Python:\n",
    "# Our model is a bit more complicated because we use a Gaussian process, but it's doing something similar under the hood. We do this for each wavelength separately.\n",
    "\n",
    "ncolor = 256\n",
    "neg_frac = 0.2 # time ranges from -10 to 40\n",
    "# colors1 = plt.cm.Blues(np.linspace(0.2, 1, int(ncolor*neg_frac)))\n",
    "# colors2 = plt.cm.Oranges_r(np.linspace(0, 0.8, int(ncolor*(1-neg_frac))))\n",
    "\n",
    "# colors1 = plt.cm.Blues_r(np.linspace(0., 0.8, int(ncolor*neg_frac)))\n",
    "# colors2 = plt.cm.Oranges(np.linspace(0.2, 1.0, int(ncolor*(1-neg_frac))))\n",
    "\n",
    "# colors1 = plt.cm.hot(np.linspace(0.0, 0.4, int(ncolor*neg_frac)))\n",
    "# colors2 = plt.cm.hot(np.linspace(0.4, 0.7, int(ncolor*(1-neg_frac))))\n",
    "colors1 = plt.cm.coolwarm_r(np.linspace(0.5, 1.0, int(ncolor*neg_frac))[::-1])\n",
    "colors2 = plt.cm.coolwarm_r(np.linspace(0.0, 0.5, int(ncolor*(1-neg_frac)))[::-1])\n",
    "# colors1 = plt.cm.Spectral(np.linspace(0.0, 0.5, int(ncolor*neg_frac)))\n",
    "# colors2 = plt.cm.Spectral(np.linspace(0.5, 1.0, int(ncolor*(1-neg_frac))))\n",
    "\n",
    "# combine them and build a new colormap\n",
    "colors = np.vstack((colors1, colors2))\n",
    "# cmap = LinearSegmentedColormap.from_list('my_colormap', colors)\n",
    "# cmap = plt.cm.cividis\n",
    "\n",
    "# cmap = plt.cm.turbo\n",
    "# cmap = plt.cm.viridis\n",
    "# cmap = plt.cm.Spectral_r\n",
    "\n",
    "# intrinsic_dispersion = 0.1\n",
    "# N = 200\n",
    "# measurement_uncertainties = np.random.uniform(0.05, 0.1, N)\n",
    "# residuals = np.random.normal(0, np.sqrt(measurement_uncertainties**2 + intrinsic_dispersion**2), N)\n",
    "\n",
    "# # Fit for the intrinsic dispersion\n",
    "# def gaussian_likelihood(residuals, measurement_uncertainties, intrinsic_dispersion):\n",
    "#     variance = measurement_uncertainties**2 + intrinsic_dispersion**2\n",
    "#     return 1 / np.sqrt(2 * np.pi * variance) * np.exp(-residuals**2 / 2 / variance)\n",
    "\n",
    "# def negative_log_likelihood(x):\n",
    "#     likelihoods = gaussian_likelihood(residuals, measurement_uncertainties, x[0])\n",
    "#     return -np.sum(np.log(likelihoods))\n",
    "\n",
    "# from scipy.optimize import minimize\n",
    "# res = minimize(negative_log_likelihood, [0.05])\n",
    "\n",
    "# print(f\"Recovered intrinsic dispersion: {res.x[0]}\")\n",
    "\n",
    "\n",
    "\n",
    "# latent_dims = [1,2,3,4] # 3, 4, 5, 6, 7]\n",
    "latent_dims = [-1, 0, 1, 2, 3]#-1,0,1,2,3]\n",
    "\n",
    "savefig = True\n",
    "# savefig = False\n",
    "\n",
    "smooth_bins=7\n",
    "\n",
    "# tstr = 'test'\n",
    "tstrs = ['train', 'test']\n",
    "# tstrs = ['train']\n",
    "# tstrs = ['test']\n",
    "\n",
    "# file_string = 'test'\n",
    "\n",
    "# file_string = 'layers256-128-32_minz_0pt02_data_augment_decorrelatedust'\n",
    "# file_string = 'layers256-128-32_minz_0pt02_dtimecorrect'#_longchain_Avvary'\n",
    "# file_string = 'layers256-128-32_3stage_train_decorrelate_all'#_longchain_Avvary'\n",
    "\n",
    "# file_string = 'layers256-128-32_3stage_train_decorrelate_all_seed0' #_magnitude_nosig'\n",
    "file_string = \"layers256-128-32_3stage_train_decorrelate_all_seed0\"#_nosigmaweight_smallcov\"\n",
    "\n",
    "params['min_train_redshift'] = 0.02\n",
    "params['max_train_redshift'] = 1.00\n",
    "params['max_light_cut'] = [-10, 40]\n",
    "params['twins_cut'] = False\n",
    "\n",
    "\n",
    "fig, axss = plt.subplots(nrows=2, ncols=len(latent_dims),\n",
    "                        sharey=True, figsize=(4*len(latent_dims), 6), squeeze=False)#, gridspec_kw={'height_ratios': [3, 1]})\n",
    "  \n",
    "fig.subplots_adjust(wspace=0.05, hspace=0.175)\n",
    "    \n",
    "for itstr, tstr in enumerate(tstrs):\n",
    "\n",
    "    axs = axss[itstr]                 \n",
    "\n",
    "    # Make dummie mappable\n",
    "    xx = np.linspace(-10, 40, 100)\n",
    "    dummie_cax = axs[0].scatter(xx, xx, c=xx, cmap=cmap)\n",
    "    # Clear axis\n",
    "    axs[0].cla()\n",
    "\n",
    "    for ax in axs:\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(1.)\n",
    "\n",
    "        ax.tick_params('both', length=6, width=1., which='major', color='k', direction='in')                                         \n",
    "        ax.tick_params('both', length=3, width=1., which='minor', color='k', direction='in')\n",
    "\n",
    "\n",
    "        ax.set_ylim(0, 0.4)\n",
    "\n",
    "        ax.set_yscale('linear')\n",
    "\n",
    "        ax.grid()\n",
    "        \n",
    "        if itstr == 1:\n",
    "            ax.set_xlabel('Wavelength [$\\AA$]')\n",
    "\n",
    "        if itstr == 0:\n",
    "            ax.set_xticklabels([])\n",
    "\n",
    "    if itstr==1:\n",
    "        axs[0].set_ylabel('                              Unmodeled Dispersion [mag]')\n",
    "\n",
    "    \n",
    "    for il, latent_dim in enumerate(latent_dims):\n",
    "        \n",
    "        if il == 2:\n",
    "            axs[il].set_title(tstr.capitalize(), fontsize=20)\n",
    "\n",
    "        if itstr == 0:\n",
    "\n",
    "            if latent_dim < 0:\n",
    "                latent_str = \"SALT2\\n\"+r\"$x_0, x_1, c$\" \n",
    "            else:\n",
    "                latent_str = \"PAE\\n\"+r\"$\\Delta M, \\Delta A_V\" \n",
    "                if latent_dim > 0:\n",
    "                    for idim in range(latent_dim):\n",
    "                        latent_str += r\", z_{:d}\".format(idim+1)\n",
    "                latent_str+=\"$\"\n",
    "            axs[il].text(train_data['wavelengths'][-70], 0.375,\n",
    "                         latent_str, fontsize=16, ha='center', va='top',\n",
    "                            path_effects=[pe.Stroke(linewidth=4, foreground='w'), pe.Normal()], zorder=100)\n",
    "\n",
    "#         if latent_dim == 3 or latent_dim==-1:\n",
    "#             file_string = \"layers256-128-32_3stage_train_decorrelate_all_seed0\"\n",
    "#             print(file_string)\n",
    "#             train_data_file = '../outputs/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, 3, file_string)\n",
    "#             test_data_file  = '../outputs/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, 3, file_string)\n",
    "\n",
    "#         else:\n",
    "\n",
    "#             file_string = \"layers256-128-32_3stage_train_decorrelate_all_seed0\"\n",
    "#             print(file_string)\n",
    "#             train_data_file = '../outputs/old/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "#             test_data_file  = '../outputs/old/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "            \n",
    "        latent_dim_load = latent_dim\n",
    "        if latent_dim_load < 0:\n",
    "            latent_dim_load = 3\n",
    "            \n",
    "        if latent_dim_load == 3:\n",
    "            train_data_file = '../outputs/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim_load, file_string)\n",
    "            test_data_file  = '../outputs/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim_load, file_string)\n",
    "        else:\n",
    "            train_data_file = '../outputs/pre_wavelength_mask/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim_load, file_string)\n",
    "            test_data_file  = '../outputs/pre_wavelength_mask/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim_load, file_string)\n",
    "            \n",
    "            train_mask_data_file = '../outputs/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, 3, file_string)\n",
    "            test_mask_data_file  = '../outputs/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, 3, file_string)\n",
    "            \n",
    "            train_data_mask = load_data(train_mask_data_file)\n",
    "            test_data_mask = load_data(test_mask_data_file)\n",
    "\n",
    "    #     train_data['spectra_ae'] = np.clip(train_data['spectra_ae'], 0., np.inf)\n",
    "    #     test_data['spectra_ae'] = np.clip(test_data['spectra_ae'], 0., np.inf)\n",
    "\n",
    "        if tstr == 'train':\n",
    "            train_data = load_data(train_data_file)\n",
    "            data = train_data\n",
    "            if latent_dim_load != 3:\n",
    "                data_mask = train_data_mask\n",
    "            else:\n",
    "                data_mask = data\n",
    "        if tstr == 'test':\n",
    "            test_data = load_data(test_data_file)\n",
    "            data = test_data\n",
    "            if latent_dim_load != 3:\n",
    "                data_mask = test_data_mask\n",
    "            else: \n",
    "                data_mask = data\n",
    "        # from scipy.optimize import minimize\n",
    "        # res = minimize(negative_log_likelihood, [0.05])\n",
    "\n",
    "        # print(f\"Recovered intrinsic dispersion: {res.x[0]}\")\n",
    "\n",
    "        dm = get_train_mask(data, params)\n",
    "\n",
    "        #         dm_redshift = (data['redshifts'] > min_train_redshift) & \\\n",
    "        #                       (data['redshifts'] < max_train_redshift)\n",
    "\n",
    "        if latent_dim < 0:\n",
    "            t_bin_cent, mean_spectra, measurement_dispersion, intrinsic_dispersion = compute_intrinsic_dispersion_vs_time(data['spectra'][dm], data['spectra_salt'][dm], data['sigma'][dm], data_mask['mask'][dm], data['times'][dm])\n",
    "        else: \n",
    "            # t_bin_cent, mean_spectra, measurement_dispersion, intrinsic_dispersion = compute_intrinsic_dispersion_vs_time(data['spectra'][dm], data['spectra_ae'][dm], data['sigma'][dm], data['times'][dm])\n",
    "            t_bin_cent, mean_spectra, measurement_dispersion, intrinsic_dispersion = compute_intrinsic_dispersion_vs_time(data['spectra'][dm], data['spectra_mcmc'][dm], data['sigma'][dm], data_mask['mask'][dm], data['times'][dm])\n",
    "\n",
    "        zorders = [1,2,3,2,1,0,0,0,0,0,0]\n",
    "        alpha = 0.9\n",
    "        lw = 3\n",
    "\n",
    "        # rms = -2.5*np.log10((mean_spectra)/(mean_spectra+intrinsic_dispersion))\n",
    "        # rms = 2.5*np.log10((mean_spectra+intrinsic_dispersion)/(mean_spectra))\n",
    "        rms = frac_to_mag(intrinsic_dispersion/mean_spectra)\n",
    "\n",
    "        for it in range(t_bin_cent.shape[0]):\n",
    "            rmsi = savgol_filter(rms[it], smooth_bins, 2)\n",
    "\n",
    "\n",
    "            axs[il].plot(train_data['wavelengths'], rmsi, '-',\n",
    "                    path_effects=[pe.Stroke(linewidth=3, foreground='w'), pe.Normal()],\n",
    "                    c=cmap_scatter(t_bin_cent[it]), \n",
    "    #                 lw=lw-abs(0.2-t_bin_cent[it]*2), \n",
    "    #                 alpha=alpha-abs(0.2-t_bin_cent[it]/2), \n",
    "                    lw=2, \n",
    "                    alpha=0.8, \n",
    "                    zorder=zorders[it])#, label='MAP')\n",
    "\n",
    "        # ax1.set_title('SALT2', fontsize=16)\n",
    "\n",
    "fig.subplots_adjust(right=0.95)\n",
    "cax = fig.add_axes([0.96, 0.3, 0.0075, 0.4])\n",
    "fig.colorbar(dummie_cax, cax=cax, label='Days from max light')\n",
    "\n",
    "if savefig: \n",
    "    fname = 'rmse_train_test_all_dim'\n",
    "    plt.savefig(figdir+fname+'.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize spectral effects of varying model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as pe\n",
    "\n",
    "# Plot SN while varying z latent params to see effects\n",
    "\n",
    "savefig = True\n",
    "# savefig = False\n",
    "\n",
    "figdir = '../figures/'\n",
    "\n",
    "# Set up figure aesthetics\n",
    "latent = ['$z_{:d}$'.format(i+1) for i in range(train_data['z_latent'].shape[1]-3)]\n",
    "labels = ['$\\Delta t$', '$\\Delta M$', '$\\Delta A_V$'] + latent\n",
    "\n",
    "plot_line_labels = True\n",
    "\n",
    "label_lines = {\n",
    "    'SiII\\n4130': (3920, 4130),\n",
    "    'SiII\\n5972': (5650, 5900),\n",
    "    'SiII\\n6355': (5950, 6400),\n",
    "    'CaII\\nH&K': (3530, 3870),\n",
    "    'CaII\\nIR triplet': (7800, 8450),\n",
    "    'SII\\ndoublet': (5180, 5570),\n",
    "    'OI\\ntriplet': (7300, 7680),\n",
    "}\n",
    "\n",
    "lwt = 2\n",
    "lw  = 3\n",
    "\n",
    "c0  = 'orangered'\n",
    "c1  = 'C0'\n",
    "\n",
    "dim_plus_2 = 1 #2\n",
    "\n",
    "wave_txt = train_data['wavelengths'][-1]+100\n",
    "\n",
    "alpha=0.8\n",
    "plttypes = ['normal', 'residual']\n",
    "aoffsets_list = [[0.1, 3], [0.1, 5], [1.5, 2.75]] #0.7]\n",
    "\n",
    "# Get observation times and bin latent params\n",
    "dm = get_train_mask(train_data, params)\n",
    "z_latent_use = train_data['z_latent_mcmc']\n",
    "\n",
    "nsamples = 10\n",
    "percentiles = np.arange(10, 100, nsamples)\n",
    "z_bin_edge = np.percentile(z_latent_use, percentiles, axis=0)\n",
    "nsamples = z_bin_edge.shape[0]\n",
    "\n",
    "Asamp = np.linspace(0.5, 1.5, nsamples)\n",
    "\n",
    "nshow = 6 #3\n",
    "times_lin = np.zeros((1, params['n_timestep'])) \n",
    "times_lin[:, :nshow] = np.linspace(0,1,nshow)\n",
    "mask_     = np.ones((1, params['n_timestep'], 288))\n",
    "\n",
    "times_orig_lin = times_lin*50 - 10\n",
    "times_lin[0,0] += 1./50\n",
    "times_lin[0,nshow-1] -= 1./50\n",
    "\n",
    "print(times_lin, times_orig_lin)\n",
    "\n",
    "# cmap = plt.cm.coolwarm\n",
    "colors = cmap_spectra(np.linspace(0,1,nsamples))\n",
    "\n",
    "z_median = np.median(z_latent_use[dm], axis=0)[None, ...]\n",
    "\n",
    "# get u array of zeros\n",
    "print(z_median, z_median.shape, colors.shape)\n",
    "\n",
    "for dim in range(z_bin_edge.shape[1]):\n",
    "\n",
    "    aoffsets = [0.75, 1.5]\n",
    "    zsamp = z_bin_edge[:, dim]\n",
    "\n",
    "    for iplt, plttype in enumerate(plttypes):\n",
    "        \n",
    "        if plttype == 'normal':   title = 'Varying {:s}'.format(labels[dim])\n",
    "        if plttype == 'residual': title = 'Ratio'\n",
    "\n",
    "        fig, ((ax1)) = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(8,6))#, gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax1.spines[axis].set_linewidth(1.)\n",
    "\n",
    "        ax1.tick_params('both', length=6, width=1., which='major', color='k',direction='in')                                         \n",
    "        ax1.tick_params('both', length=3, width=1., which='minor', color='k',direction='in')\n",
    "\n",
    "        ymin = 100\n",
    "        ymax = -100\n",
    "        \n",
    "        # get origin value\n",
    "        z_ = z_median.copy()\n",
    "        xpred_m = PAE.decoder((z_, times_lin,  mask_))[0]\n",
    "        \n",
    "        offset = np.arange(times_lin.shape[1]) * -1 * aoffsets[iplt]\n",
    "        \n",
    "        for sample in range(z_bin_edge.shape[0]):\n",
    "\n",
    "            # set u values along slice\n",
    "            z_ = z_median.copy()\n",
    "            z_[:, dim:dim+dim_plus_2] = zsamp[sample]\n",
    "\n",
    "            xpred_i = PAE.decoder((z_, times_lin, mask_))[0] #/ (10**(-0.4* (CL*np.exp(z_[:, -1]) + z_[:, 0])))\n",
    "\n",
    "            for i in range(nshow):\n",
    "                labp = None\n",
    "                labo = None\n",
    "                if i==0:\n",
    "                    labs = 'Encoder'\n",
    "                    labp = 'MAP'\n",
    "                if plttype == 'normal':\n",
    "                    xpi = xpred_i[i] + offset[i]\n",
    "                    ax1.plot(train_data['wavelengths'], xpi, '-',\n",
    "                             c=colors[sample], lw=lw, alpha=alpha,\n",
    "                            path_effects=[pe.Stroke(linewidth=lw+1, foreground='w'), pe.Normal()],\n",
    "                            )#, label=labp)\n",
    "\n",
    "                    if sample == len(zsamp)//2:\n",
    "                        ax1.text(wave_txt, offset[i], '{:.0f} days'.format(times_orig_lin[0,i]), fontsize=16)\n",
    "\n",
    "                    ymin = min(ymin, min(xpi) )\n",
    "                    ymax = max(ymax, max(xpi) )\n",
    "\n",
    "                if plttype == 'residual':\n",
    "                    xti = xpred_m[i]\n",
    "                    xpi = xpred_i[i]\n",
    "                    ax1.plot(train_data['wavelengths'], xpi/xti - 1 + offset[i],\n",
    "                             '-', c=colors[sample], lw=lw, alpha=alpha,\n",
    "                            path_effects=[pe.Stroke(linewidth=lw+1, foreground='w'), pe.Normal()],\n",
    "                            )#, label=labp)\n",
    "\n",
    "                    if sample == len(zsamp)//2:\n",
    "                        ax1.text(wave_txt, (xpi/xti)[-1] - 1 + offset[i], '{:.0f} days'.format(times_orig_lin[0,i]), fontsize=16)\n",
    "                \n",
    "                    ymin = min(ymin, min(xpi/xti -1 + offset[i]) )\n",
    "                    ymax = max(ymax, max(xpi/xti -1 + offset[i]) )\n",
    "\n",
    "        if plot_line_labels:\n",
    "            if plttype == 'residual':\n",
    "                ymax_line_labels = 0.85\n",
    "            if plttype == 'normal':\n",
    "                ymax_line_labels = 0.95\n",
    "\n",
    "            for line_label, (min_wave, max_wave) in label_lines.items():\n",
    "                ax1.axvspan(min_wave, max_wave, alpha=0.15, ymax=ymax_line_labels, color='gray')\n",
    "                if plttype == 'normal':\n",
    "                    # display text\n",
    "                    mean_wave = (min_wave + max_wave) / 2.\n",
    "                    ax1.text(mean_wave, -aoffsets[iplt]*2.6, line_label, fontsize=10.,\n",
    "                             horizontalalignment='center', verticalalignment='bottom')\n",
    "\n",
    "\n",
    "        ax1.set_ylabel('Restframe Flux')\n",
    "        ax1.set_xlabel('Wavelength [$\\AA$]')\n",
    "        ax1.set_ylabel('Normalized Restframe Flux(t, $\\lambda$)')\n",
    "\n",
    "        if plttype == 'normal':\n",
    "            ymax = offset[0] + aoffsets[iplt] \n",
    "            ymin = offset[nshow-1]\n",
    "            ax1.set_title(title, fontsize=20, y=0.97, pad=0)\n",
    "\n",
    "        if plttype == 'residual':\n",
    "            # ymax = ymax+0.5\n",
    "            # ymin = ymin-0.5\n",
    "            ymax = offset[0] + aoffsets[iplt] * 1.5\n",
    "            ymin = offset[nshow-1] - aoffsets[iplt]*1./2\n",
    "            ax1.set_title(title, fontsize=20, y=0.91, pad=0)\n",
    "\n",
    "        ax1.set_ylim(ymin, ymax)\n",
    "        ax1.set_frame_on(False)\n",
    "        ax1.get_xaxis().tick_bottom()\n",
    "        ax1.axes.get_yaxis().set_visible(False)\n",
    "        xmin, xmax = ax1.get_xaxis().get_view_interval()\n",
    "        ymin, ymax = ax1.get_yaxis().get_view_interval()\n",
    "        ax1.add_artist(plt.Line2D((xmin, xmax), (ymin, ymin), color='black', linewidth=2))\n",
    "\n",
    "        ax1.legend(loc='upper right', frameon=False, ncol=3)\n",
    "\n",
    "\n",
    "    #     ax1.axis('off')\n",
    "\n",
    "    #     ax2.set_yscale('log')\n",
    "        ax1.set_xlim(train_data['wavelengths'][0], train_data['wavelengths'][-1])\n",
    "    #     plt.subplots_adjust(hspace=0.025)\n",
    "\n",
    "        if savefig: \n",
    "            fname = 'vary_zlatent_{:02d}Dlatent_dim{:d}_{:s}'.format(latent_dim, dim, plttype)\n",
    "            if dim_plus_2 > 1:\n",
    "                fname = 'vary_zlatent_dimplus2_{:02d}Dlatent_dim{:d}_{:s}'.format(latent_dim, dim, plttype)\n",
    "\n",
    "            plt.savefig(figdir+fname+'.pdf', bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitude residuals vs other techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_twins = pd.read_csv('../data/boone_data.dat', delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct PAE/salt2/twins table\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_twins = pd.read_csv('../data/boone_data.dat', delimiter = \",\")\n",
    "\n",
    "latent_dim = 3\n",
    "kfold = 0\n",
    "\n",
    "maxlightcut_str = ''\n",
    "\n",
    "file_string = \"layers256-128-32_3stage_train_decorrelate_all_seed0\"\n",
    "\n",
    "train_data_file = '../outputs/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "test_data_file  = '../outputs/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "\n",
    "train_data = load_data(train_data_file)\n",
    "test_data  = load_data(test_data_file)\n",
    "\n",
    "file_string = \"layers256-128-32_3stage_train_decorrelate_all\"\n",
    "\n",
    "tstrs = ['train', 'test']\n",
    "# tstrs = ['test']\n",
    "\n",
    "seeds_use = np.array([0, 1, 2, 3, 4, 5, 6, 7])#, 8, 9])\n",
    "\n",
    "df = None\n",
    "for ti, tstr in enumerate(tstrs):\n",
    "    \n",
    "    if tstr=='train':\n",
    "        data = train_data\n",
    "\n",
    "        m = '*'\n",
    "#         dm = np.full(train_data['maximum_likelihood'].shape[0], True, dtype=bool)\n",
    "#         dm = train_data['maximum_likelihood'] > test_data['maximum_likelihood']\n",
    "\n",
    "    if tstr=='test':\n",
    "        m = '*'\n",
    "        data = test_data\n",
    "\n",
    "    # get twins data for sn\n",
    "    set_snf = set(list(data['names']))\n",
    "    set_boone = set(list(df_twins['name']))\n",
    "\n",
    "    intersection = list(set_snf & set_boone)\n",
    "    in_twins = np.full(data['redshift'].shape[0], False)\n",
    "    mask_twins = np.full(data['redshift'].shape[0], False)\n",
    "\n",
    "    dm_rbtl = np.zeros(data['redshift'].shape[0])\n",
    "    av_rbtl = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    dm_twins_err = np.zeros(data['redshift'].shape[0])\n",
    "    av_twins_err = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    dm_resid_twins = np.zeros(data['redshift'].shape[0])\n",
    "    dm_resid_salt = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    xi1_twins = np.zeros(data['redshift'].shape[0])\n",
    "    xi2_twins = np.zeros(data['redshift'].shape[0])\n",
    "    xi3_twins = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    \n",
    "    c_salt = data['c']\n",
    "    x1_salt = data['x1']\n",
    "\n",
    "    for i, name in enumerate(intersection):\n",
    "        ind_snf = np.argwhere(data['names'] == name)[0]\n",
    "        dfi = df_twins[df_twins['name'] == name]\n",
    "\n",
    "        mask_twins[ind_snf] = dfi['mask_twins']\n",
    "        in_twins[ind_snf]   = True\n",
    "\n",
    "        dm_resid_twins[ind_snf] = dfi['dm_residuals_twins']\n",
    "        dm_resid_salt[ind_snf] = dfi['dm_residuals_salt']\n",
    "\n",
    "        dm_rbtl[ind_snf] = dfi['rbtl_dm']\n",
    "        av_rbtl[ind_snf] = dfi['rbtl_av']\n",
    "\n",
    "        dm_twins_err[ind_snf] = dfi['rbtl_dm_err']\n",
    "        av_twins_err[ind_snf] = dfi['rbtl_av_err']\n",
    "        \n",
    "        xi1_twins[ind_snf] = dfi['xi_1']\n",
    "        xi2_twins[ind_snf] = dfi['xi_2']\n",
    "        xi3_twins[ind_snf] = dfi['xi_3']\n",
    "\n",
    "    min_train_redshift = 0.02\n",
    "    max_train_redshift = 1.0\n",
    "\n",
    "    dm_redshift = (data['redshift'] > min_train_redshift) & \\\n",
    "                  (data['redshift'] < max_train_redshift) & \\\n",
    "                    (np.sum(data['mask'], axis=(1,2)) > 0)\n",
    "\n",
    "    \n",
    "    # Dl = c*z/H0\n",
    "    z = data['redshift']  \n",
    "    z1 = (z*3e5+300.)/3e5 # add pec vel err to get new redshift      \n",
    "\n",
    "    dl = z #(1+z)*z\n",
    "    dl1 = z1 #(1+z1)*z1 # don't need leading constant c/H0\n",
    "    \n",
    "    # mag_err_vel = -2.5*np.log10(((dl**2*(1+z))/(dl1**2*(1+z1)))) # factors to move to z=0.05\n",
    "    mag_err_vel = abs(-5*np.log10(dl/dl1)) # factors to move to z=0.05\n",
    "\n",
    "    latent = ['$z_{:d}$'.format(i+1) + '$^{PAE}$' for i in range(train_data['z_latent'].shape[1]-3)]\n",
    "\n",
    "    mask_grid = in_twins\n",
    "    # mask_grid = mask_twins\n",
    "    # mask_grid = dm_redshift & mask_grid\n",
    "\n",
    "    for iseed, seed in enumerate(seeds_use):\n",
    "        data_filei = f'../outputs/{tstr}_data_kfold{kfold}_posterior_{latent_dim:02d}Dlatent_{file_string}_seed{seed}{maxlightcut_str}.npy'\n",
    "        datai = load_data(data_filei)\n",
    "\n",
    "        if iseed == 0:\n",
    "            A = np.zeros(( seeds_use.shape[0], datai['amplitude_mcmc'].shape[0] ))\n",
    "            A_err = np.zeros(( seeds_use.shape[0], datai['amplitude_mcmc'].shape[0] ))\n",
    "\n",
    "        A[iseed] = datai['amplitude_mcmc']\n",
    "        A_err[iseed] = np.sqrt(datai['amplitude_mcmc_err']**2)# + mag_err_vel**2)\n",
    "\n",
    "        \n",
    "        \n",
    "    # http://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf\n",
    "    weight = 1./A_err**2\n",
    "    n_eff = np.sum(weight*A, axis=0)**2/np.sum(weight**2, axis=0)\n",
    "\n",
    "    # A_mean = A.mean(0)\n",
    "    weighted_mean = np.sum(weight*A, axis=0)/np.sum(weight, axis=0)\n",
    "\n",
    "    weighted_deviation  = np.sqrt(A.shape[0]/(A.shape[0] - 1) *  \n",
    "                            ( np.sum(weight*A**2, axis=0)/np.sum(weight, axis=0) - weighted_mean**2))\n",
    "\n",
    "    weighted_err = weighted_deviation #/np.sqrt(A.shape[0])\n",
    "    weighted_err = np.sqrt(weighted_err**2 + mag_err_vel**2)\n",
    "\n",
    "    dfi = pd.DataFrame({\n",
    "        'name': data['names'][in_twins],\n",
    "        'redshift': data['redshift'][in_twins],\n",
    "        'in_twins_final_analysis': mask_twins[in_twins]*1,\n",
    "        'in_train_set': np.array([tstr == 'train']*len(mask_twins[in_twins]))*1,\n",
    "        \n",
    "        # 'dm_pae': data['z_latent_mcmc'][in_twins, 1],\n",
    "        'dm_pae': weighted_mean[in_twins],\n",
    "        'd_dm_pae': weighted_deviation[in_twins],\n",
    "        \n",
    "        'dp_pae': data['z_latent_mcmc'][in_twins, 0],\n",
    "        'dav_pae': data['z_latent_mcmc'][in_twins, 2],\n",
    "        'z1_pae': data['z_latent_mcmc'][in_twins, 3],\n",
    "        'z2_pae': data['z_latent_mcmc'][in_twins, 4],\n",
    "        'z3_pae': data['z_latent_mcmc'][in_twins, 5],\n",
    "        \n",
    "        'd_dp_pae': data['z_latent_mcmc_err'][in_twins, 1],\n",
    "        'd_dav_pae': data['z_latent_mcmc_err'][in_twins, 2],\n",
    "        'd_z1_pae': data['z_latent_mcmc_err'][in_twins, 3],\n",
    "        'd_z2_pae': data['z_latent_mcmc_err'][in_twins, 4],\n",
    "        'd_z3_pae': data['z_latent_mcmc_err'][in_twins, 5],\n",
    "                      \n",
    "        'logp_u_pae': data['logp_u_latent_mcmc'][in_twins],\n",
    "        'logp_z_pae': data['logp_z_latent_mcmc'][in_twins],\n",
    "        \n",
    "        'dm_resid_salt': dm_resid_salt[in_twins],\n",
    "        'c_salt': c_salt[in_twins],\n",
    "        'x1_salt': x1_salt[in_twins],\n",
    "        \n",
    "        'dm_rbtl': dm_rbtl[in_twins],\n",
    "        'av_rbtl': av_rbtl[in_twins],\n",
    "        'dm_resid_twins': dm_resid_twins[in_twins],\n",
    "        'xi1_twins': xi1_twins[in_twins],\n",
    "        'xi2_twins': xi2_twins[in_twins],\n",
    "        'xi3_twins': xi3_twins[in_twins],\n",
    "        \n",
    "\n",
    "    })\n",
    "\n",
    "    if df is None:\n",
    "        df = dfi.copy()\n",
    "    else:\n",
    "        df = df.append(dfi, ignore_index=True)\n",
    "        \n",
    "df.to_csv(\"../data/pae_salt_twins_params.csv\", index=False)\n",
    "df.to_csv(\"../data/pae_salt_twins_params.txt\", index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_twins = pd.read_csv('../data/boone_data.dat', delimiter = \",\")\n",
    "\n",
    "latent_dim = 3\n",
    "kfold = 0\n",
    "\n",
    "maxlightcut_str = ''\n",
    "\n",
    "\n",
    "file_string = \"layers256-128-32_3stage_train_decorrelate_all_seed0\"\n",
    "\n",
    "train_data_file = '../outputs/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "test_data_file  = '../outputs/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "\n",
    "train_data = load_data(train_data_file)\n",
    "test_data  = load_data(test_data_file)\n",
    "\n",
    "file_string = \"layers256-128-32_3stage_train_decorrelate_all\"\n",
    "\n",
    "# savefig = True\n",
    "savefig = False\n",
    "\n",
    "# show_plot_grid = True\n",
    "show_plot_grid = False\n",
    "\n",
    "show_plot_grid_one_vs_other = True\n",
    "# show_plot_grid_one_vs_other = False\n",
    "\n",
    "# standardize = True\n",
    "standardize = False\n",
    "\n",
    "cmap = plt.cm.coolwarm\n",
    "colors = cmap(np.linspace(0, 1, 256))\n",
    "\n",
    "c_pae = colors[int(0.9*len(colors))]\n",
    "color_salt = colors[int(0.1*len(colors))]\n",
    "c_twins = 'C1'\n",
    "c_bg = 'gray'\n",
    "c_bg_2 = colors[int(0.7*len(colors))]\n",
    "\n",
    "ms = 8\n",
    "yminmax = 0.75\n",
    "\n",
    "alpha_out = 0.3\n",
    "\n",
    "alpha0 = 0.6\n",
    "alpha1 = 0.8\n",
    "\n",
    "fontsize = 12\n",
    "fontsize_meas = 14\n",
    "fontsize_label = 18\n",
    "\n",
    "ypos_meas = 0.85\n",
    "ypos_label = 0.95\n",
    "\n",
    "xpos_meas = 0.85\n",
    "xpos_label = 0.85\n",
    "\n",
    "frameon = True\n",
    "if latent_dim==2: xyminmax = [[-0.4, 0.4], [0, 1], [-1.5, 1.5], [-0.5, 3], [-0.4, 0.4]]\n",
    "if latent_dim==3: xyminmax = [[-0.4, 0.4], [-1.5, 0.5], [0, 0.3], [-1.5, 1.5], [-0.5, 0.75], [-0.4, 0.4]]\n",
    "\n",
    "tstrs = ['train', 'test']\n",
    "# tstrs = ['test']\n",
    "\n",
    "# seeds_use = np.arange(8)\n",
    "seeds_use = np.array([0, 1, 2, 3, 4, 5, 6, 7])#, 8, 9])\n",
    "\n",
    "grid_xy = 2.25 #1.75\n",
    "spacing = 0.06\n",
    "\n",
    "for ti, tstr in enumerate(tstrs):\n",
    "    \n",
    "    if tstr=='train':\n",
    "        data = train_data\n",
    "\n",
    "        m = '*'\n",
    "#         dm = np.full(train_data['maximum_likelihood'].shape[0], True, dtype=bool)\n",
    "#         dm = train_data['maximum_likelihood'] > test_data['maximum_likelihood']\n",
    "\n",
    "    if tstr=='test':\n",
    "        m = '*'\n",
    "        data = test_data\n",
    "\n",
    "    # get twins data for sn\n",
    "    set_snf = set(list(data['names']))\n",
    "    set_boone = set(list(df_twins['name']))\n",
    "\n",
    "    intersection = list(set_snf & set_boone)\n",
    "    in_twins = np.full(data['redshift'].shape[0], False)\n",
    "    mask_twins = np.full(data['redshift'].shape[0], False)\n",
    "\n",
    "    dm_rbtl = np.zeros(data['redshift'].shape[0])\n",
    "    av_rbtl = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    dm_twins_err = np.zeros(data['redshift'].shape[0])\n",
    "    av_twins_err = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    dm_resid_twins = np.zeros(data['redshift'].shape[0])\n",
    "    dm_resid_salt = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    xi1_twins = np.zeros(data['redshift'].shape[0])\n",
    "    xi2_twins = np.zeros(data['redshift'].shape[0])\n",
    "    xi3_twins = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    \n",
    "    c_salt = data['c']\n",
    "    x1_salt = data['x1']\n",
    "\n",
    "    for i, name in enumerate(intersection):\n",
    "        ind_snf = np.argwhere(data['names'] == name)[0]\n",
    "        dfi = df_twins[df_twins['name'] == name]\n",
    "\n",
    "        mask_twins[ind_snf] = dfi['mask_twins']\n",
    "        in_twins[ind_snf]   = True\n",
    "\n",
    "        dm_resid_twins[ind_snf] = dfi['dm_residuals_twins']\n",
    "        dm_resid_salt[ind_snf] = dfi['dm_residuals_salt']\n",
    "\n",
    "        dm_rbtl[ind_snf] = dfi['rbtl_dm']\n",
    "        av_rbtl[ind_snf] = dfi['rbtl_av']\n",
    "\n",
    "        dm_twins_err[ind_snf] = dfi['rbtl_dm_err']\n",
    "        av_twins_err[ind_snf] = dfi['rbtl_av_err']\n",
    "        \n",
    "        xi1_twins[ind_snf] = dfi['xi_1']\n",
    "        xi2_twins[ind_snf] = dfi['xi_2']\n",
    "        xi3_twins[ind_snf] = dfi['xi_3']\n",
    "\n",
    "    min_train_redshift = 0.02\n",
    "    max_train_redshift = 1.0\n",
    "\n",
    "    dm_redshift = (data['redshift'] > min_train_redshift) & \\\n",
    "                  (data['redshift'] < max_train_redshift) & \\\n",
    "                    (np.sum(data['mask'], axis=(1,2)) > 0)\n",
    "\n",
    "    \n",
    "    # Dl = c*z/H0\n",
    "    z = data['redshift']  \n",
    "    z1 = (z*3e5+300.)/3e5 # add pec vel err to get new redshift      \n",
    "\n",
    "    dl = z #(1+z)*z\n",
    "    dl1 = z1 #(1+z1)*z1 # don't need leading constant c/H0\n",
    "    \n",
    "    # mag_err_vel = -2.5*np.log10(((dl**2*(1+z))/(dl1**2*(1+z1)))) # factors to move to z=0.05\n",
    "    mag_err_vel = abs(-5*np.log10(dl/dl1)) # factors to move to z=0.05\n",
    "\n",
    "    latent = ['$z_{:d}$'.format(i+1) + '$^{PAE}$' for i in range(train_data['z_latent'].shape[1]-3)]\n",
    "\n",
    "    mask_grid = in_twins\n",
    "    # mask_grid = mask_twins\n",
    "    mask_grid = dm_redshift & mask_grid\n",
    "\n",
    "\n",
    "    labels_pae = [\n",
    "        # '$\\Delta t$',\n",
    "        '$\\Delta M^{PAE}$',\n",
    "        '$\\Delta A_V^{PAE}$',\n",
    "    ] + latent\n",
    "    labels_twins = [\n",
    "        r'$\\xi_3^{Twins}$',\n",
    "        r'$\\xi_2^{Twins}$',\n",
    "        r'$\\xi_1^{Twins}$',\n",
    "        r'$\\Delta A_V^{RBTL}$',\n",
    "        r'$\\Delta m^{Twins}$',\n",
    "        r'$\\Delta m^{RBTL}$',\n",
    "    ]\n",
    "    labels_salt = ['$x_1^{SALT2}$',\n",
    "                   '$c^{SALT2}$',\n",
    "                   '$\\Delta m^{SALT2}$',\n",
    "                  ]\n",
    "\n",
    "#     c_err = np.exp(data['z_latent_mcmc'][mask_twins][:, -1] + data['z_latent_mcmc_err'][mask_twins][:, -1]) - np.exp(data['z_latent_mcmc'][mask_twins][:, -1])\n",
    "    arr_pae = data['z_latent_mcmc'][mask_grid, 1:]\n",
    "    arr_pae_err = data['z_latent_mcmc_err'][mask_grid, 1:]\n",
    "    \n",
    "#     arr_pae[:, 0] *= 50\n",
    "#     arr_pae_err[:, 0] *= 50\n",
    "    \n",
    "    arr_pae_err[:, 0] = np.sqrt(arr_pae_err[:,1]**2 + mag_err_vel[mask_grid]**2)\n",
    "\n",
    "    # labels_pae = list(np.roll(labels_pae, -1))\n",
    "    # arr_pae = np.roll(arr_pae, -1, axis=1)\n",
    "    # arr_pae_err = np.roll(arr_pae_err, -1, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(data['redshift'][mask_grid], arr_pae_err[:,1]/mag_err_vel[mask_grid], c=arr_pae[:,1])\n",
    "    plt.xlabel('redshift')\n",
    "    plt.ylabel('err_pae/err_vel')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    arr_twins = np.c_[\n",
    "        xi3_twins[mask_grid],\n",
    "        xi2_twins[mask_grid],\n",
    "        xi1_twins[mask_grid],\n",
    "        av_rbtl[mask_grid],\n",
    "        dm_resid_twins[mask_grid],\n",
    "        dm_rbtl[mask_grid]]\n",
    "    arr_twins_err = np.c_[\n",
    "        xi1_twins[mask_grid]*0.,\n",
    "        xi1_twins[mask_grid]*0.,\n",
    "        xi1_twins[mask_grid]*0.,\n",
    "        av_twins_err[mask_grid],\n",
    "        dm_twins_err[mask_grid],\n",
    "        dm_twins_err[mask_grid],\n",
    "    ]\n",
    "    \n",
    "    arr_salt = np.c_[x1_salt[mask_grid], c_salt[mask_grid], dm_resid_salt[mask_grid]+mag_err_vel[mask_grid]]\n",
    "    arr_salt_err = arr_salt*0.\n",
    "    \n",
    "    # arr_others = np.c_[arr_twins, arr_salt]\n",
    "    # arr_others_err = np.c_[arr_twins_err, arr_salt_err]\n",
    "    # labels_others = labels_twins + labels_salt\n",
    "    labels_others = [\n",
    "        r'$\\xi_3^{Twins}$',\n",
    "        r'$\\xi_2^{Twins}$',\n",
    "        r'$\\xi_1^{Twins}$',\n",
    "        '$x_1^{SALT2}$',\n",
    "        '$\\Delta A_V^{RBTL}$',\n",
    "        '$c^{SALT2}$',\n",
    "        '$\\Delta M^{Twins}$',\n",
    "        '$\\Delta M^{RBTL}$',\n",
    "        '$\\Delta M^{SALT2}$',\n",
    "\n",
    "        ]\n",
    "    arr_others = np.c_[\n",
    "        xi3_twins[mask_grid],\n",
    "        xi2_twins[mask_grid],\n",
    "        xi1_twins[mask_grid],\n",
    "        x1_salt[mask_grid],\n",
    "        av_rbtl[mask_grid],\n",
    "        c_salt[mask_grid],\n",
    "        dm_resid_twins[mask_grid],\n",
    "        dm_rbtl[mask_grid],\n",
    "        dm_resid_salt[mask_grid],\n",
    "    ]\n",
    "    arr_others_err = np.c_[\n",
    "        xi3_twins[mask_grid]*0,\n",
    "        xi2_twins[mask_grid]*0,\n",
    "        xi1_twins[mask_grid]*0,\n",
    "        x1_salt[mask_grid]*0,\n",
    "        av_twins_err[mask_grid],\n",
    "        c_salt[mask_grid]*0,\n",
    "        dm_twins_err[mask_grid],\n",
    "        dm_twins_err[mask_grid],\n",
    "        dm_resid_salt[mask_grid]*0.+mag_err_vel[mask_grid],\n",
    "    ]\n",
    "\n",
    "    if tstr=='train':\n",
    "        arr_pae_all = arr_pae.copy()\n",
    "        arr_pae_err_all = arr_pae_err.copy()\n",
    "        arr_others_all = arr_others.copy()\n",
    "        arr_others_err_all = arr_others_err.copy()   \n",
    "        # color_all = data['redshift'][mask_grid].copy()\n",
    "        color_all = arr_pae[:, 0]\n",
    "        \n",
    "    if tstr=='test':\n",
    "        arr_pae_all = np.concatenate([arr_pae_all, arr_pae], axis=0)\n",
    "        arr_pae_err_all = np.concatenate([arr_pae_err_all, arr_pae_err])\n",
    "        arr_others_all = np.concatenate([arr_others_all, arr_others], axis=0)\n",
    "        arr_others_err_all = np.concatenate([arr_others_err_all, arr_others_err], axis=0)  \n",
    "        color_all = np.concatenate([color_all, arr_pae[:, 0]])\n",
    "        \n",
    "    arr = np.c_[arr_pae, arr_twins, arr_salt]\n",
    "    err = np.c_[arr_pae_err, arr_twins_err, arr_salt_err]\n",
    "    labels = labels_pae + labels_twins + labels_salt\n",
    "#     err = np.c_[data['amplitude_mcmc_err'][in_twins], data['z_latent_mcmc_err'][in_twins][:, 1:-1], c_err, data['dtime_mcmc_err'][in_twins]*50, av_twins_err[in_twins], dm_twins_err[in_twins]]\n",
    "\n",
    "    if show_plot_grid:\n",
    "        plot_grid(arr, labels=labels, color_arr=data['redshift'][mask_grid], errs=err, #xyminmax=xyminmax,\n",
    "                  color_label='Redshift', cmap=plt.cm.viridis, figsize=(12,12), s=6,\n",
    "                  savefig=savefig, filename=figdir+'AE_and_RBTL_{:02d}latent_params_{:s}.pdf'.format(latent_dim, tstr))\n",
    "\n",
    "    if show_plot_grid_one_vs_other:\n",
    "#         plot_grid_one_vs_other(arr_pae, arr_salt, labels1=labels_pae, labels2=labels_salt,\n",
    "#                                errs1=arr_pae_err, errs2=arr_salt_err,\n",
    "#                                color_arr=data['redshift'][mask_grid], spacing=spacing, #xyminmax=xyminmax,\n",
    "#                   color_label='Redshift', cmap=plt.cm.viridis, figsize=(grid_xy*arr_pae.shape[1], grid_xy*arr_salt.shape[1]), s=6, markeredgecolor='none',\n",
    "#                   savefig=savefig, filename=figdir+'PAE_vs_salt_{:02d}latent_params_{:s}.pdf'.format(latent_dim, tstr))\n",
    "        \n",
    "#         plot_grid_one_vs_other(arr_pae, arr_twins, labels1=labels_pae, labels2=labels_twins,\n",
    "#                                errs1=arr_pae_err, errs2=arr_twins_err,\n",
    "#                                color_arr=data['redshift'][mask_grid], spacing=spacing, #xyminmax=xyminmax,\n",
    "#                   color_label='Redshift', cmap=plt.cm.viridis, figsize=(grid_xy*arr_pae.shape[1], grid_xy*arr_twins.shape[1]), s=6, markeredgecolor='none',\n",
    "#                   savefig=savefig, filename=figdir+'PAE_vs_twins_{:02d}latent_params_{:s}.pdf'.format(latent_dim, tstr))\n",
    "        \n",
    "        \n",
    "#         plot_grid_one_vs_other(arr_pae, arr_others, labels1=labels_pae, labels2=labels_others,\n",
    "#                                errs1=arr_pae_err, errs2=arr_others_err,\n",
    "#                                color_arr=data['redshift'][mask_grid], spacing=spacing, #xyminmax=xyminmax,\n",
    "#                   color_label='Redshift', cmap=plt.cm.viridis, figsize=(grid_xy*arr_pae.shape[1], grid_xy*arr_others.shape[1]), s=6, markeredgecolor='none',\n",
    "#                   savefig=savefig, filename=figdir+'PAE_vs_others_{:02d}latent_params_{:s}.pdf'.format(latent_dim, tstr))\n",
    " \n",
    "        print(arr_others.shape)\n",
    "        plot_grid_one_vs_other(arr_others_all[:, ::-1], arr_pae_all[:, ::-1], labels1=labels_others[::-1], labels2=labels_pae[::-1],\n",
    "                               errs1=arr_others_err_all[:, ::-1], errs2=arr_pae_err_all[:, ::-1],\n",
    "                               color_arr=color_all, spacing=spacing, color_symmetric=True, #xyminmax=xyminmax,\n",
    "                  color_label='$\\Delta M^{PAE}$', cmap=cmap_scatter, figsize=(grid_xy*arr_others.shape[1], grid_xy*arr_pae.shape[1]), s=6, markeredgecolor='none',\n",
    "                  savefig=savefig, filename=figdir+'PAE_vs_others_{:02d}latent_params_{:s}.pdf'.format(latent_dim, tstr))\n",
    "        \n",
    "    if standardize: \n",
    "        z_in = data['z_latent_mcmc'][:, start_dim:end_dim]\n",
    "        # z_in[:, -1] = np.exp(z_in[:, -1])\n",
    "        if standardize_time:\n",
    "            z_in = np.c_[z_in, data['dtime_mcmc']]\n",
    "\n",
    "        if 'NN' in fit_model:\n",
    "            A_pred = Amodel.predict(z_in).flatten()\n",
    "        if fit_model=='OLS':\n",
    "            A_pred = pfit[0] + z_in @ pfit[1:]\n",
    "\n",
    "        data['amplitude_mcmc'] -= A_pred\n",
    "    \n",
    "\n",
    "\n",
    "#     wrms_twins =  np.sqrt(np.sum( (dm_twins[in_twins][dm_redshift] - dm_twins[in_twins][dm_redshift].mean())**2 / dm_twins_err[in_twins][dm_redshift]**2) / np.sum(1./dm_twins_err[in_twins][dm_redshift]**2))\n",
    "#     wrms_hmc =  np.sqrt(np.sum( (data['amplitude_mcmc'][in_twins][dm_redshift]- data['amplitude_mcmc'][in_twins][dm_redshift].mean())**2 / data['amplitude_mcmc_err'][in_twins][dm_redshift]**2) / np.sum(1./data['amplitude_mcmc_err'][in_twins][dm_redshift]**2))\n",
    "#     mask_performance = in_twins\n",
    "\n",
    "    mask_performance = mask_twins\n",
    "    \n",
    "    dm_redshift = dm_redshift[mask_performance]\n",
    "\n",
    "    wrms_resid_salt =  np.std(dm_resid_salt[mask_twins]) \n",
    "\n",
    "    wrms_rbtl =  np.std(dm_rbtl[mask_twins]) \n",
    "    wrms_resid_twins =  np.std(dm_resid_twins[mask_twins]) \n",
    "\n",
    "    wrms_hmc =  np.std(data['amplitude_mcmc'][mask_twins]) \n",
    "\n",
    "    k_nmad = 1.4826\n",
    "    mad_rbtl   = k_nmad * np.median( np.abs(dm_rbtl[mask_performance][dm_redshift] - np.median(dm_rbtl[mask_performance][dm_redshift]) ))\n",
    "    mad_hmc    = k_nmad * np.median( np.abs(data['amplitude_mcmc'][mask_performance][dm_redshift] - np.median(data['amplitude_mcmc'][mask_performance][dm_redshift]) ))\n",
    "\n",
    "    mad_resid_salt  = k_nmad * np.median( np.abs(dm_resid_salt[mask_performance][dm_redshift] - np.median(dm_resid_salt[mask_performance][dm_redshift]) ))\n",
    "    mad_resid_twins = k_nmad * np.median( np.abs(dm_resid_twins[mask_performance][dm_redshift] - np.median(dm_resid_twins[mask_performance][dm_redshift]) ))\n",
    "\n",
    "    lab_rbtl = '   RMS: {:.3f}\\nNMAD: {:.3f}'.format(wrms_rbtl, mad_rbtl)    \n",
    "    lab_salt = '   RMS: {:.3f}\\nNMAD: {:.3f}'.format(wrms_resid_salt, mad_resid_salt)    \n",
    "    lab_twins = '   RMS: {:.3f}\\nNMAD: {:.3f}'.format(wrms_resid_twins, mad_resid_twins)\n",
    "    lab_hmc = '   RMS: {:.3f}\\nNMAD: {:.3f}'.format(wrms_hmc, mad_hmc)\n",
    "\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(20,4))\n",
    "\n",
    "    nx = 3\n",
    "    # SALT\n",
    "    ax = fig.add_subplot(1, nx, 1)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(1.)\n",
    "\n",
    "    ax.tick_params('both', length=6, width=1., which='major', color='k', direction='in')                                         \n",
    "    ax.tick_params('both', length=3, width=1., which='minor', color='k', direction='in')\n",
    "\n",
    "    ax.errorbar(data['redshift'][in_twins & ~mask_twins], dm_resid_salt[in_twins & ~mask_twins], yerr=mag_err_vel[in_twins & ~mask_twins],\n",
    "                ms=ms, marker='o', color=color_salt, markeredgecolor='k', ls='none', lw=2, zorder=1, alpha=alpha_out)\n",
    "\n",
    "    ax.errorbar(data['redshift'][mask_twins], dm_resid_salt[mask_twins], yerr=mag_err_vel[mask_twins],\n",
    "                ms=ms, marker='o', label=lab_salt, color=color_salt, markeredgecolor='k', ls='none', lw=2, zorder=1, alpha=alpha1)\n",
    "\n",
    "    ax.text(xpos_meas, ypos_meas, lab_salt,\n",
    "            horizontalalignment='center', verticalalignment='top',\n",
    "            transform=ax.transAxes, fontsize=fontsize_meas)\n",
    "    ax.text(xpos_label, ypos_label, 'SALT2',\n",
    "            horizontalalignment='center', verticalalignment='top',\n",
    "            transform=ax.transAxes, fontsize=fontsize_label)    \n",
    "    \n",
    "    ax.axvspan(0, min_train_redshift, alpha=0.2, color='gray')\n",
    "    ax.set_ylabel('$\\Delta M$')\n",
    "    if tstr=='train':\n",
    "        ax.set_xticklabels([])\n",
    "    else:\n",
    "        ax.set_xlabel('Redshift')\n",
    "\n",
    "    # ax.legend(frameon=True, loc='upper right', fontsize=fontsize, handletextpad=0.05)\n",
    "    ax.axhline(0, color='k', ls='--')\n",
    "    ax.axvline(min_train_redshift, color='k', ls='--')\n",
    "\n",
    "    ax.set_xlim(0,0.119)\n",
    "    ax.set_ylim(-yminmax,yminmax)\n",
    "    # ax.set_title('SALT2', fontsize=18)\n",
    "\n",
    "    \n",
    "    ax = fig.add_subplot(1, nx, 2)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(1.)\n",
    "\n",
    "    ax.tick_params('both', length=6, width=1., which='major', color='k', direction='in')                                         \n",
    "    ax.tick_params('both', length=3, width=1., which='minor', color='k', direction='in')\n",
    "\n",
    "    # ax.errorbar(data['redshift'][in_twins & ~mask_twins], dm_rbtl[in_twins & ~mask_twins], yerr=dm_twins_err[in_twins & ~mask_twins],\n",
    "    #             ms=ms, marker='o', color=c_bg, markeredgecolor='k', ls='none', lw=2, zorder=1, alpha=alpha_out)\n",
    "    # ax.errorbar(data['redshift'][mask_twins], dm_rbtl[mask_twins], yerr=dm_twins_err[mask_twins],\n",
    "    #             ms=ms, marker='o', label=lab_rbtl, color=c_bg, markeredgecolor='k', ls='none', lw=2, zorder=0, alpha=alpha0)\n",
    "\n",
    "    ax.errorbar(data['redshift'][in_twins & ~mask_twins], dm_resid_twins[in_twins & ~mask_twins], yerr=dm_twins_err[in_twins & ~mask_twins],\n",
    "                ms=ms, marker='o', color=c_twins, markeredgecolor='k', ls='none', lw=2, zorder=1, alpha=alpha_out)\n",
    "    ax.errorbar(data['redshift'][mask_twins], dm_resid_twins[mask_twins], yerr=dm_twins_err[mask_twins],\n",
    "                ms=ms, marker='o', label=lab_twins, color=c_twins, markeredgecolor='k', ls='none', lw=2, zorder=1, alpha=alpha1)\n",
    "\n",
    "    ax.text(xpos_meas, ypos_meas, lab_twins,\n",
    "            horizontalalignment='center', verticalalignment='top',\n",
    "            transform=ax.transAxes, fontsize=fontsize_meas)\n",
    "    ax.text(xpos_label, ypos_label, 'Twins',\n",
    "            horizontalalignment='center', verticalalignment='top',\n",
    "            transform=ax.transAxes, fontsize=fontsize_label)    \n",
    "    \n",
    "\n",
    "    ax.axvspan(0, min_train_redshift, alpha=0.2, color='gray')\n",
    "    ax.set_yticklabels([])\n",
    "    if tstr=='train':\n",
    "        ax.set_xticklabels([])\n",
    "    else:\n",
    "        ax.set_xlabel('Redshift')\n",
    "\n",
    "    # ax.legend(frameon=True, loc='upper right', fontsize=fontsize, handletextpad=0.05)\n",
    "    ax.axhline(0, color='k', ls='--')\n",
    "    ax.axvline(min_train_redshift, color='k', ls='--')\n",
    "\n",
    "    ax.set_xlim(0,0.119)\n",
    "    ax.set_ylim(-yminmax,yminmax)\n",
    "    ax.set_title(tstr.capitalize(), fontsize=18)\n",
    "    \n",
    "\n",
    "    for iseed, seed in enumerate(seeds_use):\n",
    "        data_filei = f'../outputs/{tstr}_data_kfold{kfold}_posterior_{latent_dim:02d}Dlatent_{file_string}_seed{seed}{maxlightcut_str}.npy'\n",
    "        datai = load_data(data_filei)\n",
    "\n",
    "        if iseed == 0:\n",
    "            A = np.zeros(( seeds_use.shape[0], datai['amplitude_mcmc'].shape[0] ))\n",
    "            A_err = np.zeros(( seeds_use.shape[0], datai['amplitude_mcmc'].shape[0] ))\n",
    "\n",
    "        A[iseed] = datai['amplitude_mcmc']\n",
    "        A_err[iseed] = np.sqrt(datai['amplitude_mcmc_err']**2)# + mag_err_vel**2)\n",
    "\n",
    "\n",
    "    # ND\n",
    "    for iplt in range(A.shape[0]):\n",
    "        wrms_hmc =  np.std(A[iplt][mask_twins]) \n",
    "        lab_hmc = '$Model{:d}: \\sigma={:.3f}, NMAD={:.3f}$'.format(iplt,  wrms_hmc, mad_hmc)\n",
    "\n",
    "        k_nmad = 1.4826\n",
    "        mad_hmc    = k_nmad * np.median( np.abs(A[iplt][mask_twins] - np.median(A[iplt][mask_twins]) ))\n",
    "\n",
    "        # ax.errorbar(data['redshift'][in_twins & ~mask_twins], A[iplt][in_twins & ~mask_twins], yerr=A_err[iplt][in_twins & ~mask_twins],\n",
    "        #             ms=ms, marker='o', color='C{:d}'.format(iplt%10), markeredgecolor='k', ls='none', lw=2, zorder=1, alpha=alpha_out)\n",
    "        # ax.errorbar(data['redshift'][mask_twins], A[iplt][mask_twins], yerr=A_err[iplt][mask_twins],\n",
    "        #             ms=ms, marker='o', color='C{:d}'.format(iplt%10), markeredgecolor='k', label=lab_hmc, ls='none', lw=2, zorder=4, alpha=alpha1) #\n",
    "\n",
    "        \n",
    "        \n",
    "#     # http://seismo.berkeley.edu/~kirchner/Toolkits/Toolkit_12.pdf\n",
    "    weight = 1./A_err**2\n",
    "    n_eff = np.sum(weight*A, axis=0)**2/np.sum(weight**2, axis=0)\n",
    "\n",
    "    # A_mean = A.mean(0)\n",
    "    weighted_mean = np.sum(weight*A, axis=0)/np.sum(weight, axis=0)\n",
    "\n",
    "    weighted_deviation  = np.sqrt(A.shape[0]/(A.shape[0] - 1) *  \n",
    "                            ( np.sum(weight*A**2, axis=0)/np.sum(weight, axis=0) - weighted_mean**2))\n",
    "\n",
    "    weighted_err = weighted_deviation #/np.sqrt(A.shape[0])\n",
    "    weighted_err = np.sqrt(weighted_err**2 + mag_err_vel**2)\n",
    "\n",
    "    # save final parameters to file\n",
    "    np.savez('../data/final_magnitude_fits_{:s}{:s}.npz'.format(tstr, maxlightcut_str),\n",
    "             dm=weighted_mean,\n",
    "             dm_sig=weighted_err,\n",
    "             dm_vel=mag_err_vel,\n",
    "\n",
    "             dm_resid_salt=dm_resid_salt,\n",
    "             dm_resid_salt_err=mag_err_vel,\n",
    "             \n",
    "             dm_rbtl=dm_rbtl,\n",
    "             dm_rbtl_err=mag_err_vel,\n",
    "\n",
    "             dm_resid_twins=dm_resid_twins,\n",
    "             dm_resid_twins_err=dm_twins_err,\n",
    "             \n",
    "            in_twins=in_twins,\n",
    "            mask_twins=mask_twins)\n",
    "    \n",
    "    wrms_hmc =  np.std(weighted_mean[mask_twins], axis=0)\n",
    "\n",
    "    k_nmad = 1.4826\n",
    "    mad_hmc = k_nmad * np.median( np.abs(weighted_mean[mask_twins] - np.median(weighted_mean[mask_twins]) ))\n",
    "\n",
    "    lab_hmc = '   RMS: {:.3f}\\nNMAD: {:.3f}'.format(wrms_hmc, mad_hmc)\n",
    "\n",
    "    ax = fig.add_subplot(1, nx, 3)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(1.)\n",
    "\n",
    "    ax.tick_params('both', length=6, width=1., which='major', color='k', direction='in')                                         \n",
    "    ax.tick_params('both', length=3, width=1., which='minor', color='k', direction='in')\n",
    "   \n",
    "\n",
    "    # ND\n",
    "    # weighted_mean = A[0]\n",
    "    # weighted_err = weighted_mean*0\n",
    "    ax.errorbar(data['redshift'][in_twins & ~mask_twins], weighted_mean[in_twins & ~mask_twins], yerr=weighted_err[in_twins & ~mask_twins],\n",
    "                ms=ms, marker='o', color=c_pae, markeredgecolor='k', ls='none', lw=2, zorder=1, alpha=alpha_out)\n",
    "    ax.errorbar(data['redshift'][mask_twins], weighted_mean[mask_twins], yerr=weighted_err[mask_twins],\n",
    "                ms=ms, marker='o', markeredgecolor='k', label=lab_hmc, color=c_pae, ls='none', lw=2, zorder=4, alpha=alpha1)\n",
    "    \n",
    "    ax.text(xpos_meas, ypos_meas, lab_hmc,\n",
    "            horizontalalignment='center', verticalalignment='top',\n",
    "            transform=ax.transAxes, fontsize=fontsize_meas)\n",
    "    ax.text(xpos_label, ypos_label, 'PAE',\n",
    "            horizontalalignment='center', verticalalignment='top',\n",
    "            transform=ax.transAxes, fontsize=fontsize_label)\n",
    "\n",
    "    ax.axvspan(0, min_train_redshift, alpha=0.2, color='gray')\n",
    "    if tstr=='train':\n",
    "        ax.set_xticklabels([])\n",
    "    else:\n",
    "        ax.set_xlabel('Redshift')\n",
    "\n",
    "    # ax.legend(frameon=True, loc='upper right', fontsize=fontsize, handletextpad=0.05)#, title=r'$\\sigma, NMAD$')\n",
    "    ax.axhline(0.0, color='k', ls='--')\n",
    "    ax.axvline(min_train_redshift, color='k', ls='--')\n",
    "\n",
    "    ax.set_xlim(0,0.119)\n",
    "    ax.set_ylim(-yminmax,yminmax)\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "\n",
    "    if savefig:\n",
    "        plt.savefig(figdir+'deltamag_vs_others_{:02d}latent_{:s}.pdf'.format(latent_dim, tstr), bbox_inches='tight')\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr_pae_all[:, ::-1].shape)\n",
    "for i in range(5):\n",
    "    plt.hist(arr_pae_all[:, i])\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.mixture\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# np.save('dt_train', data_fit)\n",
    "\n",
    "\n",
    "gmm = sklearn.mixture.GaussianMixture()\n",
    "\n",
    "r_train = gmm.fit(arr_pae_all[:, 1][:, np.newaxis]) # GMM requires 2D data as of sklearn version 0.16\n",
    "gmm_mean_train =  r_train.means_[0, 0]\n",
    "gmm_std_train = np.sqrt(r_train.covariances_[0, 0])\n",
    "print(\"Train mean : %f, var : %f\" % (gmm_mean_train, gmm_std_train))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.hist(arr_pae_all[:, 1], alpha=0.5, bins=20, color='C0', density=True, label='$\\mu$=%.2f, $\\sigma$=%.2f'%(gmm_mean_train, gmm_std_train))\n",
    "\n",
    "xx = np.linspace(arr_pae_all[:, 1], arr_pae_all[:, 1], 100)\n",
    "plt.plot(xx, norm.pdf(xx, gmm_mean_train, gmm_std_train), color='C0')\n",
    "\n",
    "plt.axvline(0, color='k', ls=':')\n",
    "\n",
    "plt.xlabel('$\\Delta A_V$')\n",
    "plt.ylabel('Normalized counts')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(arr_pae_all[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform bootstrap error sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     np.savez(\n",
    "#              deltam=weighted_mean,\n",
    "#              deltam_sig=weighted_err,\n",
    "#             in_twins=in_twins,\n",
    "#             mask_twins=mask_twins)\n",
    "def calculate_nmad(thetas):\n",
    "    k_nmad = 1.4826\n",
    "    nmad = k_nmad * np.median( np.abs(thetas - np.median(thetas)) )\n",
    "    return nmad\n",
    "\n",
    "def bootstrap_errors(thetas, nresample=10):\n",
    "    # takes in list of measurements and performs boostrap sampling\n",
    "    \n",
    "    sample_rms = np.zeros(nresample)\n",
    "    sample_nmad = np.zeros(nresample)\n",
    "\n",
    "    for isamp in range(nresample):\n",
    "        samples = np.random.choice(thetas, thetas.shape[0], replace=True)\n",
    "        \n",
    "        sample_rms[isamp] = np.std(samples)\n",
    "        sample_nmad[isamp] = calculate_nmad(samples)\n",
    "    \n",
    "    bootstrap_rms  = np.mean(sample_rms)\n",
    "    bootstrap_nmad = np.mean(sample_nmad)\n",
    "    \n",
    "    bootstrap_rms_std  = np.sqrt(1./(nresample-1) * np.sum( (bootstrap_rms - sample_rms)**2))\n",
    "    bootstrap_nmad_std = np.sqrt(1./(nresample-1) * np.sum( (bootstrap_nmad - sample_nmad)**2))\n",
    "\n",
    "    return bootstrap_rms_std, bootstrap_nmad_std\n",
    "\n",
    "maxlightcut_str = ''\n",
    "# maxlightcut_str = '_maxlightcut'\n",
    "\n",
    "# ['dm', 'dm_sig', 'dm_resid_salt', 'dm_resid_salt_err', 'dm_rbtl', 'dm_rbtl_err', 'dm_resid_twins', 'dm_resid_twins_err', 'in_twins', 'mask_twins']\n",
    "nbootstrap_sample = 10000\n",
    "\n",
    "# dm_strs = ['dm_resid_salt', 'dm_rbtl', 'dm_resid_twins', 'dm']#, 'dm_vel']\n",
    "dm_strs = ['dm']#, 'dm_vel']\n",
    "\n",
    "dm_strs_to_display = {'dm': 'PAE',\n",
    "                      'dm_resid_salt': 'SALT2',\n",
    "                      'dm_resid_twins': 'Twins',\n",
    "                       'dm_rbtl': 'RBTL',\n",
    "                     'dm_vel': 'Vel'}\n",
    "\n",
    "tstrs = ['train', 'test']\n",
    "remove_vel = True\n",
    "table = {}\n",
    "for tstr in tstrs:\n",
    "    tablei = {}\n",
    "\n",
    "    for dm_str in dm_strs:\n",
    "        data = np.load('../data/final_magnitude_fits_{:s}{:s}.npz'.format(tstr, maxlightcut_str))\n",
    "        rms_vel = np.sqrt(np.sum(data['dm_vel'][data['mask_twins']]**2)/data['dm_vel'][data['mask_twins']].shape[0])\n",
    "        print(rms_vel)\n",
    "\n",
    "        print(tstr, data['in_twins'].sum(), data['mask_twins'].sum())\n",
    "        measurements = data[dm_str][data['mask_twins']]\n",
    "\n",
    "        rms =  np.std(measurements, axis=0)\n",
    "        nmad = calculate_nmad(measurements)\n",
    "\n",
    "        rms_std, nmad_std = bootstrap_errors(measurements, nbootstrap_sample)\n",
    "\n",
    "        d = {}\n",
    "        d['NMAD'] = '{:.3f} pm {:.3f}'.format(nmad, nmad_std)\n",
    "        d['RMS'] = '{:.3f} pm {:.3f}'.format(rms, rms_std)\n",
    "        d['RMS_noVel'] = '{:.3f} pm {:.3f}'.format(np.sqrt(rms**2-rms_vel**2), rms_std)\n",
    "\n",
    "#         if remove_vel:\n",
    "#             dm = measurements > 0\n",
    "#             measurements_i = measurements[dm].copy()\n",
    "#             measurements_i -= data['dm_vel'][data['mask_twins']][dm]\n",
    "#             measurements_i[measurements_i < 0] = 0.\n",
    "#             measurements[dm] = measurements_i\n",
    "            \n",
    "#             dm = measurements < 0\n",
    "#             measurements_i = measurements[dm].copy()\n",
    "#             measurements_i += data['dm_vel'][data['mask_twins']][dm]\n",
    "#             measurements_i[measurements_i > 0] = 0.\n",
    "#             measurements[dm] = measurements_i    \n",
    "            \n",
    "#         rms =  np.std(measurements, axis=0)\n",
    "#         nmad = calculate_nmad(measurements)\n",
    "\n",
    "#         rms_std, nmad_std = bootstrap_errors(measurements, nbootstrap_sample)\n",
    "     \n",
    "#         d['RMS_unexplained'] = '{:.3f} pm {:.3f}'.format(rms, rms_std)\n",
    "\n",
    "        # d['rms_err'] = rms_std\n",
    "        # d['nmad'] = nmad\n",
    "        # d['nmad_err'] = nmad_std\n",
    "\n",
    "\n",
    "        tablei[dm_strs_to_display[dm_str]] = d\n",
    "\n",
    "        print(r'{:s}: RMS = {:.3f} $\\pm$ {:.3f}, NMAD {:.3f} $\\pm$ {:.3f}'.format(tstr, \n",
    "                                                                                  rms,\n",
    "                                                                                  rms_std,\n",
    "                                                                                  nmad,\n",
    "                                                                                  nmad_std))\n",
    "    table[tstr] = tablei\n",
    "    \n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does magnitude residual depend on SN p(z) or p(u) density?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# savefig = True\n",
    "savefig = False\n",
    "\n",
    "# name,iau_name,salt_x1,salt_x1_err,salt_c,salt_c_err,rbtl_av,rbtl_av_err,rbtl_dm,rbtl_dm_err,xi_1,xi_2,xi_3,dm_residuals_twins,dm_residuals_salt,mask_twins,mask_salt\n",
    "df_twins = pd.read_csv('../data/boone_data.dat', delimiter = \",\")\n",
    "\n",
    "latent_dim = 3\n",
    "kfold = 0\n",
    "\n",
    "# file_string = 'layers256-128-32_multistage_train_decorrelate_all'#_1e-3clip'\n",
    "file_string = 'layers256-128-32_3stage_train_decorrelate_all_seed0' #_magnitude_nosig_twinscut_maxlight'   #_1e-3clip'\n",
    "\n",
    "train_data_file = '../outputs/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "test_data_file  = '../outputs/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "\n",
    "train_data = load_data(train_data_file)\n",
    "test_data  = load_data(test_data_file)\n",
    "\n",
    "\n",
    "# standardize = True\n",
    "standardize = False\n",
    "\n",
    "cmap = plt.cm.coolwarm\n",
    "colors = cmap(np.linspace(0, 1, 256))\n",
    "\n",
    "c_pae = colors[int(0.9*len(colors))]\n",
    "c_salt = colors[int(0.1*len(colors))]\n",
    "c_twins = 'C1'\n",
    "c_bg = 'gray'\n",
    "c_bg_2 = colors[int(0.7*len(colors))]\n",
    "\n",
    "ms = 8\n",
    "yminmax = 0.75\n",
    "\n",
    "alpha=0.9\n",
    "alpha_out = 0.3\n",
    "\n",
    "alpha0 = 0.6\n",
    "alpha1 = 0.8\n",
    "\n",
    "fontsize = 12\n",
    "frameon = True\n",
    "if latent_dim==2: xyminmax = [[-0.4, 0.4], [0, 1], [-1.5, 1.5], [-0.5, 3], [-0.4, 0.4]]\n",
    "if latent_dim==3: xyminmax = [[-0.4, 0.4], [-1.5, 0.5], [0, 0.3], [-1.5, 1.5], [-0.5, 0.75], [-0.4, 0.4]]\n",
    "\n",
    "tstrs = ['train', 'test']\n",
    "\n",
    "grid_xy = 2\n",
    "\n",
    "use_pz = True\n",
    "ymin = -15\n",
    "\n",
    "log_p_cuts = np.linspace(-5, -1, 100) #[-10., -5., -2.]\n",
    "\n",
    "c_pae = colors[int(0.9*len(colors))]\n",
    "c_salt = colors[int(0.1*len(colors))]\n",
    "c_twins = 'C1'\n",
    "c_bg = 'gray'\n",
    "c_bg_2 = colors[int(0.7*len(colors))]\n",
    "\n",
    "for ti, tstr in enumerate(tstrs):\n",
    "    if tstr=='train':\n",
    "        data = train_data\n",
    "        m = '*'\n",
    "\n",
    "    if tstr=='test':\n",
    "        m = '*'\n",
    "        data = test_data\n",
    "\n",
    "        \n",
    "    # get twins data for sn\n",
    "    set_snf = set(list(data['names']))\n",
    "    set_boone = set(list(df_twins['name']))\n",
    "\n",
    "    intersection = list(set_snf & set_boone)\n",
    "    in_twins = np.full(data['redshift'].shape[0], False)\n",
    "    mask_twins = np.full(data['redshift'].shape[0], False)\n",
    "\n",
    "    dm_rbtl = np.zeros(data['redshift'].shape[0])\n",
    "    av_rbtl = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    dm_twins_err = np.zeros(data['redshift'].shape[0])\n",
    "    av_twins_err = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    dm_resid_twins = np.zeros(data['redshift'].shape[0])\n",
    "    dm_resid_salt = np.zeros(data['redshift'].shape[0])\n",
    "\n",
    "    for i, name in enumerate(intersection):\n",
    "        ind_snf = np.argwhere(data['names'] == name)[0]\n",
    "        dfi = df_twins[df_twins['name'] == name]\n",
    "\n",
    "        mask_twins[ind_snf] = dfi['mask_twins']\n",
    "        in_twins[ind_snf]   = True\n",
    "\n",
    "        dm_resid_twins[ind_snf] = dfi['dm_residuals_twins']\n",
    "        dm_resid_salt[ind_snf] = dfi['dm_residuals_salt']\n",
    "\n",
    "        dm_rbtl[ind_snf] = dfi['rbtl_dm']\n",
    "        av_rbtl[ind_snf] = dfi['rbtl_av']\n",
    "\n",
    "        dm_twins_err[ind_snf] = dfi['rbtl_dm_err']\n",
    "        av_twins_err[ind_snf] = dfi['rbtl_av_err']\n",
    "        \n",
    "    min_train_redshift = 0.02\n",
    "    max_train_redshift = 1.0\n",
    "\n",
    "    dm_redshift = (data['redshift'] > min_train_redshift) & \\\n",
    "                  (data['redshift'] < max_train_redshift) & \\\n",
    "                    (np.sum(data['mask'], axis=(1,2)) > 0)\n",
    "\n",
    "    mask_performance = mask_twins\n",
    "\n",
    "    z = data['redshift']  \n",
    "    z1 = (z*3e5+300.)/3e5 # add pec vel err      \n",
    "\n",
    "    dl = (1+z)*z\n",
    "    dl1 = (1+z1)*z1 # don't need leading constant c/H0\n",
    "    \n",
    "    mag_err_vel = -2.5*np.log10(((dl**2*(1+z))/(dl1**2*(1+z1)))) # factors to move to z=0.05\n",
    "\n",
    "    latent = ['$z_{:d}$'.format(i+1) + '$^{PAE}$' for i in range(train_data['z_latent'].shape[1]-3)]\n",
    "\n",
    "\n",
    "    if standardize: \n",
    "        z_in = data['z_latent_mcmc'][:, start_dim:end_dim]\n",
    "        # z_in[:, -1] = np.exp(z_in[:, -1])\n",
    "        if standardize_time:\n",
    "            z_in = np.c_[z_in, data['dtime_mcmc']]\n",
    "\n",
    "        if 'NN' in fit_model:\n",
    "            A_pred = Amodel.predict(z_in).flatten()\n",
    "        if fit_model=='OLS':\n",
    "            A_pred = pfit[0] + z_in @ pfit[1:]\n",
    "\n",
    "        data['amplitude_mcmc'] -= A_pred\n",
    "   \n",
    "\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "    lp_u = np.log(1./np.sqrt(2*np.pi) * np.exp(-1./2 * np.sum(data['u_latent_mcmc']**2, axis=1)))\n",
    "\n",
    "    if use_pz:\n",
    "        lp_u -= data['logJ_u_latent_mcmc']\n",
    "\n",
    "    lp_u -= lp_u.max()\n",
    "    lp_u = np.nan_to_num(lp_u, nan=ymin)\n",
    "    \n",
    "    # lp_u = lp_u[mask_performance & dm_redshift]\n",
    "    \n",
    "    nx = 1\n",
    "    # SALT\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(1.)\n",
    "\n",
    "    ax.tick_params('both', length=6, width=1., which='major', color='k', direction='in')                                         \n",
    "    ax.tick_params('both', length=3, width=1., which='minor', color='k', direction='in')\n",
    "\n",
    "    mask_performance = mask_twins\n",
    "\n",
    "    inds_keep = np.arange(lp_u.shape[0])\n",
    "\n",
    "    dm_resid_twins = dm_resid_twins[mask_performance]\n",
    "    dm_resid_salt = dm_resid_salt[mask_performance]\n",
    "    dm_rbtl = dm_rbtl[mask_performance]\n",
    "    dm_pae = data['amplitude_mcmc'][mask_performance]\n",
    "\n",
    "    lp_u = lp_u[mask_performance]\n",
    "    inds_keep = inds_keep[mask_performance]\n",
    "\n",
    "    inds_sort = np.argsort(lp_u)\n",
    "    inds_lin = np.arange(lp_u.shape[0])\n",
    "\n",
    "    wrms_resid_salt = []\n",
    "    wrms_resid_twins = []\n",
    "    wrms_rbtl = []\n",
    "    wrms_hmc = []\n",
    "    n_sn = []\n",
    "    \n",
    "    min_num = 10\n",
    "    for ind_min in range(inds_sort.shape[0]-min_num):#, 0, -1):\n",
    "        dm = inds_sort >= ind_min\n",
    "        \n",
    "        n_sn.append(dm.sum())\n",
    "\n",
    "        wrms_resid_salt.append(np.std(dm_resid_salt[dm]) )\n",
    "\n",
    "        wrms_rbtl.append(np.std(dm_rbtl[dm]) )\n",
    "        wrms_resid_twins.append(np.std(dm_resid_twins[dm]) )\n",
    "        \n",
    "        wrms_hmc.append(np.std(dm_pae[dm]) )\n",
    "\n",
    "    print(n_sn)\n",
    "    ax.plot(n_sn, wrms_resid_salt,\n",
    "                ms=ms, \n",
    "                color=c_salt, markeredgecolor='k',lw=2, zorder=1, alpha=alpha, label='SALT2')\n",
    "\n",
    "    ax.plot(n_sn, wrms_rbtl,\n",
    "                ms=ms, \n",
    "                color=c_bg, markeredgecolor='k',lw=2, zorder=1, alpha=alpha, label='RBTL')\n",
    "\n",
    "    ax.plot(n_sn, wrms_resid_twins,\n",
    "                ms=ms, \n",
    "                color=c_twins, markeredgecolor='k',lw=2, zorder=1, alpha=alpha, label='Twins')\n",
    "\n",
    "    ax.plot(n_sn, wrms_hmc,\n",
    "                ms=ms, \n",
    "                color=c_pae, markeredgecolor='k',lw=2, zorder=1, alpha=alpha, label='PAE')\n",
    "\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])\n",
    "    ax.grid()\n",
    "    # ax.axvspan(0, min_train_redshift, alpha=0.2, color='gray')\n",
    "    ax.set_xlabel('$N_{SN}(log\\ p(z) > log\\ p_{cut})$')\n",
    "    ax.set_ylabel('$RMS_{\\Delta m}$')\n",
    "    \n",
    "    ax.legend(frameon=True, loc='upper center', \n",
    "              fontsize=fontsize, handletextpad=0.25, ncol=4,\n",
    "             bbox_to_anchor=(0.5, 1.00))#, title=r'$\\sigma, NMAD$')\n",
    "    # ax.axhline(0.0, color='k', ls='--')\n",
    "    # ax.axvline(min_train_redshift, color='k', ls='--')\n",
    "\n",
    "    # ax.set_xlim(0,0.119)\n",
    "    # ax.set_ylim(-yminmax,yminmax)\n",
    "    # ax.set_yticklabels([])\n",
    "\n",
    "    ax.set_title(tstr.capitalize(), fontsize=18)\n",
    "\n",
    "    if savefig:\n",
    "        plt.savefig(figdir+'deltamag_vs_density_{:02d}latent_{:s}.pdf'.format(latent_dim, tstr), bbox_inches='tight')\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 3\n",
    "\n",
    "file_string = 'layers256-128-32_3stage_train_decorrelate_all_seed0'\n",
    "\n",
    "train_data_file = '../outputs/train_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "test_data_file  = '../outputs/test_data_kfold{:d}_posterior_{:02d}Dlatent_{:s}.npy'.format(kfold, latent_dim, file_string)\n",
    "\n",
    "train_data = load_data(train_data_file)\n",
    "test_data  = load_data(test_data_file)\n",
    "\n",
    "params['min_train_redshift'] = 0.02\n",
    "params['max_train_redshift'] = 1.00\n",
    "\n",
    "params['max_light_cut'] = [-10, 40]\n",
    "params['twins_cut'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weird = ['SNF20070528-003', \n",
    "'SNF20070912-000', \n",
    "'SNF20080723-012', \n",
    "'SN2009ig',        \n",
    "'SN2005di',        \n",
    "'SNF20061009-008', \n",
    "'SNF20071001-005',\n",
    "        ]\n",
    "np.isin(test_data['names'], weird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data for peculiar galaxies\n",
    "peculiar_sn = np.genfromtxt('../data/peculiar_sn.txt', delimiter='\\t', dtype=None, names=('name', 'peculiar_type'))\n",
    "peculiar_sn_names = [peculiar_sn[i][0].split()[0].decode(\"utf-8\") for i in range(peculiar_sn.shape[0])]\n",
    "peculiar_sn_types = [peculiar_sn[i][0].split()[1].decode(\"utf-8\") for i in range(peculiar_sn.shape[0])]\n",
    "\n",
    "print(peculiar_sn_names)\n",
    "print(peculiar_sn_types)\n",
    "\n",
    "train_data['sn_type'] = ['normal']*len(train_data['names'])\n",
    "test_data['sn_type'] = ['normal']*len(test_data['names'])\n",
    "\n",
    "for n, t in zip(peculiar_sn_names, peculiar_sn_types):\n",
    "    \n",
    "    if n in train_data['names']:\n",
    "        ind = np.argwhere(train_data['names']==n)[0][0]\n",
    "        train_data['sn_type'][ind] = t\n",
    " \n",
    "    if n in test_data['names']:\n",
    "        ind = np.argwhere(test_data['names']==n)[0][0]\n",
    "        test_data['sn_type'][ind] = t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(train_data['names'] == \"SNPGC51271\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change SN names from internal to external\n",
    "sn_names = np.genfromtxt('../data/name_convention_sn.txt', delimiter='\\t', dtype=None, names=('name_in', 'name_out'))\n",
    "names_in = [sn_names[i][0].split()[0].decode(\"utf-8\") for i in range(sn_names.shape[0])]\n",
    "names_out = [sn_names[i][0].split()[1].decode(\"utf-8\") for i in range(sn_names.shape[0])]\n",
    "\n",
    "print(names_in, names_out)\n",
    "train_names_in = train_data['names']\n",
    "train_names_out = train_data['names']\n",
    "\n",
    "test_names_in = test_data['names']\n",
    "test_names_out = test_data['names']\n",
    "\n",
    "for n_in, n_out in zip(names_in, names_out):\n",
    "    \n",
    "    if n_in in train_data['names']:\n",
    "        print('name change train ', n_in, n_out)\n",
    "        ind = np.argwhere(train_data['names']==n_in)[0][0]\n",
    "        train_names_out[ind] = n_out\n",
    " \n",
    "    if n_in in test_data['names']:\n",
    "        print('name change test ', n_in, n_out)\n",
    "        ind = np.argwhere(test_data['names']==n_in)[0][0]\n",
    "        test_names_out[ind] = n_out\n",
    " \n",
    "train_data['names'] = train_names_out\n",
    "test_data['names'] = test_names_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recheck SN types given SN name can have been updated \n",
    "for n, t in zip(peculiar_sn_names, peculiar_sn_types):\n",
    "    \n",
    "    if n in train_data['names']:\n",
    "        ind = np.argwhere(train_data['names']==n)[0][0]\n",
    "        train_data['sn_type'][ind] = t\n",
    " \n",
    "    if n in test_data['names']:\n",
    "        ind = np.argwhere(test_data['names']==n)[0][0]\n",
    "        test_data['sn_type'][ind] = t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sn_type'] = np.array(train_data['sn_type'])\n",
    "test_data['sn_type']  = np.array(test_data['sn_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sn belonging to types\n",
    "\n",
    "data = train_data\n",
    "# data = test_data\n",
    "dm_T = data['sn_type'] == '91T-like'\n",
    "dm_bg = data['sn_type'] == '91bg-like'\n",
    "\n",
    "for j in range(data['z_latent_mcmc'].shape[1] - 1):\n",
    "    plt.figure()\n",
    "    plt.scatter(data['z_latent_mcmc'][:, j], data['z_latent_mcmc'][:, j+1], alpha=0.5)\n",
    "    plt.scatter(data['z_latent_mcmc'][dm_T, j], data['z_latent_mcmc'][dm_T, j+1], alpha=0.5, c='C1')\n",
    "    plt.scatter(data['z_latent_mcmc'][dm_bg, j], data['z_latent_mcmc'][dm_bg, j+1], alpha=0.5, c='C3')\n",
    "\n",
    "    # plt.xlim(-20,20)\n",
    "    # plt.ylim(-20,20)\n",
    "    # plt.xlabel(labels[j])\n",
    "    # plt.ylabel(labels[j+1])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patheffects as pe\n",
    "\n",
    "savefig=True\n",
    "# savefig=False\n",
    "\n",
    "# use_pz = False\n",
    "use_pz = True\n",
    "\n",
    "num_annotate = 10 # number of low p samples to annotate\n",
    "pcut_annotate = -5.5\n",
    "\n",
    "fontsize = 12\n",
    "text_rotation_angle = 0 #-40\n",
    "text_lw = 2\n",
    "\n",
    "dx = 0.0005\n",
    "dy_max = 0.25\n",
    "dy_min = 0.5\n",
    "\n",
    "nplt_train = 3\n",
    "nplt_test = 3\n",
    "nplt_train_max = 4\n",
    "nplt_test_max = 0\n",
    "\n",
    "ms = 100\n",
    "\n",
    "fontsize_label=16\n",
    "\n",
    "alpha_train = 0.7\n",
    "alpha_test = 0.7\n",
    "alpha_pec = 1.0\n",
    "\n",
    "ymin = -9 - dy_min\n",
    "ymax = 0\n",
    "\n",
    "peculiar_colors = ['C0', 'C1', 'C2', 'C3']\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "# 2D\n",
    "print(train_data['logp_u_latent_mcmc'].min(), test_data['logp_u_latent_mcmc'].min())\n",
    "\n",
    "p_u_train = np.log(1./np.sqrt(2*np.pi) * np.exp(-1./2 * np.sum(train_data['u_latent_mcmc']**2, axis=1)))\n",
    "p_u_test  = np.log(1./np.sqrt(2*np.pi) * np.exp(-1./2 * np.sum(test_data['u_latent_mcmc']**2, axis=1)))\n",
    "\n",
    "if use_pz:\n",
    "    p_u_train -= train_data['logJ_u_latent_mcmc']\n",
    "    p_u_test  -= test_data['logJ_u_latent_mcmc']\n",
    "\n",
    "p_u_train = np.nan_to_num(p_u_train, nan=ymin)\n",
    "p_u_test = np.nan_to_num(p_u_test, nan=ymin)\n",
    "\n",
    "pmax = max(p_u_train.max(), p_u_test.max())\n",
    "\n",
    "inds_train = np.argsort(p_u_train)#[::-1]\n",
    "inds_test = np.argsort(p_u_test)#[::-1]\n",
    "\n",
    "inds_train_max = np.argsort(p_u_train)[::-1]\n",
    "inds_test_max = np.argsort(p_u_test)[::-1]\n",
    "\n",
    "p_u_train -= pmax\n",
    "p_u_test -= pmax\n",
    "\n",
    "dm_test = p_u_test == False\n",
    "dm_train = p_u_train == False\n",
    "\n",
    "p_u_test[p_u_test < ymin+dy_min] = ymin+dy_min\n",
    "p_u_train[p_u_train < ymin+dy_min] = ymin+dy_min\n",
    "\n",
    "dm_train[p_u_train < ymin+dy_min] = True\n",
    "dm_test[p_u_test < ymin+dy_min] = True\n",
    "\n",
    "# z_train = train_data['redshift']\n",
    "# z_test  = train_data['redshift']\n",
    "\n",
    "ax.scatter(train_data['redshift'], p_u_train, edgecolor='none', color='gray', alpha=alpha_train, s=ms)\n",
    "ax.scatter(test_data['redshift'], p_u_test, edgecolor='none', color='gray', alpha=alpha_test, s=ms)\n",
    "\n",
    "peculiar_strings = sorted(np.unique( np.concatenate((train_data['sn_type'], test_data['sn_type']))))\n",
    "ic = 0\n",
    "for i, peculiar_string in enumerate(peculiar_strings):\n",
    "    if peculiar_string not in ['normal']:\n",
    "        dm_train = train_data['sn_type'] == peculiar_string\n",
    "        dm_test = test_data['sn_type'] == peculiar_string\n",
    "\n",
    "        if peculiar_string == 'extreme':\n",
    "            peculiar_string = 'Extreme velocity'\n",
    "            \n",
    "        ax.scatter(train_data['redshift'][dm_train], p_u_train[dm_train], \n",
    "                   edgecolor='none', color=peculiar_colors[ic], alpha=alpha_pec, s=ms)\n",
    "        \n",
    "    \n",
    "        ax.scatter(test_data['redshift'][dm_test], p_u_test[dm_test],\n",
    "                   label=peculiar_string, edgecolor='none', color=peculiar_colors[ic], alpha=alpha_pec, s=ms)\n",
    "\n",
    "#         for i, name in enumerate(train_data['names'][dm_train]):\n",
    "#             ax.annotate(name, (dx+train_data['redshift'][dm_train][i], p_u_train[dm_train][i]),\n",
    "#                 fontsize=fontsize, ha='left', va='top', rotation=text_rotation_angle,\n",
    "#                              path_effects=[pe.Stroke(linewidth=text_lw, foreground='w'), pe.Normal()])\n",
    "#             # txt.set_path_effects([PathEffects.withStroke(linewidth=5, foreground='w')])\n",
    "            \n",
    "#         for i, name in enumerate(test_data['names'][dm_test]):\n",
    "#             ax.annotate(name, (dx+test_data['redshift'][dm_test][i], p_u_test[dm_test][i]),\n",
    "#                 fontsize=fontsize, ha='left', va='top', rotation=text_rotation_angle,\n",
    "#                 path_effects=[pe.Stroke(linewidth=text_lw, foreground='w'), pe.Normal()])\n",
    "\n",
    "                \n",
    "        ic+=1\n",
    "        \n",
    "p_u = np.concatenate((p_u_train, p_u_test))\n",
    "redshift = np.concatenate((train_data['redshift'], test_data['redshift']))\n",
    "names = np.concatenate((train_data['names'], test_data['names']))\n",
    "\n",
    "\n",
    "inds_sort = np.argsort(p_u)\n",
    "\n",
    "p_u = p_u[inds_sort]\n",
    "dm = p_u <= pcut_annotate\n",
    "\n",
    "p_u = p_u[dm]\n",
    "redshift = redshift[inds_sort][dm]\n",
    "names = names[inds_sort][dm]\n",
    "\n",
    "for i in range(p_u.shape[0]):\n",
    "    ha = 'left'\n",
    "    va = 'bottom'\n",
    "    # if i==0:\n",
    "        # ha = 'right'\n",
    "    ax.text(redshift[i], p_u[i], f\"{i}\", #: {names[i]}\", \n",
    "            color='k', alpha=alpha_train,\n",
    "           path_effects=[pe.Stroke(linewidth=3, foreground='w'), pe.Normal()], ha=ha, va=va, rotation=text_rotation_angle, fontsize=12)\n",
    "    print(names[i])\n",
    "\n",
    "# peculiar_string = '91bg-like'\n",
    "# dm_train = train_data['sn_type'] == peculiar_string\n",
    "# dm_test = test_data['sn_type'] == peculiar_string\n",
    "\n",
    "# ax.scatter(train_data['redshift'][dm_train], train_data['logp_u_latent_mcmc'][dm_train], \n",
    "#            label=peculiar_string, edgecolor='k', color=peculiar_colors[1], alpha=0.5)\n",
    "# ax.scatter(test_data['redshift'][dm_test], test_data['logp_u_latent_mcmc'][dm_test],\n",
    "#            edgecolor='k', color=peculiar_colors[1], alpha=0.9)\n",
    "\n",
    "# dm_train = train_data['sn_type'] == '09dc-like'\n",
    "# dm_test = test_data['sn_type'] == '09dc-like'\n",
    "\n",
    "# dm_train = train_data['sn_type'] == '02cx-like'\n",
    "# dm_test = test_data['sn_type'] == '02cx-like'\n",
    "\n",
    "# ax.scatter(train_data['redshift'][dm_train], train_data['logp_u_latent_mcmc'][dm_train], label='Train', edgecolor='k', color='b', alpha=0.5)\n",
    "# ax.scatter(test_data['redshift'][dm_test], test_data['logp_u_latent_mcmc'][dm_test], label='Test', edgecolor='k', color='b', alpha=0.9)\n",
    "\n",
    "\n",
    "ax.axvspan(0, params['min_train_redshift'], alpha=0.2, color='gray')\n",
    "ax.set_xlabel('Redshift', fontsize=fontsize_label)\n",
    "\n",
    "ax.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, 1.105), \n",
    "          fontsize=fontsize_label-2, handletextpad=0.05, ncol=4, columnspacing=0.25)\n",
    "ax.axvline(params['min_train_redshift'], color='k', ls='--')\n",
    "\n",
    "ax.set_xlim(0,0.119)\n",
    "ax.set_ylim(ymin, ymax+dy_max)\n",
    "#\n",
    "# ax.set_ylim(-yminmax,yminmax)\n",
    "\n",
    "if use_pz:\n",
    "    ax.set_ylabel('log p(z) + const.', fontsize=fontsize_label)\n",
    "else:\n",
    "    ax.set_ylabel('log p(u) + const.', fontsize=fontsize_label)\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.025, hspace=0.025)\n",
    "\n",
    "\n",
    "print(inds_train)\n",
    "inds_lin = np.arange(inds_train.shape[0]+inds_test.shape[0])\n",
    "\n",
    "# for i, name in enumerate(train_data['names'][inds_train][:nplt_train]):\n",
    "#     ax.annotate(name, (dx+train_data['redshift'][inds_train][i], p_u_train[inds_train][i]),\n",
    "#                 fontsize=fontsize, ha='left', va='top', rotation=text_rotation_angle,\n",
    "#                 path_effects=[pe.Stroke(linewidth=text_lw, foreground='w'), pe.Normal()])\n",
    "                \n",
    "# for i, name in enumerate(test_data['names'][inds_test][:nplt_test]):\n",
    "#     ax.annotate(name, (dx+test_data['redshift'][inds_test][i], p_u_test[inds_test][i]),\n",
    "#                 fontsize=fontsize, ha='left', va='top', rotation=text_rotation_angle,\n",
    "#                 path_effects=[pe.Stroke(linewidth=text_lw, foreground='w'), pe.Normal()])\n",
    "     \n",
    "# for i, name in enumerate(train_data['names'][inds_train_max][:nplt_train_max]):\n",
    "#     ax.annotate(name, (dx+train_data['redshift'][inds_train_max][i], p_u_train[inds_train_max][i]),\n",
    "#                 fontsize=fontsize, ha='left', va='top', rotation=text_rotation_angle,\n",
    "#                 path_effects=[pe.Stroke(linewidth=text_lw, foreground='w'), pe.Normal()])\n",
    "                \n",
    "# for i, name in enumerate(test_data['names'][inds_test_max][:nplt_test_max]):\n",
    "#     ax.annotate(name, (dx+test_data['redshift'][inds_test_max][i], p_u_test[inds_test_max][i]),\n",
    "#                 fontsize=fontsize, ha='left', va='top', rotation=text_rotation_angle,\n",
    "#                 path_effects=[pe.Stroke(linewidth=text_lw, foreground='w'), pe.Normal()])\n",
    "     \n",
    "\n",
    "inds_below = np.argwhere(dm_train == True)\n",
    "print(inds_below)\n",
    "\n",
    "inds_below = np.argwhere(dm_test == True)\n",
    "print(inds_below)\n",
    "\n",
    "# print(dm_train)\n",
    "# print(dm_test)\n",
    "\n",
    "\n",
    "if savefig:\n",
    "    if use_pz:\n",
    "        plt.savefig('../figures/log_pz_all.pdf', bbox_inches='tight')     \n",
    "    else:\n",
    "        plt.savefig('../figures/log_pu_all.pdf', bbox_inches='tight')     \n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# dm_redshift_train = (data_train['redshifts'] > train_data['min_train_redshift']) & \\\n",
    "#               (data_train['redshifts'] < train_data['max_train_redshift'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# savefig = False\n",
    "savefig = True\n",
    "\n",
    "def plot_grid(data, labels=None, errs=None, names=None, sn_type=None, color_arr=None, color_label=None, xyminmax=None,\n",
    "              figsize=(8,8), spacing=0.05, s=30, alpha=1.0, cmap=plt.cm.viridis, markeredgecolor='k',\n",
    "              savefig=False, filename='plot_grid.pdf', rasterized=False, num_annotate=0):\n",
    "    \"\"\"plot variables against each other. Similar to seaborns pairplot, but more functionality\"\"\"\n",
    "    nx = data.shape[1]-1\n",
    "    ny = data.shape[1]-1\n",
    "\n",
    "    if labels is None:\n",
    "        labels = ['z_{:d}'.format(i) for i in range(data.shape[1])]\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = gridspec.GridSpec(ncols=ny, nrows=nx, figure=fig)\n",
    "\n",
    "    plt.subplots_adjust(wspace=spacing, hspace=spacing)\n",
    "    peculiar_colors = ['C0', 'C1', 'C2', 'C3']\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.cm.viridis\n",
    "       \n",
    "    for i in range(nx):\n",
    "        for j in range(i, ny):\n",
    "            \n",
    "            ii = ny-j\n",
    "            jj = (ny-i+1)%(ny+1)\n",
    "\n",
    "            ax = fig.add_subplot(gs[j, i])\n",
    "            if color_arr is not None:\n",
    "                if i==0 and j==0:\n",
    "                    xx = np.linspace(color_arr.min(), color_arr.max(), 256)\n",
    "                    dummie_cax = ax.scatter(xx, xx, c=xx, cmap=cmap)\n",
    "                    # Clear axis\n",
    "                    ax.cla()\n",
    "\n",
    "            for axis in ['top','bottom','left','right']:\n",
    "                ax.spines[axis].set_linewidth(1.)\n",
    "\n",
    "            ax.tick_params('both', length=6, width=1., which='major', color='k', direction='in')                                         \n",
    "            ax.tick_params('both', length=3, width=1., which='minor', color='k', direction='in')\n",
    "\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(labels[ii])\n",
    "            else:\n",
    "                ax.set_yticklabels([])\n",
    "            \n",
    "            if j == ny-1:\n",
    "                ax.set_xlabel(labels[jj])\n",
    "            else:\n",
    "                ax.set_xticklabels([])\n",
    "\n",
    "            if color_arr is None:\n",
    "                if errs is None:\n",
    "                    ax.scatter(data[:, jj], data[:, ii], s=s, alpha=alpha, rasterized=rasterized)\n",
    "                else:\n",
    "                    ax.errorbar(data[:, jj], data[:, ii], xerr=errs[:, jj], yerr=errs[:, ii], marker='o', color='gray',\n",
    "                                ms=s, alpha=alpha, markeredgecolor=markeredgecolor,\n",
    "                                rasterized=rasterized, ls='none', zorder=-1)\n",
    "                    \n",
    "                    peculiar_strings = ['09dc-like', '91T-like', '91bg-like', 'extreme', 'normal']\n",
    "                    ic = 0\n",
    "                    for ipec, peculiar_string in enumerate(peculiar_strings):\n",
    "                        if peculiar_string not in ['normal']:\n",
    "                            dm = sn_type == peculiar_string\n",
    "\n",
    "                            if peculiar_string == 'extreme':\n",
    "                                peculiar_string = 'Extreme velocity'\n",
    "\n",
    "                            ax.errorbar(data[dm, jj], data[dm, ii], xerr=errs[dm, jj], yerr=errs[dm, ii], marker='o', color=peculiar_colors[ic],\n",
    "                                ms=s+2, markeredgecolor=markeredgecolor,\n",
    "                                rasterized=rasterized, ls='none', alpha=alpha_pec, label=peculiar_string)\n",
    "                    \n",
    "                            # ax.scatter(data[dm, jj], data[dm, ii], \n",
    "                            #            edgecolor='k', color=peculiar_colors[ic], alpha=alpha_pec, s=ms)\n",
    "\n",
    "                            # for iii, name in enumerate(names[dm]):\n",
    "                            #     ax.annotate(name, (dx+data[dm, jj][iii], data[dm, ii][iii]),\n",
    "                            #         fontsize=fontsize, ha='left', va='top', rotation=text_rotation_angle,\n",
    "                            #                      path_effects=[pe.Stroke(linewidth=text_lw, foreground='w'), pe.Normal()])\n",
    "\n",
    "                            ic += 1\n",
    "            else:\n",
    "                if errs is None:\n",
    "                    c = ax.scatter(data[:, jj], data[:, ii], s=s, c=color_arr, edgecolors='k', lw=0.25, alpha=alpha, cmap=cmap, rasterized=rasterized)\n",
    "                else:\n",
    "                    carr = (color_arr-color_arr.min())/(color_arr.max()-color_arr.min())\n",
    "                    for ierr in range(data[:, jj].shape[0]):\n",
    "                        ax.errorbar(data[ierr, jj], data[ierr, ii], xerr=errs[ierr, jj], yerr=errs[ierr, ii], markeredgecolor=markeredgecolor,\n",
    "                                    color=cmap(carr[ierr]), marker='o', ms=s, alpha=alpha, rasterized=rasterized, ls='none', \n",
    "                                     lw=2)\n",
    "\n",
    "            for iann in range(num_annotate):\n",
    "                ha = 'left'\n",
    "                va = 'bottom'\n",
    "                # if iann==0:\n",
    "                #     ha = 'right'\n",
    "                ax.text(data[iann, jj], data[iann, ii], f\"{iann}\", color='k', alpha=alpha_train,\n",
    "                       path_effects=[pe.Stroke(linewidth=3, foreground='w'), pe.Normal()], ha=ha, va=va, fontsize=12)\n",
    "\n",
    "\n",
    "            if xyminmax is not None:\n",
    "                ax.set_ylim(xyminmax[ii])\n",
    "                ax.set_xlim(xyminmax[jj])\n",
    "                \n",
    "            if j == ny-1:\n",
    "                ax.set_xticklabels(ax.get_xticks(), rotation=-15)\n",
    "            if i == nx//2 and j==ny//2:\n",
    "                ax.legend(frameon=False, loc='lower center', bbox_to_anchor=(0.5, 1.05), \n",
    "                          fontsize=fontsize_label-2, handletextpad=0.05, ncol=1, columnspacing=0.25)                \n",
    "    if color_arr is not None:\n",
    "        cax = fig.add_axes([0.92, 0.25, 0.015, 0.5])\n",
    "        fig.colorbar(dummie_cax, cax=cax, label=color_label)\n",
    "\n",
    "    if savefig:\n",
    "        plt.savefig(filename, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "\n",
    "filename = '../figures/grid_PAE_{:d}Dlatent_peculiar_all.pdf'.format(latent_dim)\n",
    "\n",
    "# latent = 'u_latent_mcmc'\n",
    "latent = 'z_latent_mcmc'\n",
    "\n",
    "data = {}\n",
    "data['names'] = np.concatenate((train_data['names'], test_data['names']), axis=0)\n",
    "data['sn_type'] = np.concatenate((train_data['sn_type'], test_data['sn_type']), axis=0)\n",
    "data['z_latent_mcmc'] = np.concatenate((train_data[latent], test_data[latent]), axis=0)\n",
    "data['z_latent_mcmc_err'] = np.concatenate((train_data[latent+'_err'], test_data[latent+'_err']), axis=0)\n",
    "\n",
    "if latent == 'z_latent_mcmc':\n",
    "    data['z_latent_mcmc'][:,0] *= 50 #days\n",
    "    data['z_latent_mcmc_err'][:,0]  *= 50 #days\n",
    "\n",
    "data['redshift'] = np.concatenate((train_data['redshift'], test_data['redshift']), axis=0)\n",
    "data['times_orig'] = np.concatenate((train_data['times_orig'], test_data['times_orig']), axis=0)\n",
    "\n",
    "p_u_train = np.log(1./np.sqrt(2*np.pi) * np.exp(-1./2 * np.sum(train_data['u_latent_mcmc']**2, axis=1)))\n",
    "p_u_test  = np.log(1./np.sqrt(2*np.pi) * np.exp(-1./2 * np.sum(test_data['u_latent_mcmc']**2, axis=1)))\n",
    "\n",
    "if use_pz:\n",
    "    p_u_train -= train_data['logJ_u_latent_mcmc']\n",
    "    p_u_test  -= test_data['logJ_u_latent_mcmc']\n",
    "    \n",
    "pmax = max(p_u_train.max(), p_u_test.max())\n",
    "\n",
    "p_u_train -= pmax\n",
    "p_u_test -= pmax\n",
    "\n",
    "p_u = np.concatenate((p_u_train, p_u_test))\n",
    "inds_sort = np.argsort(p_u)\n",
    "for k, v in data.items(): \n",
    "    if v.shape[0] == inds_sort.shape[0]:\n",
    "        data[k] = v[inds_sort] # sort, so that first in array are ones to annotate\n",
    "\n",
    "dm = p_u <= pcut_annotate\n",
    "num_annotate = np.sum(dm)\n",
    "    \n",
    "# dm = get_train_mask(data, params)\n",
    "dm = np.full(data['redshift'].shape[0], True, dtype=bool)\n",
    "\n",
    "names = data['names'][dm]\n",
    "sn_type = data['sn_type'][dm]\n",
    "\n",
    "istart = 0\n",
    "labels = ['$u_{:d}$'.format(i+1) for i in range(data['z_latent_mcmc'].shape[1])]\n",
    "if latent == 'z_latent_mcmc':\n",
    "    istart = 2\n",
    "    labels = ['$\\Delta A_V$'] + ['$z_{:d}$'.format(i+1) for i in range(latent_dim)]\n",
    "\n",
    "arr_use = data['z_latent_mcmc'][dm, istart:]\n",
    "err_use = data['z_latent_mcmc_err'][dm, istart:]\n",
    "\n",
    "\n",
    "print(arr_use.shape, err_use.shape)\n",
    "# labels = np.roll(labels, -1)\n",
    "# arr_use = np.roll(arr_use, -1, axis=1)\n",
    "# err_use = np.roll(err_use, -1, axis=1)\n",
    "\n",
    "plot_grid(arr_use, errs=err_use, names=names, sn_type=sn_type, \n",
    "          labels=labels, savefig=savefig, filename=filename,\n",
    "         figsize=(8, 8), s=5, alpha=0.8, markeredgecolor='none', num_annotate=num_annotate) \n",
    "\n",
    "\n",
    "# ax.scatter(train_data['redshift'], p_u_train, edgecolor='k', color='gray', alpha=alpha_train, s=ms)\n",
    "# ax.scatter(test_data['redshift'], p_u_test, edgecolor='k', color='gray', alpha=alpha_test, s=ms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_use.shape, names.shape\n",
    "ind_use = 2\n",
    "dm = arr_use[:, ind_use] > 0.3\n",
    "\n",
    "ind_sort = np.argsort(arr_use[dm][:, ind_use])[::-1]\n",
    "print(ind_sort)\n",
    "arr_use[dm][ind_sort], names[dm][ind_sort], sn_type[dm][ind_sort]\n",
    "\n",
    "'SN2012cu', 'LSQ12cyz', 'PTF11mkx', 'SN2006X', 'SNF20070803-005',\n",
    "        'LSQ12fhe', 'SN2011hr', 'LSQ12gdj', 'PTF11bju', 'SNF20080522-000',\n",
    "        'SNF20080720-001', 'PTF10ygu', 'PSNJ07250042+23', 'SN2007le',\n",
    "        'SNF20060624-019', 'LSQ12gxj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(train_data['wavelengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SN_trials.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
