{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33feec2-dc8a-48db-896f-ea00d4a752da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import suPAErnova as snpae\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "\n",
    "# Packages needed only for type checking\n",
    "if TYPE_CHECKING:\n",
    "    from suPAErnova.uttils.suPAErnova_types import CONFIG\n",
    "\n",
    "# Some useful paths to have access to\n",
    "cwd = Path.cwd()\n",
    "examples_dir = cwd.parent.parent.parent.parent\n",
    "data_dir = examples_dir / \"suPAErnova_data\" # Put your data into this directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e901be-45c1-432f-a665-9abc50f0e0f5",
   "metadata": {},
   "source": [
    "# Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e5991d-f7ea-461d-bfc1-da3019afcba4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose = False # Increase log verbosity\n",
    "force = False # Rerun all steps every time\n",
    "\n",
    "cfg = snpae.setup_global_config({}, verbose=verbose, force=force) # Just pass an empty dictionary to initialise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93146e-4e0d-43a7-b53b-674f5ac7bd68",
   "metadata": {},
   "source": [
    "# Data Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6503e-3b48-4cac-857f-ae87a1938e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that these keys *MUST* be captilalised. This is not the case when using a `suPAErnova.toml` config file.\n",
    "data_cfg = {\n",
    "    # === Required Keys ===\n",
    "\n",
    "    # Path to directory containing data.\n",
    "    #   Can be absolute or relative to the base path.\n",
    "    \"DATA_DIR\": str(data_dir), # Needs to be a string so that SuPAErnova can validate it\n",
    "    \n",
    "    # Metadata CSV containing SN names and SALT fit parameters.\n",
    "    #   Can be absolute or relative to the data path.\n",
    "    \"META\": \"meta.csv\",\n",
    "\n",
    "    # TXT file containing additional SALT fit parameters.\n",
    "    #   Can be absolute or relative to the data path.\n",
    "    \"IDR\": \"IDR_eTmax.txt\",\n",
    "\n",
    "    # TXT file containing a mask of bad spectra / wavelength ranges.\n",
    "    #   Can be absolute or relative to the data path.\n",
    "    \"MASK\": \"mask_info_wmin_wmax.txt\",\n",
    "}\n",
    "\n",
    "cfg[\"DATA\"] = data_cfg\n",
    "data = snpae.steps.DATAStep(cfg)\n",
    "success, result = data.setup()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    data.log.error(f\"Error running setup: {result}\")\n",
    "success, result = data.run()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    data.log.error(f\"Error running: {result}\")\n",
    "success, result = data.result()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    data.log.error(f\"Error saving results: {result}\")\n",
    "\n",
    "# The result() function of a step returns the original cfg passed in, but now with the step stored in cfg[\"GLOBAL\"][\"RESULTS\"]step.name]\n",
    "# This allows later steps to access the results of previous steps.\n",
    "cfg = result\n",
    "print(cfg[\"GLOBAL\"][\"RESULTS\"][data.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18b269-2872-433c-b9e4-084754343d00",
   "metadata": {},
   "source": [
    "# TF PAE Setup\n",
    "Just using the default parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4a9bd0-65d2-4588-a1d2-51c202630a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_pae_cfg = {}\n",
    "cfg[\"TF_PAE\"] = tf_pae_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721b543-c1a0-4147-b2ab-c0ad9b3907d2",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "The easiest way to interact with the `SuPAErnova` pipeline, without having to delve into the source code, is through callback functions. These are user-defined functions which run before and after different stages within each step. When using a `suPAErnova.toml` file, these callback function are defined in scripts, with the path to these scripts provided in the confg file. Here you can directly define your functions and pass them in.\n",
    "\n",
    "You do have to be careful with some of these callback function as many will be run within `@tf.function` decorated function, meaning there are some annoying side-effects you'll need to pay attention to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6964c2-3e51-4a80-9c48-240629598752",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = {}\n",
    "\n",
    "# analyse\n",
    "def pre_analyse(self) -> None:\n",
    "    print(\"pre-analyse callback\")\n",
    "\n",
    "def post_analyse(self) -> None:\n",
    "    print(\"post-analyse callback\")\n",
    "\n",
    "callbacks[\"ANALYSE\"] = {\"PRE\": pre_analyse, \"POST\": post_analyse}\n",
    "\n",
    "# result\n",
    "def pre_result(self) -> None:\n",
    "    print(\"pre-result callback\")\n",
    "\n",
    "def post_result(self) -> None:\n",
    "    print(\"post-result callback\")\n",
    "\n",
    "callbacks[\"RESULT\"] = {\"PRE\": pre_result, \"POST\": post_result}\n",
    "\n",
    "# run\n",
    "def pre_run(self) -> None:\n",
    "    print(\"pre-run callback\")\n",
    "\n",
    "def post_run(self) -> None:\n",
    "    print(\"post-run callback\")\n",
    "\n",
    "callbacks[\"RUN\"] = {\"PRE\": pre_run, \"POST\": post_run}\n",
    "\n",
    "# setup\n",
    "def pre_setup(self) -> None:\n",
    "    print(\"pre-setup callback\")\n",
    "\n",
    "def post_setup(self) -> None:\n",
    "    print(\"post-setup callback\")\n",
    "\n",
    "callbacks[\"SETUP\"] = {\"PRE\": pre_setup, \"POST\": post_setup}\n",
    "\n",
    "# validate\n",
    "def pre_validate(self) -> None:\n",
    "    print(\"pre-validate callback\")\n",
    "\n",
    "def post_validate(self) -> None:\n",
    "    print(\"post-validate callback\")\n",
    "\n",
    "callbacks[\"VALIDATE\"] = {\"PRE\": pre_validate, \"POST\": post_validate}\n",
    "\n",
    "# train_all\n",
    "def pre_train_all(self) -> None:\n",
    "    print(\"pre-train_all callback\")\n",
    "\n",
    "def post_train_all(self) -> None:\n",
    "    print(\"post-train_all callback\")\n",
    "\n",
    "callbacks[\"TRAIN_ALL\"] = {\"PRE\": pre_train_all, \"POST\": post_train_all}\n",
    "\n",
    "# train_amplitude\n",
    "def pre_train_amplitude(self) -> None:\n",
    "    print(\"pre-train_amplitude callback\")\n",
    "\n",
    "def post_train_amplitude(self) -> None:\n",
    "    print(\"post-train_amplitude callback\")\n",
    "\n",
    "callbacks[\"TRAIN_AMPLITUDE\"] = {\"PRE\": pre_train_amplitude, \"POST\": post_train_amplitude}\n",
    "\n",
    "# train_colour  \n",
    "def pre_train_colour(self) -> None:\n",
    "    print(\"pre-train_colour callback\")\n",
    "\n",
    "def post_train_colour(self) -> None:\n",
    "    print(\"post-train_colour callback\")\n",
    "\n",
    "callbacks[\"TRAIN_COLOUR\"] = {\"PRE\": pre_train_colour, \"POST\": post_train_colour}\n",
    "\n",
    "# train_latents    \n",
    "def pre_train_latents(self) -> None:\n",
    "    print(\"pre-train_latents callback\")\n",
    "\n",
    "def post_train_latents(self) -> None:\n",
    "    print(\"post-train_latents callback\")\n",
    "\n",
    "callbacks[\"TRAIN_LATENTS\"] = {\"PRE\": pre_train_latents, \"POST\": post_train_latents}\n",
    "\n",
    "# train_model      \n",
    "def pre_train_model(self) -> None:\n",
    "    print(\"pre-train_model callback\")\n",
    "\n",
    "def post_train_model(self) -> None:\n",
    "    print(\"post-train_model callback\")\n",
    "\n",
    "callbacks[\"TRAIN_MODEL\"] = {\"PRE\": pre_train_model, \"POST\": post_train_model}\n",
    "\n",
    "# train_time     \n",
    "def pre_train_time(self) -> None:\n",
    "    print(\"pre-train_time callback\")\n",
    "\n",
    "def post_train_time(self) -> None:\n",
    "    print(\"post-train_time callback\")\n",
    "\n",
    "callbacks[\"TRAIN_TIME\"] = {\"PRE\": pre_train_time, \"POST\": post_train_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a049cee-0152-4fd9-9ab2-254141afab10",
   "metadata": {},
   "source": [
    "## Custom activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f1612-f786-45d9-8d8a-735101fb2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_activation\n",
    "def pre_setup_activation(self) -> None:\n",
    "    print(\"pre-setup_activation callback\")\n",
    "    print(self.opts[\"ACTIVATION\"]) # The string the user has passed\n",
    "\n",
    "\n",
    "def post_setup_activation(self) -> None:\n",
    "    print(\"post-setup_activation callback\")\n",
    "    \n",
    "    print(self.opts[\"ACTIVATION\"]) # The corresponding pre-built activation function\n",
    "    # Here, you can replace `self.opts[\"ACTIVATION\"]` with your own function\n",
    "    # As long as it has the signature f(x: _ActivationInput) -> tf.Tensor\n",
    "    # It should work fine.\n",
    "\n",
    "    original_activation = self.opts[\"ACTIVATION\"]\n",
    "\n",
    "    # Add this to register your function as serialisable, allowing it to be saved and loaded alongside the TF_PAEModel\n",
    "    @ks.utils.register_keras_serializable(name=\"custom_activation\")\n",
    "    def custom_activation(x: \"ks.activations._ActivationInput\") -> \"tf.Tensor\":\n",
    "        # Warning, this prints *a lot*, here for demonstration purposes, but I wouldn't recommend actually doing this.\n",
    "        # TODO: Find where this is being used outside of a `@tf.function` wrapper, and fix that\n",
    "        # print(f\"my custom activation function received {x} as input\")\n",
    "        \n",
    "        return original_activation(x)\n",
    "    self.opts[\"ACTIVATION\"] = custom_activation\n",
    "\n",
    "    print(self.opts[\"ACTIVATION\"]) # Your custom activation function\n",
    "\n",
    "callbacks[\"SETUP_ACTIVATION\"] = {\"PRE\": pre_setup_activation, \"POST\": post_setup_activation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4671b8d-72b4-4494-9aee-075cfe822701",
   "metadata": {},
   "source": [
    "## Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbfd7c-c1c5-43e7-a8f1-9f5d0d572684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_loss\n",
    "def pre_setup_loss(self) -> None:\n",
    "    print(\"pre-setup_loss callback\")\n",
    "    print(self.opts[\"LOSS\"]) # The string the user has passed\n",
    "\n",
    "\n",
    "def post_setup_loss(self) -> None:\n",
    "    print(\"post-setup_loss callback\")\n",
    "    \n",
    "    print(self.opts[\"LOSS\"]) # The corresponding pre-built loss function\n",
    "    # Here, you can replace `self.opts[\"LOSS\"]` with your own function\n",
    "    # As long as it has the signature f(x: tf.Tensor, x_pred: tf.Tensor, kwargs: CONFIG[tf.Tensor]) -> tf.Tensor\n",
    "    # It should work fine.\n",
    "\n",
    "    # Here:\n",
    "    #   - `x` is the amplitude (flux) of the real SN spectra\n",
    "    #   - `x_pred` is the amplitude (flux) of the corresponding encode-decode SN spectra\n",
    "    #   - `kwargs` is a dictionary containing:\n",
    "    #       \"sigma\": The uncertainty in the amplitude (flux) of the real SN spectra\n",
    "    #       \"mask\": The mask of spectral data which should be ignored\n",
    "    #       \"model\": The `TF_PAEModel` being train, which itself will contain lots of other information you can use\n",
    "    #\n",
    "    # The returned tensor must have shape=(), i.e. be a single number as a Tensor. This can usually be achieved through the various reduce_* functions.\n",
    "\n",
    "    original_loss = self.opts[\"LOSS\"]\n",
    "    def custom_loss(x: \"tf.Tensor\", x_pred: \"tf.Tensor\", kwargs: \"CONFIG[tf.Tensor]\") -> \"tf.Tensor\":\n",
    "        # This will only print once, since this runs within a `@tf.function` wrapper\n",
    "                # Use tf.print to print every time this is run\n",
    "        print(f\"my custom loss function received a real spectra: {x}, a predicted spectra: {x_pred}, and kwargs: {kwargs}\")\n",
    "        return original_loss(x, x_pred, kwargs)\n",
    "    \n",
    "    self.opts[\"LOSS\"] = custom_loss\n",
    "    print(self.opts[\"LOSS\"]) # Your custom loss function\n",
    "\n",
    "callbacks[\"SETUP_LOSS\"] = {\"PRE\": pre_setup_loss, \"POST\": post_setup_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8dace-ca24-4d9f-9e1c-99d5ec3efcda",
   "metadata": {},
   "source": [
    "## Custom optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc402a-6130-4ac0-8897-b5d46344c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_optimiser\n",
    "def pre_setup_optimiser(self) -> None:\n",
    "    print(\"pre-setup_optimiser callback\")\n",
    "    print(self.opts[\"OPTIMISER\"]) # The string the user has passed\n",
    "\n",
    "\n",
    "def post_setup_optimiser(self) -> None:\n",
    "    print(\"post-setup_optimiser callback\")\n",
    "    \n",
    "    print(self.optimiser) # The corresponding pre-built optimiser function\n",
    "    # Here, you can replace `self.optimiser` with your own function\n",
    "    # As long as it has the signature f(lr: ks.optimizers.schedules.LearningRateSchedule | float, kwargs: CFG) -> ks.optimizers.Optimizer \n",
    "    # It should work fine.\n",
    "\n",
    "    # Here:\n",
    "    #   - `lr` is the incoming learning rate either as a float or a LearningRateSchedule\n",
    "    #   - `kwargs` is a dictionary containing:\n",
    "    #       \"lr_decay_steps\": The user-defined `lr_decay_steps` parameter\n",
    "    #       \"lr_decay_rate\": The user-defined `lr_decay_rate` parameter\n",
    "    #       \"weight_decay_rate\": The user-defined `weight_decay_rate` parameter\n",
    " \n",
    "    original_optimiser = self.optimiser\n",
    "    def custom_optimiser(lr: \"ks.optimizers.schedules.LearningRateSchedule | float\", kwargs: \"CFG\") -> \"ks.optimizers.Optimizer\":\n",
    "        # This will only print once, since this runs within a `@tf.function` wrapper\n",
    "        # Use tf.print to print every time this is run\n",
    "        print(f\"my custom optimiser received a learning rate: {lr} and kwargs: {kwargs}\")\n",
    "        return original_optimiser(lr, kwargs)\n",
    "    \n",
    "    self.optimiser = custom_optimiser\n",
    "    print(self.optimiser) # Your custom optimiser function\n",
    "\n",
    "callbacks[\"SETUP_OPTIMISER\"] = {\"PRE\": pre_setup_optimiser, \"POST\": post_setup_optimiser}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48570ef3-23a7-47a4-8b4d-7492efee8258",
   "metadata": {},
   "source": [
    "## Custom scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267d074-6f6a-4f50-b08f-77b2d79179ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_scheduler\n",
    "def pre_setup_scheduler(self) -> None:\n",
    "    print(\"pre-setup_scheduler callback\")\n",
    "    print(self.opts[\"SCHEDULER\"]) # The string the user has passed\n",
    "\n",
    "\n",
    "def post_setup_scheduler(self) -> None:\n",
    "    print(\"post-setup_scheduler callback\")\n",
    "    \n",
    "    print(self.scheduler) # The corresponding pre-built scheduler function\n",
    "    # Here, you can replace `self.scheduler` with your own function\n",
    "    # As long as it has the signature f(lr: float, kwargs: CFG) -> ks.optimizers.schedules.LearningRateSchedule | flaot\n",
    "    # It should work fine.\n",
    "\n",
    "    # Here:\n",
    "    #   - `lr` is the incoming learning rate either as a float\n",
    "    #   - `kwargs` is a dictionary containing:\n",
    "    #       \"lr_decay_steps\": The user-defined `lr_decay_steps` parameter\n",
    "    #       \"lr_decay_rate\": The user-defined `lr_decay_rate` parameter\n",
    "    #       \"weight_decay_rate\": The user-defined `weight_decay_rate` parameter\n",
    " \n",
    "    original_scheduler = self.scheduler\n",
    "    def custom_scheduler(lr: \"float\", kwargs: \"CFG\") -> \"ks.optimizers.schedules.LearningRateSchedule | float\":\n",
    "        # This will only print once, since this runs within a `@tf.function` wrapper\n",
    "        # Use tf.print to print every time this is run\n",
    "        print(f\"my custom scheduler received a learning rate: {lr} and kwargs: {kwargs}\")\n",
    "        return original_scheduler(lr, kwargs)\n",
    "    \n",
    "    self.scheduler = custom_scheduler\n",
    "    print(self.scheduler) # Your custom scheduler function\n",
    "\n",
    "callbacks[\"SETUP_SCHEDULER\"] = {\"PRE\": pre_setup_scheduler, \"POST\": post_setup_scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242f336-c93a-490c-bdef-df96f83cce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"TF_PAE\"][\"CALLBACKS\"] = callbacks\n",
    "\n",
    "tf_pae = snpae.steps.TF_PAEStep(cfg)\n",
    "print(tf_pae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dadb65-da07-4c61-ac99-bed28641d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, result = tf_pae.setup()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    tf_pae.log.error(f\"Error running setup: {result}\")\n",
    "success, result = tf_pae.run()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    tf_pae.log.error(f\"Error running: {result}\")\n",
    "success, result = tf_pae.result()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    tf_pae.log.error(f\"Error saving results: {result}\")\n",
    "success, result = tf_pae.analyse()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    tf_pae.log.error(f\"Error analysing: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
