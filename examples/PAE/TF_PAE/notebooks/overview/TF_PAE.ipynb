{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33feec2-dc8a-48db-896f-ea00d4a752da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import suPAErnova as snpae\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Some useful paths to have access to\n",
    "cwd = Path.cwd()\n",
    "examples_dir = cwd.parent.parent.parent.parent\n",
    "data_dir = examples_dir / \"suPAErnova_data\" # Put your data into this directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e901be-45c1-432f-a665-9abc50f0e0f5",
   "metadata": {},
   "source": [
    "# Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e5991d-f7ea-461d-bfc1-da3019afcba4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose = False # Increase log verbosity\n",
    "force = False # Don't rerun steps if not necessary\n",
    "\n",
    "cfg = snpae.setup_global_config({}, verbose=verbose, force=force) # Just pass an empty dictionary to initialise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f49232-349b-4683-9c77-0425b72bef66",
   "metadata": {},
   "source": [
    "# Order of Operations\n",
    "To run any `PAE` step, you must first run the `DATA` step, storing its results for later use. If you try to run a `PAE` step anyway, you'll get the following error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79222223-1afb-417e-bc51-ba2e1d10bc3f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "cfg[\"TF_PAE\"] = {}\n",
    "_tf_pae = snpae.steps.TF_PAEStep(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed22a6d-fc61-48b4-98b5-f5a47da0bf36",
   "metadata": {},
   "source": [
    "# Data Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae6503e-3b48-4cac-857f-ae87a1938e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that these keys *MUST* be captilalised. This is not the case when using a `suPAErnova.toml` config file.\n",
    "data_cfg = {\n",
    "    # === Required Keys ===\n",
    "\n",
    "    # Path to directory containing data.\n",
    "    #   Can be absolute or relative to the base path.\n",
    "    \"DATA_DIR\": str(data_dir), # Needs to be a string so that SuPAErnova can validate it\n",
    "    \n",
    "    # Metadata CSV containing SN names and SALT fit parameters.\n",
    "    #   Can be absolute or relative to the data path.\n",
    "    \"META\": \"meta.csv\",\n",
    "\n",
    "    # TXT file containing additional SALT fit parameters.\n",
    "    #   Can be absolute or relative to the data path.\n",
    "    \"IDR\": \"IDR_eTmax.txt\",\n",
    "\n",
    "    # TXT file containing a mask of bad spectra / wavelength ranges.\n",
    "    #   Can be absolute or relative to the data path.\n",
    "    \"MASK\": \"mask_info_wmin_wmax.txt\",\n",
    "}\n",
    "\n",
    "cfg[\"DATA\"] = data_cfg\n",
    "data = snpae.steps.DATAStep(cfg)\n",
    "success, result = data.setup()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    data.log.error(f\"Error running setup: {result}\")\n",
    "success, result = data.run()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    data.log.error(f\"Error running: {result}\")\n",
    "success, result = data.result()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    data.log.error(f\"Error saving results: {result}\")\n",
    "\n",
    "# The result() function of a step returns the original cfg passed in, but now with the step stored in cfg[\"GLOBAL\"][\"RESULTS\"]step.name]\n",
    "# This allows later steps to access the results of previous steps.\n",
    "cfg = result\n",
    "print(cfg[\"GLOBAL\"][\"RESULTS\"][data.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4fe8ae-5cd9-4a40-9368-9dcd656920aa",
   "metadata": {},
   "source": [
    "# TF_PAE Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a77cae4-12e0-4cc8-9f72-a98a96a8e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that these keys *MUST* be captilalised. This is not the case when using a `suPAErnova.toml` config file.\n",
    "tf_pae_cfg = { # TODO: Reorder / Reformat / Rename these keys to make it easy to handle\n",
    "    # Path to a colourlaw file. If provided, will be used as the kernel initialiser in the decoder.\n",
    "    #   Defaults to \"\" (i.e. no colourlaw file)\n",
    "    \"colourlaw\": \"\",\n",
    "\n",
    "    # Which kfold (or kfolds if passing a list) to use\n",
    "    #   Defaults to 0\n",
    "    \"kfold\": 0,\n",
    "\n",
    "    # What fraction of training data to split into validation data. Set to 0.0 to use test data as validation data.\n",
    "    #   Defaults to 0.0\n",
    "    \"validation_frac\": 0.0,\n",
    "\n",
    "    # Number of epochs to use when training ŒîA·µ•\n",
    "    #   Defaults to 1000\n",
    "    \"epochs_colour\": 1000,\n",
    "\n",
    "    # Number of epochs to use when training ŒîA·µ• and any number of non-physical latent parameters\n",
    "    #   Defaults to 1000\n",
    "    \"epochs_latent\": 1000,\n",
    "\n",
    "    # Number of epoch to use when training ŒîA·µ• all non-physical latent parameters, and Œî‚Ñ≥\n",
    "    #   Defaults to 1000\n",
    "    \"epochs_amplitude\": 1000,\n",
    "\n",
    "    # Number of epoch to use when training ŒîA·µ• all non-physical latent parameters, Œî‚Ñ≥, and ŒîùìÖ (i.e. all parameters)\n",
    "    #   Defaults to 5000\n",
    "    \"epochs_all\": 5000,\n",
    "\n",
    "    # Split training into several stages (true), or train all parameters at once (false).\n",
    "    #   When splitting, the result of the previous stage will be used as the starting point for the next stage.\n",
    "    #   When splitting, the following stages are run:\n",
    "    #   If physical_latent:\n",
    "    #       Stage 0: Train ŒîA·µ•\n",
    "    #   If latent_dim > 0:\n",
    "    #       Stage latent_dim - (latent_dim - 1): Train ŒîA·µ• and the first non-physical latent parameter\n",
    "    #       Stage latent_dim - (latent_dim - 2): Train ŒîA·µ• and the first two non-physical latent parameters\n",
    "    #       ...\n",
    "    #       Stage latent_dim: Train ŒîA·µ• and all non-physical latent parameters\n",
    "    #   If physical_latent:\n",
    "    #       Stage latent_dim + 1: Train ŒîA·µ•, all non-physical latent parameters, and Œî‚Ñ≥\n",
    "    #       Stage latent_dim + 2: Train ŒîA·µ•, all non-physical latent parameters, Œî‚Ñ≥, and ŒîùìÖ\n",
    "    #   Defaults to true\n",
    "    \"split_training\": True,\n",
    "\n",
    "    # Minimum redshift for spectral data.\n",
    "    #   Defaults to 0.02\n",
    "    \"min_train_redshift\": 0.02,\n",
    "\n",
    "    # Maximum redshift for spectral data.\n",
    "    #   Defaults to 1.0\n",
    "    \"max_train_redshift\": 1.0,\n",
    "\n",
    "    # Minimum phase for spectral data, relative to peak. Spectral data earlier than this phase will be cut. Defaults to -10\n",
    "    \"min_train_phase\": -10,\n",
    "\n",
    "    # Maximum phase for spectral data, relative to peak. Spectral data later than this phase will be cut.\n",
    "    #   Defaults to 40\n",
    "    \"max_train_phase\": 40,\n",
    "\n",
    "    # Loss function to use during training. Possible values include:\n",
    "    #   - \"MAE\": [Mean Absolute Error](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError)\n",
    "    #   - \"WMAE\": Weighted [Mean Absolute Error](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanAbsoluteError)\n",
    "    #   - \"MSE\": [Mean Squared Error](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)\n",
    "    #   - \"WMSE\": Weighted [Mean Squared Error](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)\n",
    "    #   - \"RMSE\": Root [Mean Squared Error](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)\n",
    "    #   - \"WRMSE\": Weighted Root [Mean Squared Error](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError)\n",
    "    #   - \"NGLL\": Negative Gaussian Log Likelihood\n",
    "    #   - \"HUBER\": [Huber](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber)\n",
    "    #   - \"WHUBER\": Weighted [Huber](https://www.tensorflow.org/api_docs/python/tf/keras/losses/Huber)\n",
    "    #   - \"MAGNITUDE\": Custom loss function comparing spectral magnitudes\n",
    "    #   - \"NULL\": Custom loss function which returns None no matter what. Designed for use with custom user-defined loss functions, integrated through callbacks.\n",
    "    #   Defaults to WMAE\n",
    "    \"loss\": \"WMAE\",\n",
    "\n",
    "    # Scaling factor which punishes the overall difference between real spectra and their corresponding encode-decode spectra.\n",
    "    #   Defaults to 500.0\n",
    "    \"loss_amplitude_offset\": 500.0,\n",
    "\n",
    "    # Scaling factor which encourages median ŒîùìÖ close to 1\n",
    "    #   Defaults to 1000.0\n",
    "    \"loss_amplitude_parameter\": 1000.0,\n",
    "\n",
    "    # Scaling factor applied to the covariant loss between physical model parameters\n",
    "    #   Defaults to 50000.0\n",
    "    \"loss_covariance\": 50000.0,\n",
    "\n",
    "    # Ignore correlations with dust (i.e. colour)\n",
    "    #   TODO: Better explanation\n",
    "    #   Defaults to true\n",
    "    \"decorrelate_dust\": True,\n",
    "\n",
    "    # Ignore correlations\n",
    "    #   TODO: Better explanation\n",
    "    #   Defaults to true\n",
    "    \"decorrelate_all\": True,\n",
    "\n",
    "    # Set the size of each batch the data will be randomly split between.\n",
    "    #   Must evenly split the data.\n",
    "    #   TODO: Replace batch_size with num_batches which will then split the data as evenly as possible between that many batches. Should result in a better user experience\n",
    "    #   Defaults to 57, the size of the batches used in the original SuPAErnova examples\n",
    "    \"batch_size\": 57,\n",
    "\n",
    "    # Controls the scale of random offsets applied to the training data's amplitude parameter.\n",
    "    #   Will apply a gaussian offset with width determined by the observational uncertainty, scaled by `noise_scale`.\n",
    "    #   Thus a noise_scale of 0.0 will not apply any offset.\n",
    "    #   Defaults to 1.0\n",
    "    \"noise_scale\": 1.0,\n",
    "\n",
    "    # The fraction of spectra to randomly mask throughout training.\n",
    "    #   Defaults to 0.1\n",
    "    \"mask_frac\": 0.1,\n",
    "\n",
    "    # Controls the scale of random offsets applied to the training data's phase parameter.\n",
    "    #   If sigma_time is a positive value (`n`), apply a gaussian offset with constant width `n / 50`.\n",
    "    #   If sigma_time is 0, apply a gaussian offset with width determined by SALT constraints (`dphase / 50`).\n",
    "    #   If sigma_time is -1, no offset will be applied.\n",
    "    #   Defaults to 0.3\n",
    "    \"sigma_time\": 0.3,\n",
    "\n",
    "    # Whether to save intermediate models during training.\n",
    "    #   Defaults to true\n",
    "    \"save_model\": True,\n",
    "\n",
    "    # In between training stages, load the best model of the previous stage (if true), or the final epoch of the previous stage (if false).\n",
    "    #   Defaults to false\n",
    "    \"load_best\": False,\n",
    "\n",
    "    # Whether to use the [\"DENSE\"](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) or [\"CONVOLUTIONAL\"](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) model architecture.\n",
    "    #   Defaults to \"DENSE\"\n",
    "    \"layer_type\": \"DENSE\",\n",
    "\n",
    "    # Which activation function to use. Possible values include:\n",
    "    #   - \"ELU\": Use an [Exponential Linear Unit](https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu) activation function\n",
    "    #   - \"GELU\": Use a [Gaussian Error Linear Unit](https://www.tensorflow.org/api_docs/python/tf/keras/activations/gelu) activation function\n",
    "    #   - \"RELU\": Use a [REctified Linear Unit](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) activation function\n",
    "    #   - \"SWISH\": Use a [Swish / Silu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/silu) activation function\n",
    "    #   - \"TANH\": Use a [Hyperbolic Tangent](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh) activation function\n",
    "    #   - \"NULL\": Custom activation function which returns None no matter what. Designed for use with custom user-defined activation functions, integrated through callbacks.\n",
    "    #   Defaults to \"RELU\"\n",
    "    \"activation\": \"RELU\",\n",
    "\n",
    "    # What learning rate to use for training\n",
    "    #   Defaults to 0.005\n",
    "    \"lr\": 0.005,\n",
    "\n",
    "    # What learning rate to use when training the final model, which required training all parameters simultaneously.\n",
    "    #   Default to 0.001\n",
    "    \"lr_deltat\": 0.001,\n",
    "    \n",
    "    # Which optimiser to use. Possible values include:\n",
    "    #   - \"ADAM\": Use an [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimiser.\n",
    "    #   - \"ADAMW\": Use a [Weighted Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/AdamW) optimiser.\n",
    "    #   - \"SGD\": Use a [Gradient Descent with Momentum](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) optimiser.\n",
    "    #   - \"NULL\": Custom optimiser which returns None no matter what. Designed for use with custom user-defined optimisers, integrated through callbacks.\n",
    "    #   Defaults to \"ADAMW\"\n",
    "    \"optimiser\": \"ADAMW\",\n",
    "\n",
    "    # Rate of weight decay to use in your chosen optimiser\n",
    "    #   Default to 0.00001\n",
    "    \"weight_decay_rate\": 0.00001,\n",
    "\n",
    "    # Which scheduler to use. Possible values include:\n",
    "    #   - \"EXPONENTIAL\": Use an [Exponential Decay](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) scheduler.\n",
    "    #   - \"IDENTITY\": Use an Identity scheduler, which just returns the provided learning rate.\n",
    "    #   - \"NULL\": Custom scheduler which returns None no matter what. Designed for use with custom user-defined schedulers, integrated through callbacks.\n",
    "    #   Defaults to \"EXPONENTIAL\"\n",
    "    \"scheduler\": \"EXPONENTIAL\",\n",
    "\n",
    "    # Rate of learning rate decay to use in your chosen scheduler\n",
    "    #   Defaults to 0.95\n",
    "    \"lr_decay_rate\": 0.95,\n",
    "\n",
    "    # Number of learning rate decay steps to use in your chosen scheduler\n",
    "    #   Defaults to 300\n",
    "    \"lr_decay_steps\": 300,\n",
    "\n",
    "    # [L2 Regularisation](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2)  penalty to apply during training, or 0.0 for no penalty.\n",
    "    #   Defaults to 0.0\n",
    "    \"kernel_regulariser\": 0.0,\n",
    "\n",
    "    # Whether to include a [Dropout Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) to the encoder, with a rate of `dropout`. Setting this to 0.0 disables the Dropout Layer.\n",
    "    #   Defaults to 0.0\n",
    "    \"dropout\": 0.0,\n",
    "\n",
    "    # Whether to include a [Batch Normalisation Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) to the encoder.\n",
    "    #   Defaults to false\n",
    "    \"batchnorm\": False,\n",
    "\n",
    "    # Whether to include physical latent parameters.\n",
    "    #   Defaults to true, which corresponds to three physical latent parameters:\n",
    "    #     - ŒîùìÖ: Different in time of peak brightness relative to the SALT fits (labelled as `phase` throughout the source code)\n",
    "    #     - Œî‚Ñ≥ : Magnitude residual (labeled as `amplitude` or `amp` throughout the source code)\n",
    "    #     - ŒîA·µ•: Relative extrinsic extinction (labeled as `AV` throughout the source code)\n",
    "    \"physical_latent\": True,\n",
    "\n",
    "    # Dimensions of each dense layer in the encoder.\n",
    "    #   Default to [256, 128]\n",
    "    \"encode_dims\": [256, 128],\n",
    "\n",
    "    # Dimensions of each dense layer in the decoder. If an empty list is passed, the reverse of encode_dims will be used.\n",
    "    #   Defaults to []\n",
    "    \"decode_dims\": [],\n",
    "\n",
    "    # The number of non-physical latent dimensions to encode.\n",
    "    #   Defaults to 3\n",
    "    \"latent_dim\": 3,\n",
    "}\n",
    "\n",
    "cfg[\"TF_PAE\"] = tf_pae_cfg\n",
    "tf_pae = snpae.steps.TF_PAEStep(cfg)\n",
    "print(tf_pae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16ba7f-983b-42d7-8742-96e10f0533d2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f43796-da73-4b6a-9fef-775207d91683",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, result = tf_pae.setup()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    tf_pae.log.error(f\"Error running setup: {result}\")\n",
    "print(tf_pae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd82c37b-00bb-411e-a09c-e340a2b66545",
   "metadata": {},
   "source": [
    "# Run\n",
    "Warning, this will actually perform the `PAE` training and may take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889f0a2-3e04-4083-8f21-e3c199c55fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, result = tf_pae.run()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    tf_pae.log.error(f\"Error running: {result}\")\n",
    "print(tf_pae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ac086-b66f-4ff0-b8eb-1dd7ff0afaee",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01d8c44-f22f-4ec0-bc52-d0afa5c4e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, result = tf_pae.result()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    tf_pae.log.error(f\"Error saving results: {result}\")\n",
    "\n",
    "# The result() function of a step returns the original cfg passed in, but now with the step stored in cfg[\"GLOBAL\"][\"RESULTS\"]step.name]\n",
    "# This allows later steps to access the results of previous steps.\n",
    "cfg = result\n",
    "print(cfg[\"GLOBAL\"][\"RESULTS\"][tf_pae.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226efc7-16d0-4286-9286-2028ecfe11fb",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "No automatic analyse function integrated at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38407757-5228-4bd0-abd3-215a82951794",
   "metadata": {},
   "outputs": [],
   "source": [
    "success, result = tf_pae.analyse()\n",
    "if not success: # Make sure you handle failures appropriately!\n",
    "    tf_pae.log.error(f\"Error analysing: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
